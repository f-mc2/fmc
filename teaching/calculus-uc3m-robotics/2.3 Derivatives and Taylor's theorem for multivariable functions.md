---
dg-home: false
dg-publish: true
---

# Derivatives and differentiability of multivariable functions

We discuss here how the notion of [[1.3 Derivatives of scalar functions|derivative]] can be extended to the multivariable vector case. 

>[!defn] Definition (Matrix of partial derivatives)
>Consider the multivariable vector function $f\colon D \subseteq \mathbb{R}^{k}\rightarrow \mathbb{R}^{n}$ given by
>$$
>f(x^{1},\cdots , x^{k})\equiv f(\mathbf{x})=\left(\begin{matrix} \;f^{1}(\mathbf{x}) \;\\ \; f^{2}(\mathbf{x}) \; \\ \;\vdots \; \\ \; f^{n}(\mathbf{x}) \;\end{matrix}\right),
>$$
>and let $\mathbf{x}_{0}$ be an interior point of $D$. The matrix of partial derivatives of $f$ at $\mathbf{x}_{0}$ is the matrix $\mathbf{D}f(\mathbf{x}_{0})$ whose $(i,j)$-th element reads
>
>$$
> (\mathbf{D}f)^{i}_{j}(\mathbf{x}_{0})\equiv f_{j}^{i}|_{\mathbf{x}_{0}}\equiv \left.\frac{\partial f^{i}}{\partial x^{j}}\right|_{\mathbf{x}_{0}} :=\lim_{h\rightarrow 0} \frac{f^{i}(\mathbf{x}_{0} + h\mathbf{e}_{j})-f^{i}(\mathbf{x}_{0})}{h} .
>$$
>The matrix of partial derivatives $\mathbf{D}f(\mathbf{x}_{0})$ is also called the **Jacobian** of $f$, and, when $n=1$, the Jacobian becomes a horizontal vector known as the **gradient** vector of $f$ at $\mathbf{x}_{0}$, and it is denoted by $\nabla f(\mathbf{x}_{0})$.

^c8b7dc

>[!rem] Remark: computation of partial derivatives
>Given $f\colon D\subseteq\mathbb{R}^{k}\rightarrow\mathbb{R}^{n}$, to compute the partial derivative $\left.\frac{\partial f^{i}}{\partial x^{j}}\right|_{\mathbf{x}_{0}}$, we first compute the derivative of $f^{i}$ with respect to $x^{j}$ using the rules we learned for [[1.3 Derivatives of scalar functions|scalar functions]] by considering all variables except $x^{j}$ to be constants. If the result is well-defined, that is the partial derivative we were looking for. If not, we must use the original definition of partial derivative in terms of limits.

>[!exmp] Example
>We now describe an example in which it is not possible to use the rules we learned for [[1.3 Derivatives of scalar functions|scalar functions]] to compute the partial derivative at a given point, and we must use the definition in terms of limit given above.
>Consider the function $f\colon \mathbb{R}^{2}\rightarrow \mathbb{R}$ given by
>$$
>f(x,y)=\sqrt[3]{x\,y} 
>$$
>and the point $(0,0)\in\mathbb{R}$. If we take the derivative of $f$ with respect to $x$ considering $y$ as a constant, we immediately obtain the function 
>$$
>F(x,y)=\frac{\sqrt[3]{y}}{3(x)^{\frac{2}{3}}}
>$$
>which is not defined at $(0,0)$. Similarly, if we take the derivative of $f$ with respect to $y$ as if $x$ was a constant, we immediately obtain the function 
>$$
>G(x,y)=\frac{\sqrt[3]{x}}{3(y)^{\frac{2}{3}}}
>$$
>which is not defined at $(0,0)$.
>However, if we exploit the definition of partial derivative given above, we obtain
>$$
>\mathbf{D}_{x}f(0,0)=\lim_{h\rightarrow 0} \frac{f(0+h,0)-f(0,0)}{h}=0 
>$$
>and similarly for $\mathbf{D}_{y}f(0,0)$. Therefore, we conclude that both partial derivatives of $f$ at $(0,0)$ exist and we have $\mathbf{D}f(0,0)=\nabla f(0,0)=\mathbf{0}$. 

In general, the existence of partial derivatives is not enough to guarantee a suitable notion of differentiability because, among other things, it does not guarantee the validity of the differentiability of composite functions (see [[2.5 Exercises#^fbb3a9|this exercise]]).

>[!defn] Definition: differentiability of multivariable vector functions
>Let $f\colon D\subseteq \mathbb{R}^{k}\rightarrow \mathbb{R}^{n}$. It is differentiable at $\mathbf{x}_{0}\in D$ if the matrix $\mathbf{D}f(\mathbf{x}_{0})$ of partial derivatives at $\mathbf{x}_{0}$ exists and
>
>$$
>\lim_{\mathbf{x}\rightarrow\mathbf{x}_{0}}\,\frac{||f(\mathbf{x}) - f(\mathbf{x}_{0}) - \mathbf{D}f(\mathbf{x}_{0})(\mathbf{x} - \mathbf{x}_{0})||}{||\mathbf{x} - \mathbf{x}_{0}||} = 0
>$$
>
>where $\mathbf{D}f(\mathbf{x}_{0})(\mathbf{x} - \mathbf{x}_{0})$ denotes the [matrix product](https://en.wikipedia.org/wiki/Matrix_multiplication) between the $(k\times n)$ matrix $\mathbf{D}f(\mathbf{x}_{0})$ and the $(n\times 1)$ matrix $(\mathbf{x} - \mathbf{x}_{0})$.
>
>Given $A\subseteq D$, the function $f$ is said to be differentiable in $A$ if the matrix $\mathbf{D}f$ of partial derivatives exists for all $\mathbf{x}\in A$.
>
>The function $f$ is said to be $C^{1}$ in $A\subseteq D$ if it is differentiable in $A$ and all its partial derivatives $\frac{\partial f^{i}}{\partial x^{j}}$ are continuous in $A$.

^801be3

>[!exmp]  Example
>Let $f\colon \mathbb{R}^{3}\rightarrow \mathbb{R}$ be given by
>$$
>f(x,y,z)=x^{2} + y^{2} + z^{2} .
>$$
>A direct computation shows that
>$$
>\nabla f=(2x, 2y, 2z)
>$$
>which is well-defined for every $\mathbf{x}=(x,y,z)\in\mathbb{R}^{3}$. In order for $f$ to be differentiable at $\mathbf{x}_{0}$, we have to check that
>$$
>\lim_{\mathbf{x}\rightarrow \mathbf{x}_{0}} \frac{x^{2} + y^{2} + z^{2} - x_{0}^{2} - y_{0}^{2} - z_{0}^{2} -2(x - x_{0})x_{0} - 2(y-y_{0})y_{0} - 2(z-z_{0})z_{0}}{\sqrt{(x - x_{0})^{2} + (y - y_{0})^{2} +(z - z_{0})^{2}}} =0 .
>$$
>Some simple algebra shows that, in the left-hand-side of the expression above, the numerator is precisely the square of the denominator so that the limit becomes
>
>$$
>\lim_{\mathbf{x}\rightarrow \mathbf{x}_{0}} \sqrt{(x - x_{0})^{2} + (y - y_{0})^{2} +(z - z_{0})^{2}} =0 ,
>$$
>
>which holds because of the very definition of [[2.2 Limits of multivariable functions|limits of multivariable scalar functions]].

>[!exmp] Example
>Consider the function $f\colon\mathbb{R}^{3}\rightarrow\mathbb{R}^{2}$ given by
>
>$$
>f(x,y,z)=\left(\begin{matrix} x^{2} + y^{2} + z^{2} \\ x + y + z  \end{matrix}\right).
>$$
>
>Following the strategy described in the remark before the previous proposition, the matrix of partial derivatives at $(x_{0},y_{0},z_{0})$ turns out to be the $(2\times 3)$ matrix given by
>
>$$
>Df(x_{0},y_{0},z_{0})=\left(\begin{matrix} \left.\frac{\partial f^{1}}{\partial x}\right|_{(x_{0},y_{0},z_{0})} & \left.\frac{\partial f^{1}}{\partial y}\right|_{(x_{0},y_{0},z_{0})}  & \left.\frac{\partial f^{1}}{\partial z}\right|_{(x_{0},y_{0},z_{0})}  \\ & & \\\left.\frac{\partial f^{2}}{\partial x}\right|_{(x_{0},y_{0},z_{0})}  & \left.\frac{\partial f^{2}}{\partial y}\right|_{(x_{0},y_{0},z_{0})}  & \left.\frac{\partial f^{2}}{\partial z}\right|_{(x_{0},y_{0},z_{0})}  \end{matrix} \right) = \left(\begin{matrix}  2 x & 2y &2z \\ & & \\ 1 & 1 & 1  \end{matrix} \right). 
>$$ 
>
>A direct computation shows that
>
>$$
>Df(\mathbf{x}_{0})(\mathbf{x} - \mathbf{x}_{0})= \left(\begin{matrix} 2x(x-x_{0}) + 2y(y-y_{0}) + 2z(z-z_{0}) \\ 0  \end{matrix}\right),
>$$
>
>which means that
>
>$$
>\begin{split}
> ||f(\mathbf{x}) - f(\mathbf{x}_{0}) - Df(\mathbf{x}_{0})(\mathbf{x} - \mathbf{x}_{0})||&=\left|\left| \left(\begin{matrix} - (x-x_{0})^{2} - (y-y_{0})^{2} - (z-z_{0})^{2}\\ 0 \end{matrix}\right) \right|\right| \\& =||\mathbf{x} - \mathbf{x}_{0}||^{2}.
> \end{split}
>$$
>
>Consequently, the limit
>
>$$
>\lim_{\mathbf{x}\rightarrow\mathbf{x}_{0}}\,\frac{||f(\mathbf{x}) - f(\mathbf{x}_{0}) - Df(\mathbf{x}_{0})(\mathbf{x} - \mathbf{x}_{0})||}{||\mathbf{x} - \mathbf{x}_{0}||}=\lim_{\mathbf{x}\rightarrow\mathbf{x}_{0}}\, ||\mathbf{x} - \mathbf{x}_{0}|| =0
>$$
>
>is satisfied and we conclude that $f$ is differentiable everywhere.

>[!defn] Definition: tangent hyperplane of a multivariable scalar function
>Given $(D,\mathbb{R}^{n},f,\mathbb{R})$ and a point $\mathbf{x}_{0}\in D$ at which all partial derivatives exist, the equation
>$$f(\mathbf{x}_{0})+\mathbf{D}f(\mathbf{x}_{0})(\mathbf{x} - \mathbf{x}_{0})=0$$
defines an hyperplane in $\mathbb{R}^{n}$ called the **tangent hyperplane** to $f$ at $\mathbf{x}_{0}$. When $f$ is differentiable at $\mathbf{x}_{0}$, the tangent hyperplane clearly exists and gives a good approximation to $f$ around $\mathbf{x}_{0}$.


# Properties of derivatives of multivariable vector functions

Some important properties of multivariable vector functions that are differentiable are collected below. Note that when $k=n=1$, the results in this subsection reduce to those we already encounter for [[1.3 Derivatives of scalar functions#^f3bc1e|scalar functions]]. 

>[!important] Proposition
>Let $f\colon A\subseteq \mathbb{R}^{k}\rightarrow \mathbb{R}^{n}$ be differentiable at $\mathbf{x}_{0}$. Then $f$ is continuous at $\mathbf{x}_{0}$.

>[!important] Proposition
>Let $f\colon A\subseteq \mathbb{R}^{k}\rightarrow \mathbb{R}^{n}$ be such that all its partial derivatives exist and are continuous in an open ball centered at $\mathbf{x}_{0}$ with radius $\epsilon>0$. Then $f$ is differentiable at $\mathbf{x}_{0}$.

>[!rem] Remark
>Always remember that:
>
>continuity of partial derivatives $\Longrightarrow$ differentiability $\Longrightarrow$ existence of partial derivatives
>
>while all converse implications are false!

>[!exmp] Example: partial derivatives exist but the function is not differentiable
>Consider the function $f\colon \mathbb{R}^{2}\rightarrow \mathbb{R}$ given by
>$$
>f(x,y)=\sqrt[3]{x\,y} 
>$$
>and the point $(0,0)\in\mathbb{R}$. Following one of the examples discussed above, we obtain
>$$
>\mathbf{D}f(0,0)=\nabla f(0,0)=\mathbf{0}.
>$$
>To check the differentiability of $f$ at $(0,0)$, we should verify the limit
>$$
>\lim_{(x,y)\rightarrow (0,0)}\,\frac{|f(x,y) - f(0,0) - \nabla f(0,0)\cdot\mathbf{x}|}{||\mathbf{x}||}=\lim_{(x,y)\rightarrow (0,0)}\,\frac{\sqrt[3]{xy}}{\sqrt{x^{2} + y^{2}}}=0 .
>$$
>When $y=0$ we obtain
>$$
>\lim_{x\rightarrow 0}\,\frac{|f(x,0) - f(0,0) - \nabla f(0,0)\cdot\mathbf{x}|}{||\mathbf{x}||}=\lim_{x\rightarrow 0}\,0=0 .
>$$
>However, when $y=x^{2}$, we have
>$$
>\lim_{x\rightarrow 0}\,\frac{|f(x,x^{2}) - f(0,0) - \nabla f(0,0)\cdot\mathbf{x}|}{||\mathbf{x}||}=\lim_{x\rightarrow 0}\,\frac{x}{\sqrt{x^{2}+x^{4}}}=\lim_{x\rightarrow 0}\,\frac{x}{|x|\sqrt{1+x^{2}}}=\pm 1 
>$$
>depending on whether $x\rightarrow 0^{+}$ or $x\rightarrow 0^{-}$. Therefore, we must conclude that the function is not differentiable at $(0,0)$.

>[!exmp] Example: a differentiable function whose partial derivatives are not continuous
>Consider the function $f\colon\mathbb{R}\rightarrow\mathbb{R}$ given by
>$$
>f(x)=\left\{\begin{matrix}x^{2}\,\sin\left(\frac{1}{x}\right) & x\neq 0 \\ & \\ 0 & x=0 \; .\end{matrix}\right. 
>$$
>The partial derivatives here collapse to the [[1.3 Derivatives of scalar functions|ordinary derivative]] of one-variable scalar functions which reads
>$$
>f'(x)=\left\{\begin{matrix}2x\,\sin\left(\frac{1}{x}\right) - \cos\left(\frac{1}{x}\right) & x\neq 0 \\ & \\ 0 & x=0 \; .\end{matrix}\right. 
>$$
>Clearly, $f'(x)$ is not continuous at $x=0$ because $\cos\left(\frac{1}{x}\right)$ has no limit for $x\rightarrow 0$. However, it is immediate to check that the limit
>$$
>\lim_{x\rightarrow 0}\,\frac{|f(x)-f(0) - f'(0)x|}{|x|}=\lim_{x\rightarrow 0}\,\left|x\sin\left(\frac{1}{x}\right)\right|= 0
>$$
>holds because trigonometric functions are bounded. We thus conclude that $f$ is differentiable at $x=0$ although its partial derivatives are not continuous there.

^bed62d

>[!Important] Proposition (Sums, products, quotients)
>1) **Sum Rule**: let $f,g \colon D\subseteq\mathbb{R}^{k}\rightarrow\mathbb{R}^{n}$  be differentiable at $\mathbf{x}_{0}$ and let $a,b\in\mathbb{R}$. Then $h(\mathbf{x}) = a f (\mathbf{x}) + b g (\mathbf{x})$ is differentiable at $\mathbf{x}_{0}$ and $\mathbf{D}h(\mathbf{x}_{0}) = a\mathbf{D}f (\mathbf{x}_{0} ) + b\mathbf{D}g (\mathbf{x}_{0} )$ (sum of matrices).
>2) **Product Rule**: let et $f,g \colon D\subseteq\mathbb{R}^{k}\rightarrow\mathbb{R}$  be differentiable at $\mathbf{x}_{0}$. Then $h(\mathbf{x}) = f (\mathbf{x})\,g (\mathbf{x})$ is differentiable at $\mathbf{x}_{0}$ and $\mathbf{D}h(\mathbf{x}_{0}) = g (\mathbf{x}_{0} )\,\mathbf{D}f (\mathbf{x}_{0} ) + f (\mathbf{x}_{0} )\,\mathbf{D}g (\mathbf{x}_{0} )$.
>3) **Quotient Rule**: with the same hypotheses as in the product rule above, let  $h(\mathbf{x}) = \frac{f (\mathbf{x})}{g (\mathbf{x})}$ and suppose $g$ is never $0$ on $D$. Then $h$ is differentiable at $\mathbf{x}_{0}$ and 
>
>$$
>\mathbf{D}h(\mathbf{x}_{0})=\frac{g(\mathbf{x}_{0})\,\mathbf{D}f(\mathbf{x}_{0})-f(\mathbf{x}_{0})\,\mathbf{D}g(\mathbf{x}_{0} )}{(g(\mathbf{x}_{0}))^{2}}.
>$$
>

>[!Important] Proposition (Chain rule)
>Let $D\subset\mathbb{R}^{k}$ and $D'\subseteq\mathbb{R}^{m}$ be open sets. Let $g\colon\mathbb{R}^{k}\rightarrow\mathbb{R}^{m}$ and $f\colon\mathbb{R}^{m}\rightarrow\mathbb{R}^{n}$ be given functions such that $g(D)\subseteq D'$, so that $f \circ g$ is defined. Suppose $g$ is differentiable at $\mathbf{x}_{0}$ and $f$ is differentiable at $\mathbf{y}_{0} = g(\mathbf{x}_{0} )$. Then $f \circ g$ is differentiable at $\mathbf{x}_{0}$ and
>
>$$
>\mathbf{D}( f\circ g)(\mathbf{x}_{0} ) = \mathbf{D}f (\mathbf{y}_{0})\cdot \mathbf{D}g(\mathbf{x}_{0})
>$$
>
>where $\cdot$ in the right-hand side is the matrix product of $\mathbf{D}f (\mathbf{y}_{0})$ with $\mathbf{D}g(\mathbf{x}_{0})$.

^9ceaa7

>[!exmp] Example: chain rule
>Let us consider the function $f\colon \mathbb{R}^{3}\rightarrow \mathbb{R}$ given by 
>$$
>f(u,v,w)=u^{2} + v^{2} -w,
>$$
>and the function $g\colon\mathbb{R}^{3}\rightarrow\mathbb{R}^{3}$ given by
>$$
>g(x,y,z)=\left(\begin{matrix} x^{2}y \\ y^{2} \\ \mathrm{e}^{-xz}\end{matrix}\right).
>$$
>We want to verify the chain rule. At this purpose, we first compute 
>$$
>\mathbf{D}f(u,v,z)=\nabla f(u,v,z)=\left(2u, 2v, -1\right),
>$$
>and
>$$
>\mathbf{D}g(x,y,z)=\left(\begin{matrix}2xy & x^{2} & 0 \\ 0 & 2y & 0 \\ -z\mathrm{e}^{xz} & 0 & -x\mathrm{e}^{-xz}  \end{matrix}\right),
>$$
>so that
>$$
>\begin{split}
>\mathbf{D}f(u(x,y,z),v(x,y,z),w(x,y,z))\cdot \mathbf{D}g(x,y,z)&=\left(2u(x,y,z), 2v(x,y,z), -1\right)\cdot\left(\begin{matrix}2xy & x^{2} & 0 \\ 0 & 2y & 0 \\ -z\mathrm{e}^{xz} & 0 & -x\mathrm{e}^{-xz}  \end{matrix}\right)= \\ & \\ & = \left(2x^{2}y, 2y^{2}, -1\right)\cdot\left(\begin{matrix}2xy & x^{2} & 0 \\ 0 & 2y & 0 \\ -z\mathrm{e}^{xz} & 0 & -x\mathrm{e}^{-xz}  \end{matrix}\right) = \\ & \\ & = 4x^{3}y^{2} + z\mathrm{e}^{-xz},\;2x^{4}y + 4y^{3}, x\mathrm{e}^{-xz} .
>\end{split}
>$$
>Then, we compute
>$$
>f\circ g(x,y,z)= f(u(x,y,z),v(x,y,z),w(x,y,z))=f(x^{2}y, y^{2}, \mathrm{e}^{-xz})= x^{4}y^{2} + y^{4} - \mathrm{e}^{-xz}
>$$
>so that
>$$
>\mathbf{D}(f\circ g)(x,y,z)=\nabla (f\circ g)(x,y,z)=\left(4x^{3}y^{2} + z\mathrm{e}^{-xz},\;2x^{4}y + 4y^{3}, x\mathrm{e}^{-xz}\right),
>$$
>and we immediately see that the chain rule holds as expected.


# Second order partial derivatives

^1b4383

Here, we will focus on multivariable scalar functions, that is, functions whose input space is $\mathbb{R}^{n}$ and whose output space is $\mathbb{R}$.

We are interested in understanding the multivariable analogue of the second derivative of a scalar function.

Consider $f\colon D\subseteq \mathbb{R}^{n}\rightarrow \mathbb{R}$ with partial derivatives $\frac{\partial f}{\partial x^{j}}$, with $j=1,...,n$. These partial derivatives can be arranged into a vector.


>[!defn] Definition: the gradient vector
>Given $f\colon D\subseteq \mathbb{R}^{n}\rightarrow \mathbb{R}$, we define the **gradient vector** $\nabla f(\mathbf{x}_{0})$ by setting
>$$ \nabla f (\mathbf{x}_{0})=\left(\left.\frac{\partial f}{\partial x^{1}}\right|_{\mathbf{x}_{0}},\cdots,\left.\frac{\partial f}{\partial x^{n}}\right|_{\mathbf{x}_{0}}\right).$$
>Note that the gradient vector defines a multivariable vector function $\nabla f\colon A\subseteq\mathbb{R}^{n}\rightarrow\mathbb{R}^{n}$.

Every $\frac{\partial f}{\partial x^{j}}$ is a multivariable scalar function, and we can thus compute its derivative with respect to any of $x^{1},\cdots,x^{n}$. Specifically, given $f\colon D\subseteq \mathbb{R}^{n}\rightarrow \mathbb{R}$, we can define its $(i,j)$-th second partial derivative at $\mathbf{x}_{0}$ as
$$
\left. f_{ij}\right|_{\mathbf{x}_{0}}\equiv\left.\frac{\partial^{2} f}{\partial x^{i}\partial x^{j}}\right|_{\mathbf{x}_{0}}:=\left(\frac{\partial}{\partial x^{i}}\left(\frac{\partial f}{\partial x^{j}}\right)\right)_{\mathbf{x}_{0}},
$$

which means taking the $i$-th partial derivative of the $j$-th partial derivative function $f_{j}\equiv \frac{\partial f}{\partial x^{j}}$. 

>[!defn] Definition: Hessian matrix
>Let $f\colon D\subseteq\mathbb{R}^{n}\rightarrow \mathbb{R}$ have all second partial derivatives at $\mathbf{x}_{0}$. The matrix
>$$
>Hf_{\mathbf{x}_{0}}=\left(\begin{matrix}f_{11}|_{\mathbf{x}_{0}}& \cdots & f_{1n}|_{\mathbf{x}_{0}} \\ & & \\ \vdots & &\vdots \\ & & \\ f_{n1}|_{\mathbf{x}_{0}} & \cdots & f_{nn}|_{\mathbf{x}_{0}}\end{matrix}\right)
>$$
> of second partial derivatives at $\mathbf{x}_{0}\in A$ is called the **Hessian matrix** of $f$ at $\mathbf{x}_{0}\in A$. 

In principle, the Hessian matrix is not symmetric because, for $i\neq j$, it may happen that
$$
\left.\frac{\partial^{2} f}{\partial x^{i}\partial x^{j}}\right|_{\mathbf{x}_{0}}\neq \left.\frac{\partial^{2} f}{\partial x^{j}\partial x^{i}}\right|_{\mathbf{x}_{0}} ,
$$
that is, the mixed partial derivatives are different.

>[!defn] Definition: $C^{2}$ functions
>If all the possible second order partial derivatives of the function $f\colon D\subseteq \mathbb{R}^{n}\rightarrow \mathbb{R}$ exist and are continuous on the open set $A\subseteq D$, then $f$ is said to be $C^{2}$ on $A$.

>[!defn] Proposition: equality of mixed partial derivatives
>If $f\colon D\subseteq \mathbb{R}^{n}\rightarrow \mathbb{R}$ is $C^{2}$ on the open set $A\subseteq D$ then 
>$$
>\frac{\partial^{2} f}{\partial x^{i}\partial x^{j}}=f_{ij}=f_{ji}=\frac{\partial^{2} f}{\partial x^{j}\partial x^{i}}
>$$
>for all $i,j$ and for all $\mathbf{x}\in\mathbf{A}$. That is, mixed second partial derivatives are equal.

Because of the proposition above, the Hessian matrix of a $C^{2}$ function is always a [symmetric matrix](https://en.wikipedia.org/wiki/Symmetric_matrix).

Once we have the matrix of second partial derivatives, we can take the partial derivatives of each element in the matrix, and obtain an object (which is neither a vector nor a matrix) encoding information on third order derivatives, and so on.

>[!defn] Definition: $C^{k}$ functions
>If all the possible $k$-th order partial derivatives of the function $f\colon D\subseteq \mathbb{R}^{n}\rightarrow \mathbb{R}$ exist and are continuous on the open set $A\subseteq D$, then $f$ is said to be $C^{k}$ on $A$.

Note that, for $f\colon D\subseteq \mathbb{R}^{n}\rightarrow\mathbb{R}$, we get $n^{k}$ partial derivatives of order $k$, and thus number of computations becomes quickly unmanageable.

# Second order Taylor's theorem

Just as in the [[1.5 Power series, trascendental functions, and Taylor series#^113a5b|one-variable case]], derivatives help in getting a good approximation of a given function. The accuracy of such an approximation is governed by Taylor's theorem. For simplicity, we focus on the second order version of the theorem.

>[!important] Theorem: second order Taylor's theorem
>Let $f\colon D\subseteq \mathbb{R}^{n}\rightarrow\mathbb{R}$ be of class $C^{3}$ on the open set $A\subseteq D$ containing $x_{0}$. Then, we have
>$$
>f(\mathbf{x})=f(\mathbf{x}_{0})+ \sum\limits_{j=1}^{n}\left.\frac{\partial f}{\partial x^{j}}\right|_{\mathbf{x}_{0}}(x^{j}-x_{0}^{j}) + \frac{1}{2}\sum\limits_{j,k=1}^{n}\left.\frac{\partial^{2} f}{\partial x^{j}\partial x^{k}}\right|_{\mathbf{x}_{0}}\,(x^{j}-x_{0}^{j})\,(x^{k}-x_{0}^{k}) + R_{2}(\mathbf{x},\mathbf{x}_{0}),
>$$
>where the function $R_{2}(\mathbf{x},\mathbf{x}_{0})$ is such that
>$$\lim_{\mathbf{x}\rightarrow\mathbf{x}_{0}}\,\frac{R_{2}(\mathbf{x},\mathbf{x}_{0})}{\Vert \mathbf{x}-\mathbf{x}_{0}\Vert^{2}}=0,$$
>and it's called the **second order remainder**.

Exploiting matrix notation, Taylor's formula can be also written as
$$
f(\mathbf{x})=f(\mathbf{x}_{0}) + \nabla f|_{\mathbf{x}_{0}}\cdot\,(\mathbf{x}-\mathbf{x}_{0}) + \frac{1}{2}(\mathbf{x}-\mathbf{x}_{0})^{T}\,Hf|_{\mathbf{x}_{0}}\,(\mathbf{x}-\mathbf{x}_{0}),
$$
where $\nabla f$ is the gradient vector of $f$, $Hf$ is the Hessian matrix of $f$, and $v^{T}$ denote the transpose of a vector/matrix.

>[!important] Form of the 2-nd order remainder
>In the hypothesis of the 2-nd order Taylor's theorem, the remainder $R_{2}(\mathbf{x},\mathbf{x}_{0})$ can be written as:
>$$ \sum_{j,k,l=1}^{n}\frac{1}{3!}\,\frac{\partial^{3} f}{\partial x^{j}\partial x^{k}\partial x^{l}}(c_{jkl})(x^{j}-x^{j}_{0})(x^{k}-x^{k}_{0})(x^{l}-x^{l}_{0}),$$
>where each $c_{jkl}$ lies somewhere on the line joining $\mathbf{x}_{0}$ to $\mathbf{x}$.



