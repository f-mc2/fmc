---
dg-home: false
dg-publish: true
---
# Power series

Once we understand that it is possible to make sense of the sum of infinite numbers using numerical series, nothing prevents us in trying to develop similar ideas for the sum of an infinite number of polynomials, thus arriving at the notion of **power series**.

>[!defn] Definition: power series
>A power series is a series whose elements are polynomial functions:
>$$
>P(x,x_{0})=\sum_{n=0}^{+\infty} \;a_{n}\,(x-x_{0})^{n}
>$$
>where $x,x_{0}\in\mathbb{R}$ and $\{a_{n}\}_{n\in\mathbb{N}}$ is a numerical sequence.

Essentially, for any fixed choice of $x,x_{0}\in \mathbb{R}$, we obtain a numerical series. However, since $x$ may vary, it makes sense to look at $P(x,x_{0})$ as a kind of function (depending on the parameter $x_{0}$). This point of view turns out to be particularly useful because it leads to the introduction of new interesting functions.

We use the  [[1.4 Sequences and series#^3a9a60|root test]] to obtain that the power series $P(x,x_{0})$ is [[1.4 Sequences and series#^32251b|absolutely convergent]] if

$$
\lim_{n\rightarrow \infty}\sqrt[n]{|a_{n}|\,|(x-x_{0})^{n}|}=|x-x_{0}|\,\left(\lim_{n\rightarrow + \infty} \sqrt[n]{|a_{n}|}\right) < 1 .
$$

The case where the limit is precisely $1$ must be handled with a case-by-case analysis.

The extended real number $\rho\geq 0$ given by

$$
\frac{1}{\rho}:=\lim_{n\rightarrow +\infty} \sqrt[n]{|a_{n}|}
$$

is then called the **convergence radius** of the power series because, when the power series is absolutely convergent according to the root test above, then $|x-x_{0}|< \rho$.

If a power series converges in $I=(x_{0}-\rho, x_{0}+\rho)$ then we can define the scalar function $(I,\mathbb{R},f,\mathbb{R})$ with

$$
f(x)=\sum_{n=0}^{+\infty}a_{n}\,(x-x_{0})^{n} .
$$

Unless $a_{n}=0$ for every $n>N$ with $N$ fixed (the polynomial case), the scalar function defined above is a **trascendental function** (we can not compute its value at $x$ using only algebraic operations).

Functions defined through a power series enjoy a lot of interesting properties.

>[!important] Proposition
>Let $x_{0},\rho\in\mathbb{R}$, with $\rho>0$, and consider the function $(I,\mathbb{R},f,\mathbb{R})$, where $I=(x_{0}-\rho,x_{0}+\rho)$, and $f(x)$ is defined through the power series
>$$
>f(x)=\sum_{n=0}^{+\infty}a_{n}\,(x-x_{0})^{n}
>$$
>with radius of convergence $\rho$. This function is [[1.3 Derivatives of scalar functions#^a6b2f2|smooth]], that is, it has  infinitely many continuous derivatives in $I$ given by
>$$
>\begin{split}
>f^{(k)}(x)&=\sum_{n=0}^{+\infty}\frac{\mathrm{d}^{k}}{\mathrm{d}x^{k}}\,\left(a_{n}\,(x-a)^{n}\right)= \\ & = \sum_{n=k}^{+\infty}n(n-1)\cdots(n-k+1)\,\left(a_{n}\,(x-a)^{n-k}\right).
>\end{split}
>$$
>In particular, any such function $f$ is continuous.

# The exponential and logarithm functions

The discussion in this section follows chapter 8 of ["Rudin - Principle of Mathematical Analysis (third edition)"](https://en.wikipedia.org/wiki/Principles_of_Mathematical_Analysis), which contains all the proofs of the results recalled here.

The convergence radius of the power series
$$
E(x,x_{0})=\sum\limits_{k=0}^{+\infty} \frac{(x-x_{0})^{k}}{k!}
$$
[[1.7 Exercises#^98a973|satisfies]]
$$
\frac{1}{\rho}=\lim_{k\rightarrow+\infty}\frac{|x-a|}{\sqrt[k]{k!}} =0,
$$
which means that the power series converges for all $x\in\mathbb{R}$.  The smooth function defined by $E(x,0)$ is called the exponential function and is denoted by
$$
\mathrm{exp}(x):=E(x,0)=\sum\limits_{k=0}^{+\infty} \frac{x^{k}}{k!}.
$$
The value $\mathrm{exp}(1)$ is called the **Euler number**, and it is denoted by $\mathrm{e}$. It is also customary to write $\mathrm{e}^{x}\equiv\mathrm{exp}(x)$.

>[!important] Proposition: the exponential function
>The exponential function $\mathrm{exp}\colon \mathbb{R}\rightarrow \mathbb{R}$ satisfies the following properties:
>1) $\mathrm{exp}(x)'=\mathrm{exp}(x)$ for all $x\in\mathbb{R}$;
>2) $\mathrm{exp}(a+b)=\mathrm{exp}(a)\,\mathrm{exp}(b)$ for all $a,b\in\mathbb{R}$ (in particular, $\mathrm{exp}(0)=1$, and $\mathrm{exp}(-x)=(\mathrm{exp}(x))^{-1}$);
>3) $\mathrm{exp}(x)>0$ for all $x\in\mathbb{R}$;
>4) it is bijective (injective and surjective);
>5) $\lim_{x\rightarrow+\infty}\,\mathrm{exp}(x)=+\infty$;
>6)  $\lim_{x\rightarrow-\infty}\,\mathrm{exp}(x)=0$.

Because of property 3 of the exponential function, we have
$$
E(x,x_{0})=\mathrm{exp}(x-x_{0})=\mathrm{exp}(x)\,\mathrm{exp}(-x_{0}),
$$
which means that $\mathrm{exp}(x)$ is all we really need to discuss all the functions of the form $E(x,x_{0})$.

>[!important] Proposition: the logarithm function
>The inverse function of $\mathrm{exp}(x)\equiv\mathrm{e}^{x}$ is called the **logarithm function** and it is denoted by $\ln(x)$. Since $\mathrm{exp}(x)>0$, the domain of the logarithm function is $(0,\infty)$. The logarithm function satisfies the following properties:
>1) $\ln(x)<0$ for $x\in(0,1)$, $\ln(1)=0$, and $\ln(x)>0$ for $x>1$;
>2) $\ln(ab)=\ln(a) + \ln(b)$ for all $a,b>0$ (in particular, $\ln(x^{n})=n\ln(x)$ for all $n\in\mathbb{N}$);  
>3) $\ln(\frac{a}{b})=\ln(a)-\ln(b)$ for all $a,b>0$;
>4) $\ln(x)'=\frac{1}{x}$ for all $x>0$ (using the [[1.3 Derivatives of scalar functions#^05d051|the rule for the derivative of the inverse]]), which means that $\ln(x)$ is infinitely differentiable;
>5) it is (obviously) invertible, and its inverse is (obviously) the exponential function;
>6) $\lim_{x\rightarrow 0^{+}}\,\ln(x)=-\infty$, and $\lim_{x\rightarrow+\infty}\,f(x)=x$.

Using the exponential and logarithm functions, given $a>0$, we define the function
$$
a^{x}:=\mathrm{e}^{x\ln(a)}.
$$
Clearly, for $a=1$, we obtain a constant function. Moreover, $a^{x}$ is well defined for all $x\in\mathbb{R}$, and $a^{x}>0$ for every $x\in\mathbb{R}$. Therefore, we obtain
$$
\ln(a^{b})=\ln(\mathrm{e}^{b\ln(a)})=b\ln(a), 
$$
which is a sort of generalization of the particular case discussed in property 2 of the logarithm function.

# The trigonometric functions

The power series 
$$
\begin{split}
S(x,x_{0})& =\sum\limits_{k=0}^{+\infty} \frac{(-1)^{k}\, (x-x_{0})^{2k+1}}{(2k+1)!} \\ & \\
C(x,x_{0})& =\sum\limits_{k=0}^{+\infty} \frac{(-1)^{k} (x-x_{0})^{2k}}{(2k)!} 
\end{split}
$$
(absolutely) converge for every $x,x_{0}\in\mathbb{R}$ because of the [[1.4 Sequences and series#^b44184|ratio test]]. Consequently, the radius of convergence is infinite. Setting $x_{0}=0$, we obtain two smooth functions denoted by
$$
\begin{split}
\sin(x) & := \sum\limits_{k=0}^{+\infty} \frac{(-1)^{k}\,x^{2k+1}}{(2k+1)!} \\ & \\
\cos(x) & := \sum\limits_{k=0}^{+\infty} \frac{(-1)^{k}\,x^{2k}}{(2k)!},
\end{split}
$$
and referred to as *sine* and *cosine* function, respectively. These functions are the building block of the so-called *trigonometric functions*.

>[!important] Proposition: sine and cosine functions
>Let $x_{0}=0$. The *sine* and *cosine* function defined above satisfy the following properties:
>1) $\sin(x+2k\pi)= \sin(x)$,  $\cos(x)=\cos(x + 2k\pi)$ for all $x\in\mathbb{R}$, and all $k\in\mathbb{Z}$ (the functions are **periodic** with period $2\pi$);
>2) $\sin(-x)=-\sin(x)$, and $\cos(-x)=\cos(x)$ for all $x\in\mathbb{R}$;
>3) $\cos(x)=\sin(x + \pi/2)$ for all $x\in\mathbb{R}$;
>4) $\sin(0)=\sin(\pi)=0$, and $\sin(\pi/2)=1$ (using property 3 we obtain the values of $\cos$ for the same input numbers);
>5) $\sin(x)'=\cos(x)$ and $\cos(x)'=-\sin(x)$ for all $x\in\mathbb{R}$;
>6) $\sin^{2}(x) + \cos^{2}(x)=1$ for all $x\in\mathbb{R}$;
>7) the inverse function of $\sin(x)$ is defined for $\frac{-\pi}{2}\leq x\leq \frac{\pi}{2}$ and is denoted as $\arcsin(x)$;
>8) the inverse function of $\cos(x)$ is defined for $0\leq x\leq \pi$ and is denoted as $\arccos(x)$;
>9) both $\sin(x)$ and $\cos(x)$ do not admit limits at $\pm\infty$ because they oscillate.

>[!exmp] Example
>Now that we know how to compute the derivative of $\sin(x)$ in a way that does not make direct use of the definition of derivative, we can apply L'HÃ´pital's to solve the limit we could not solve like this [[1.3 Derivatives of scalar functions#^77ad27|before]].

^ed7011

Since they are periodic, both $\sin(x)$ and $\cos(x)$ are not invertible in their maximal domain. However, if we fix $-\frac{\pi}{2}<x<\frac{\pi}{2}$, then both functions are invertible (because bijective), and their inverse functions are denoted by $\arcsin(x)$ and $\arccos(x)$, respectively. Note that the maximal domain of these inverse trigonometric functions is $(-1,1)$. Moreover, again because of periodicity, the choice $-\frac{\pi}{2}<x<\frac{\pi}{2}$ is not unique, and we could select different intervals in which $\sin(x)$ and $\cos(x)$ are bijective. 

Starting from $\sin(x)$ and $\cos(x)$, we define the *tangent* and *cotangent* function as
$$
\tan(x):=\frac{\sin(x)}{\cos(x)}\,\qquad \cot(x):=\frac{\cos(x)}{\sin(x)}=\frac{1}{\tan(x)},
$$
respectively. Note that both $\tan(x)$ and $\cot(x)$ are not defined on the whole real linea because there are infinitely many points at which $\sin(x)$ or $\cos(x)$ are $0$. Moreover, both $\tan(x)$ and $\cot(x)$ are periodic functions of period $\pi$ (half of $\sin(x)$ and $\cos(x)$).

It turns out that $\tan(x)$ is invertible for $\frac{\pi}{2}\leq x\leq \frac{\pi}{2}$, and its inverse function is denoted by $\arctan(x)$. Similarly, $\cot(x)$ is invertible in $0\leq x\leq \pi$, and its inverse function is denoted by $\mathrm{arccot}(x)$.

Additional information on trigonometric functions, especially in relation to their geometrical interpretation, can be found in [this Wikipedia page](https://en.wikipedia.org/wiki/Trigonometric_functions), while additional (and often useful) trigonometric identities can be found in [this Wikipedia page](https://en.wikipedia.org/wiki/List_of_trigonometric_identities).

# The Taylor series of a scalar function

A particular type of power series is the so-called **Taylor series** associated with a particular scalar function $f$.

>[!note] Definition: Taylor series 
>Let $f$ be a scalar function having continuous derivatives of any order in an interval centered at the point $x_{0}$. The power series
>$$
>\sum_{n=0}^{+\infty}\,a_{n}\;(x-x_{0})^{n}=\sum_{n=0}^{+\infty}\,\frac{f^{(n)}(x_{0})}{n!}\;(x-x_{0})^{n} 
>$$ 
>is called the **Taylor series** of $f$ centered at $x_{0}$.

>[!rem] Remark
>Computing a closed-form expression for the Taylor series of a given function is not an easy task! 

The Taylor series of a function provides a suitable way to approximate the function itself.

>[!important] Theorem: Taylor's theorem
>Let $f$ be a scalar function which is $(k+1)$-times differentiable in an open interval $I$ containing $x_{0}$. Then, for every $x\in I$, it holds
>$$
>f(x)=\sum_{n=0}^{k}\frac{f^{(n)}(x_{0})}{n!}\,(x-x_{0})^{n} + o((x-x_{0})^{k}),
>$$
>where the function $o((x-x_{0})^{k+1})$ is defined in $I$, it is called the **remainder**, and satisfies
>$$ \lim_{x\rightarrow x_{0}} \frac{o((x-x_{0})^{k})}{(x-x_{0})^{m}}=0$$
>for all $m\leq k$.
>An explicit form for the remainder, called the **Lagrange form**, is
>$$o((x-x_{0})^{k})= \frac{f^{(k+1)}((1-\theta)x_{0} + \theta x)}{(k+1)!}\,(x-x_{0})^{k+1}, $$
>where $\theta \in (0,1)$.

^113a5b
A visualization of how Taylor's approximation works ([source](https://www.geogebra.org/m/F7h24XKR))
<iframe
  id="inlineFrameExample"
  title="Inline Frame Example"
  width="800"
  height="500"
  src="https://www.geogebra.org/m/uzzpq6hk">
</iframe>

Now, a question. If the **Taylor series** of a function converges in $I=(x_{0}-\rho,x_{0}+\rho)$, can we say it must converge to the original function?

Looking at Taylor's theorem above, we may be tempted to say the answer to the previous question is positive, but we would be wrong! 

>[!exmp] Example
>The prototypical example of smooth (infinitely differentiable) function which is non-analytic and has a convergent Taylor series is the function $(\mathbb{R},\mathbb{R},f,\mathbb{R})$ given by
>$$ f(x)=\left\{\begin{matrix} \mathrm{e}^{-\frac{1}{x}} &\quad x>0 \\ 0 & \quad x\leq 0 . \end{matrix}\right. $$
>
> A [non-trivial computation](https://en.wikipedia.org/wiki/Non-analytic_smooth_function#The_function_is_smooth) shows that the Taylor series of $f$ around $x=0$ is made of terms that are all equal to $0$ so that it always converges to the zero function which is clearly different from $f$.

>[!defn] Definition: analytic function
>If the Taylor series of a function converges to the function itself in an open interval $I$, then the original function is called **analytic** in $I$. 

# Taylor series and limits

One useful exploitation of Taylor series is connected with the indeterminate form $\frac{0}{0}$ in evaluating limits. 

Let us consider the limit
$$ \lim_{x\rightarrow 0}\; \frac{f(x)}{g(x)} \,=\,\lim_{x\rightarrow 0} \;\frac{1-\cos^{2}(x)}{x^{2}} .$$
The second order  Taylor expansion for $\cos^{2}(x)$ reads
$$ \cos^{2}(x)= 1 -x^{2} + o(x^{2}) $$
so that
$$\lim_{x\rightarrow 0} \;\frac{1-\cos^{2}(x)}{x^{2}} =\lim_{x\rightarrow 0}\frac{1-1+x^{2} - o(x^{3})}{x^{2}} = \lim_{x\rightarrow 0}\left(\frac{x^{2} }{x^{2}} - \frac{o(x^{3})}{x^{2}} \right) = 1 $$
because of the property of the remainder.


