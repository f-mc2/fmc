---
dg-home: false
dg-publish: true
---
# Anti-derivatives of scalar functions

The anti-derivative (or primitive) of a scalar function is a sort of algebraic inverse of the derivative.

>[!defn] Definition: anti-derivative function
>Let $([a,b],\mathbb{R},f, \mathbb{R})$ be a continuous function. A continuous function $( [a,b],\mathbb{R},F, \mathbb{R})$ is called an **anti-derivative** (or **primitive**) function for $f$ if it is differentiable in $(a,b)$ and it satisfies
>$$ F'(x)=f(x) $$
>for every $x\in (a,b)$.

>[!rem] Remark
>It can be proved that every continuous function $([a,b],\mathbb{R},f, \mathbb{R})$ admits an anti-derivative. This anti-derivative is not unique, but the difference between any two anti-derivatives is a constant. Specifically, if $([a,b],\mathbb{R},F, \mathbb{R})$ and $([a,b],\mathbb{R},G, \mathbb{R})$ are anti-derivatives of $f$ then there exists $c\in\mathbb{R}$ for which
>$$ G(x)=F(x) +c$$
>for all $x\in (a,b)$.

The act of taking anti-derivatives is depicted as
$$
f\mapsto \; \int f\,\mathrm{d}x ,
$$
where it is implicitly assumed that $x$ is the variable upon which $f$ depends.

Given the intimate link between anti-derivatives and definite integrals we will discuss below, the act of taking the anti-derivative is also referred to as **indefinite integration**.

The operation of taking anti-derivatives satisfies a linearity property that essentially follows from the one we already saw for [[1.3 Derivatives of scalar functions#^5d20ad|derivatives]].

>[!important] Proposition
>Let $f,g\colon I\subseteq \mathbb{R}\rightarrow \mathbb{R}$ be continuous functions and $a,b\in\mathbb{R}$. Then, it follows
>$$
>\int(a\,f(x) + b\,g(x))\,\mathrm{dx}=a\,\int f(x)\,\mathrm{dx}+ b\,\int g(x)\,\mathrm{dx}.
>$$

>[!exmp] Example
>Consider the function $f\colon \mathbb{R}\rightarrow\mathbb{R}$ given by
>$$
>f(x)= a_{n}x^{n} + \cdots a_{1}x + a_{0} .
>$$
>Recalling the linearity property of the anti-derivative operation and the derivative of a polynomial we immediately get 
>$$
>\int f(x) \,\mathrm{d}x =\frac{a_{n}}{n}x^{n+1}\cdots\frac{a_{1}}{2}x^{2} + a_{0}x + c ,
>$$
>where $c\in \mathbb{R}$ characterizes all possible anti-derivatives of $f$.

We can also exploit other properties of derivatives in order to obtain their counterparts for anti-derivatives, as it's done below.

Let us end this section noting that computing antiderivatives is a craft that requires a lot of practice and a lot of study. There are a lot of different techniques for a lot of different problems, and it is impossible for us to discuss even a little fraction of these methods, given the time at our disposal. However, for those willing to explore this beautiful corner of the mathematical landscape, I suggest starting from chapter 8 of this [book](https://newdev.wileyplus.com/math-and-statistics/salas-calculus-one-and-several-variables-10e-eprof01680/).

# Change of variables

^93db8a

From the [[1.3 Derivatives of scalar functions#^c87d27|chain rule]] we obtain
$$
 \int f(g(x))\,g'(x)\,\mathrm{d}x = f(g(x)) + c ,
$$
which immediately leads to 
$$
\begin{split}
1) \qquad & \int \frac{f'(x)}{f(x)}\,\mathrm{d}x = \ln(|g(x)|) + c \\ & \\
2) \qquad & \int f'(x) (f(x))^{\alpha}\,\mathrm{d}x = \frac{(f(x))^{\alpha +1}}{\alpha + 1}  + c\qquad (\alpha\neq 1) \\ & \\
3) \qquad & \int \frac{f'(x)}{1 + (f(x))^{2}}\,\mathrm{d}x = \arctan((f(x))  + c  \\ & \\
4) \qquad & \int \frac{f'(x)}{\sqrt{1 - (f(x))^{2}}}\,\mathrm{d}x = \arcsin((f(x))  + c  .
\end{split}
$$

We can also look at this application of the chain rule from a slightly different perspective. Specifically, instead of looking at the composite function $f\circ g$ from scratch, we look at the function $F$ which is an anti-derivative of $f$:
$$
F(x) = \int f(x) \mathrm{d}x \quad \Longleftrightarrow\quad F'(x) =f(x) .
$$
Then, we want to investigate what happens if we apply a change of variable through the function $g$. Clearly, we have 
$$
F(x) \quad \Rightarrow \quad F(x(t))=F(g(t))=(F\circ g)(t),
$$
so that the chain rule implies
$$
(F\circ g)'(t)=(F'\circ g)(t)\;g'(t) = (f\circ g)(t)\;g'(t).
$$
Consequently, since taking the anti-derivative is a kind of inverse of taking the derivative, it also holds
$$
F(x(t))=(F\circ g)(t)=\int (f\circ g)(t) \;g'(t)\mathrm{d}t .
$$

Therefore, we can either take the anti-derivative of $f(x)$ with respect to $x$ directly, or first take the anti-derivative of the composite function $(f\circ g)(t)\,g'(t)$ with respect to the auxiliary variable $t$ and then write the result in terms of $x$ by exploiting $t=g^{-1}(x)$. This procedure is known as the change of variable for anti-derivatives, and it is often written as
$$
\int f(x)\;\mathrm{d}x = \left[\int (f\circ g)(t)\;g'(t)\,\mathrm{d}t\right]_{t=g^{-1}(x)},
$$
where the last change of variable $t\mapsto t=g^{-1}(x)$ for the right-hand-side is often dropped to simplify notation, at the expense of clarity and rigour, unfortunately implicitly considered. The change of variable technique is particularly useful to find anti-derivative, but, unfortunately, there is no general rule to build the change of variable $x=g(t)$. Only experience can help you build your intuition.

>[!exmp] Example
>Let us compute the anti-derivative
>$$
>\int \frac{1}{\sqrt[3]{(1-2x)^{2}} - \sqrt{1-2x}}\,\mathrm{d}x .
>$$
>It is clear that the dependence from $x$ is only through the function $1-2x$. Moreover, this function appears either to the power $\frac{3}{2}$ or to the power $\frac{1}{2}$. Therefore, we guess it would be useful to introduce an auxiliary variable of the type $t^{m}=1-2x$, where $m$ takes into account the fact that $1-2x$ appears always elevated to some power. What specific value of $m$ should we take? Every number that makes all the roots disappear would be a wise choice. For instance, we can take $m=6$ so that 
>$$
>\frac{1}{\sqrt[3]{(1-2x)^{2}} - \sqrt{1-2x}}\quad\mapsto\quad \frac{1}{\sqrt[3]{(t^{6})^{2}} - \sqrt{t^{6}}} =\frac{1}{t^{4} - t^{3}},
>$$
>but even $m=12$ would be fine. At this point, we determined the function $t(x)=g^{-1}(x)=\sqrt[6]{1-2x}$ because we defined $t$ in terms of $x$. However, we note that
>$$
>t^{6} = 1-2x \quad \Longrightarrow \quad t^{6} - 1 = -2x\quad\Longrightarrow \quad \frac{1-t^{6}}{2}= x,
>$$
>and  we obtain the change of variable function $x=g(t)=\frac{1-t^{6}}{2}$ with its derivative $g'(t)=-3t^{5}$. Therefore, applying the change of variable formula we get
>$$
>\int \frac{1}{\sqrt[3]{(1-2x)^{2}} - \sqrt{1-2x}}\,\mathrm{d}t = \int \frac{-3t^{5}}{t^{4} - t^{3}}\,\mathrm{d}t= -3\int \frac{t^{2}}{t -1}\,\mathrm{d}t .
>$$
>At this point, we note that $(t-1)(t+1)=t^{2} - 1$ so that we can also write
>$$
>\frac{t^{2}}{t -1}=\frac{t^{2}-1 + 1}{t -1}= \frac{t^{2}-1}{t -1} + \frac{1}{t -1}= t-1 + \frac{1}{t -1},
>$$
>and also
>$$
> -3\int \frac{t^{2}}{t -1}\,\mathrm{d}t =  -3\int t-1 + \frac{1}{t -1}\,\mathrm{d}t= -\frac{3}{2}(t-1)^{2} - 3\ln|t-1| +c.
> $$
> Consequently, applying the change of variable $t=g^{-1}(t)=\sqrt[6]{1-2x}$ we obtain
>$$
>\int \frac{1}{\sqrt[3]{(1-2x)^{2}} - \sqrt{1-2x}}\,\mathrm{d}x=  -\frac{3}{2}(\sqrt[6]{1-2x}-1)^{2} - 3\ln|\sqrt[6]{1-2x}-1| +c .
>$$

>[!attention] Remark
>A useful tool to guess the change of variable is to remember that, given $x=g(t)$, it holds
> $$
>\frac{\mathrm{d}x}{\mathrm{d}t}=g'(t)
>$$
>  which we formally write as
> $$
> \mathrm{d}x = g'(t) \mathrm{d}t.
> $$
>  Quite similarly, we also have
> $$
>  \frac{\mathrm{d}t}{\mathrm{d}x}=(g^{-1})'(x)=\frac{1}{g'(g^{-1}(x))} \quad \longrightarrow \quad \mathrm{d}t=(g^{-1})'(x)\mathrm{d}x .
>$$

>[!exmp] Example
>Let us compute the anti-derivative 
>$$
>\int f(x)\,\mathrm{d}x=\int\frac{(x+1)^{3}}{\sqrt{1-(x+1)^{2}}}\,\mathrm{d}x .
>$$
>It may "feel" natural to set $t=g^{-1}(x)=(x+1)^{2}$ as a reasonable change of variable. It then follows that $x=g(t)=\sqrt{t} -1$ so that $g'(t)=\frac{1}{2\sqrt{t}}$ and 
>$$
>\int\frac{(x+1)^{3}}{\sqrt{1-(x+1)^{2}}}\,\mathrm{d}x = \int \frac{t}{2\sqrt{1-t}} \,\mathrm{d}t .
>$$
>
>This anti-derivative is not immediate, but can be computed using the procedure known as "integration by part" that is discussed below. However, there is another possibile change of variable that, while looking more complex, simplifies the problem. Specifically, we set
>$$
>t=\sqrt{1-(x+1)^{2}},
>$$
>from which it follows that 
>$$
>1-t^{2} = (x+1)^{2} .
>$$
>Moreover, following what is written in the previous remark, we also have
>$$
>\mathrm{d}t =- \frac{(1+x)}{\sqrt{1- (x+1)^{2}}}\mathrm{d}x ,
>$$
>which means that
>$$
>\int\frac{(x+1)^{3}}{\sqrt{1-(x+1)^{2}}}\,\mathrm{d}x =\int t^{2}-1 \mathrm{d}t,
>$$
>where, as usual, the change of variable $t\mapsto t(x)$ in the right-hand-side is implicit. The anti-derivative on the right-hand-side is immediate being it the anti-derivative of a polynomial function:
>$$
>\int t^{2} -1 \mathrm{d}t = \frac{t^3}{3} -t,
>$$
>and thus we obtain
>$$
>\int\frac{(x+1)^{3}}{\sqrt{1-(x+1)^{2}}}\,\mathrm{d}x = \frac{(1-(x+1)^{2})^{\frac{3}{2}}}{3} - \sqrt{1-(x+1)^{2}}.
>$$

>[!rem] Remark
>It is useful to remember the following useful changes of variables:
>
>1) when we have $\sqrt{1-x^{2}}$ we can apply $x=g(t)=\sin(t)$ so that $\sqrt{1-x^2}=\cos(t)$ and $g'(t)=\cos(t)$;
>2)  when we have $\sqrt{1+x^{2}}$ we can apply $x=g(t)=\tan(t)$ so that $\sqrt{1+x^2}=\frac{1}{\cos(t)}$ and $g'(t)=\frac{1}{\cos^{2}(t)}$;
>3) when we have $\sqrt{1+x^{2}}$ we can also apply $x=g(t)=\sinh(t)$ so that $\sqrt{1+x^2}=\cosh(t)$ and $g'(t)=\cosh(t)$;
>4) when we have $\sqrt{x^{2}-1}$ we can apply $x=g(t)=\frac{1}{\cos(t)}$ so that $\sqrt{x^{2} -1}=\tan(t)$ and $g'(t)=\frac{\sin(t)}{\cos^{2}(t)}$;
>5) when we have $\sqrt{x^{2}-1}$ we can apply $x=g(t)=\cosh(t)$ so that $\sqrt{x^2 - 1}=\sinh(t)$ and $g'(t)=\sinh(t)$;
>6) when we have rational functions of trigonometric functions, we can exploit $t=g^{-1}(x)=\tan(x/2)$ so that $\sin(x)=\frac{2t}{1+t^{2}}$, $\cos(x)=\frac{1-t^{2}}{1+t^{2}}$, and $g'(t)=\frac{2}{1 + t^{2}}$.
>

# Integration by parts

^b5075f

If we exploit the so-called [[1.3 Derivatives of scalar functions#^5d20ad|Leibniz rule]] for computing the derivative of the product function
$$
\frac{\mathrm{d}}{\mathrm{d}x} (gh)(x)= g'(x) h(x) + g(x) h'(x),
$$
we obtain
$$
\int\left(\frac{\mathrm{d}}{\mathrm{d}x} (gh)(x)\right)\,\mathrm{d}x = \int\left( g'(x) h(x) + g(x) h'(x)\right)\,\mathrm{d}x
$$
which means
$$
\int  g'(x) h(x) \,\mathrm{d}x= gh(x) - \int  g(x) h'(x) \,\mathrm{d}x ,
$$
which is known as the formula for **integration by parts**.

>[!exmp] Example
> Consider the continuous function $f\colon [a,b]\rightarrow \mathbb{R}$ given by $f(x)=x\mathrm{e}^{x}$. We compute its anti-derivatives exploiting the rule of integration by parts. Specifically, we look at $\mathrm{e}^{x}$ as the derivative function of $g(x)=\mathrm{e}^{x}$ so that 
> 
> $$
> \int f(x)\,\mathrm{d}x=\int x\,\mathrm{e}^{x}\,\mathrm{d}x = x \mathrm{e}^{x} - \int \mathrm{e}^{x} \,\mathrm{d}x=(x-1)\mathrm{e}^{x} .
>$$
>
>A direct computation of the derivative of $(x-1)\mathrm{e}^{x}$ shows that we did good.

