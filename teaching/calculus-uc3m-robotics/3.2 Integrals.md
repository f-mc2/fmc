---
dg-home: false
dg-publish: true
---
# Definite integrals and anti-derivatives

Consider the scalar function $f\colon D\subseteq\mathbb{R}\rightarrow\mathbb{R}$ and the closed and bounded interval $[a,b]\subseteq D$, and build the set
$$
S_{[a,b]}^{f}:=\left\{(x,y)\in\mathbb{R}^{2}\,|\quad x\in[a,b],\;\;y=f(x)\right\} .
$$
The notion of definite integral of $f$ over $[a,b]$ gives rigorous meaning to the idea of measuring the area $\mathrm{A}(S_{[a,b]}^{f})$ of $S_{[a,b]}^{f}$ in such a way that this mathematical construction reduces to the results obtained by elementary geometry in simple cases like the area of a square (_e.g._, $f\colon \mathbb{R}\rightarrow \mathbb{R}$ given by $f(x)=1$  over the interval $[0,1]$), or the area of a triangle (_e.g._, $f(x)=x$ over the interval $[0,1]$). 

Let us start with a function $f\colon D\subseteq\mathbb{R}\rightarrow \mathbb{R}$ which is non-negative and increasing. The main idea behind the procedure of integrating $f$ is to approximate the area of $\mathrm{A}(S_{[a,b]}^{f})$ "from above" and "from below".  ^964215

To better understand the idea, we visualize the procedure of finding the "approximation from below", on the left, and "from above", on the right ([source-left](https://en.wikipedia.org/wiki/File:Riemann_Integration_and_Darboux_Lower_Sums.gif), [source-right](https://en.wikipedia.org/wiki/File:Riemann_Integration_and_Darboux_Upper_Sums.gif)): 

![[Integral_lower_upper_sum.gif]]

Other helpful visualizations can be found [here](https://github.com/FOSSEE/FSF-mathematics-python-code-archive/tree/master/FSF-2020/calculus/intro-to-calculus/riemann-integrals).

More mathematically, we first divide $[a,b]$ in two intervals selecting $c\in(a,b)$. The approximation of $\mathrm{A}(S_{[a,b]}^{f})$ from below will be given by
$$
\mathrm{A}_{2}^{-}(S_{[a,b]}^{f})=(c-a)(f(a)) + (b-c)(f(c)).
$$
This is nothing but the sum of the area of the rectangle with vertices $(a,0)$, $(c,0)$, $(a,f(a))$, $(c,f(a))$ with the area of the rectangle with vertices $(c,0)$, $(b,0)$, $(c,f(c))$, $(b,f(c))$. Quite similarly, the approximation of $\mathrm{A}(S_{[a,b]}^{f})$ from above will be given by
$$
\mathrm{A}_{2}^{+}(S_{[a,b]}^{f})=(c-a)(f(c)) + (b-c)(f(b)).
$$

We go on and divide $[a,b]$ in 3 parts and compute $\mathrm{A}_{3}^{-}(S_{[a,b]}^{f})$ and $\mathrm{A}_{3}^{+}(S_{[a,b]}^{f})$, and we go on increasing the number of parts in which we divide $[a,b]$. If the function $f$ is "nice enough", this procedure will give us two sequences, say $\mathrm{A}_{n}^{-}(S_{[a,b]}^{f})$ and $\mathrm{A}_{n}^{+}(S_{[a,b]}^{f})$, that converge to the same limit which is the area $\mathrm{A}(S_{[a,b]}^{f})$ we were looking for. 

The procedure outlined above [can be generalized](https://en.wikipedia.org/wiki/Darboux_integral) to functions which are not necessarily non-negative, and not necessarily increasing, and all functions for which $\mathrm{A}(S_{[a,b]}^{f})$ actually exists are called (Darboux or Riemann) **integrable**. Of course the interpretation of the integral as an area fails for functions admitting negative values, unless we allow for a more flexible idea of areas which contemplates also negative values.

For reasons that will be clear below, the integral $\mathrm{A}(S_{[a,b]}^{f})$ of $f\colon D\subseteq\mathbb{R}\rightarrow\mathbb{R}$ over $[a,b]$ is denoted as
$$
\mathrm{A}(S_{[a,b]}^{f})=\int_{a}^{b} f(x)\,\mathrm{d}x,
$$
where it is clear that we are borrowing the notation from the anti-derivative operation. 

The existence of the integral of a function is not guaranteed in general.

>[!important] Proposition: continuous functions are integrable
>Let $f\colon [a,b]\subset \mathbb{R}\rightarrow\mathbb{R}$ be continuos except at most at $a$ or $b$, then the integral $\mathrm{A}(S_{[a,b]}^{f})$ exists. 

As you might guess, evaluating the integral of a function by means of the approximation procedures described above is incredibly difficult, even for "nice" functions. However, some talented humans discovered a deep, beautiful, and perhaps surprising link between integrals and anti-derivatives that greatly simplify the task.

>[!important] Theorem: [first fundamental theorem of calculus](https://en.wikipedia.org/wiki/Fundamental_theorem_of_calculus#First_part)
>Let $f\colon [a,b]\rightarrow \mathbb{R}$ be continuous. For every $x\in [a,b]$, the function 
>$$
>F(x):=\int_{a}^{x}f(t)\,\mathrm{d}t =\mathrm{A}(S_{[a,x]}^{f})
>$$
>is continuous on $[a,b]$ and differentiable in $(a,b)$. Moreover, its derivative satisfies
>$$
>F'(x) =f(x)
>$$
>which means that $F(x)$ is an anti-derivative of $f(x)$.

^0523d5

>[!important] Theorem: [second fundamental theorem of calculus](https://en.wikipedia.org/wiki/Fundamental_theorem_of_calculus#Second_part)
>Let $f\colon [a,b]\rightarrow\mathbb{R}$ be such that it admits a continuous anti-derivative $F$ at all $x\in (a,b)$. Then, if $f$ is integrable it holds
>$$
>\mathrm{A}(S_{[a,b]}^{f})=\int_{a}^{b}f(t)\,\mathrm{d}t=F(b) - F(a) .
>$$

Even if the two theorems above are similar, they are different. For instance, the first one assumes $f$ to be continuous so that its anti-derivative is a [[1.3 Derivatives of scalar functions#^c07f2b|$C^{1}$-function]], while the second one only assumes $F$ to be differentiable. Then, the first one determines the anti-derivative $F$ uniquely since
$$
F(a)=\int_{a}^{a}f(t)\,\mathrm{d}t = \mathrm{A}(S_{[a,a]}^{f}) =0
$$
because the area over a single point is $0$. However, we know that $G(x)=F(x) + c$ with $c\in\mathbb{R}$ is another good anti-derivative. Indeed, $G(x)$ can be used to compute the integral of $f$ using the second theorem since
$$
F(b)\overbrace{=}^{\mathrm{Thm}\,1}\mathrm{A}(S_{[a,b]}^{f})=\int_{a}^{b}f(t)\mathrm{d}t\overbrace{=}^{\mathrm{Thm}\,2}G(b) - G(a)\overbrace{=}^{G(x)=F(x)+c} F(b) +c - F(a) - c \overbrace{=}^{F(a)=0} F(b) .
$$

In the following, when computing $\mathrm{A}(S_{[a,b]}^{f})$ we simply look for an anti-derivative $F$ of $f$ and then write $\mathrm{A}(S_{[a,b]}^{f})=F(b)-F(a)$.

We list now some important results concerning definite integral.

>[!important] Proposition: properties of the integral
>Let $f,g\colon[a,b]\subset\mathbb{R}\rightarrow\mathbb{R}$ be integrable, and let $\alpha,\beta\in\mathbb{R}$. It  holds that
>$$
>\begin{split}
>\int_{c}^{c}f(t)\,\mathrm{d}t&=0 \\ & \\
>\int_{c}^{d}f(t)\,\mathrm{d}t&= - \int_{d}^{c}f(t)\,\mathrm{d}t \\ & \\ \int_{a}^{b}\left(\alpha f(t) + \beta g(t)\right) \,\mathrm{d}t &= \alpha \int_{a}^{b}f(t) \,\mathrm{d}t+ \beta \int_{a}^{b}g(t) \,\mathrm{d}t ,
>\end{split}
>$$
>where the last property is known as the **linearity of the integral**. 

>[!important] Proposition: interval additivity
>Let $a<b<c$ be real numbers. The function $f\colon [a,c]\rightarrow\mathbb{R}$ is integrable on $[a,c]$ if and only if it is integrable in $[a,b]$ and $[b,c]$. Moreover, it holds 
>$$
>\int_{a}^{c}f\;\mathrm{d}x = \int_{a}^{b}f\;\mathrm{d}x + \int_{b}^{c}f\;\mathrm{d}x .
>$$
>In particular, a function $f\colon [a,b]\rightarrow \mathbb{R}$ which is continuous except at a finite (but perhaps arbitrarily big) number of points in $[a,b]$ is integrable in $[a,b]$.

^54f5d8

>[!important] Proposition 
>Let $f\colon[a,b]\rightarrow\mathbb{R}$ be continuous, and let $g_{1},g_{2}\colon(a,b)\rightarrow$ be differentiable. Then
>$$
>H(x):=\int_{g_{1}(x)}^{g_{2}(x)}f(t)\;\mathrm{d}t
>$$
>is differentiable in $(a,b)$ and it is such that
>$$
>H'(x)=f(g_{2}(x))\,g_{2}'(x) - f(g_{1}(x))\,g_{1}'(x)
>$$
>for all $x\in (a,b)$.

>[!important] Proposition (Change of variables for definite integrals)
>Let $g\colon[a,b]\rightarrow \mathbb{R}$ be continuous and differentiable in $(a,b)$, set $I=g([a,b])$, and let $f\colon I\subset\mathbb{R}\rightarrow \mathbb{R}$ be continuous. Then
>$$
>\int_{g(a)}^{g(b)}\,f(t)\;\mathrm{d}t= \int_{a}^{b}\,f\circ g(x)\;\mathrm{d}x .
>$$
