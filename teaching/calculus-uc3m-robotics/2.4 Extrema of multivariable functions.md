---
dg-home: false
dg-publish: true
---
# Critical points and local extrema for multivariable scalar functions

We now turn our attention to the problem of finding local and global extrema for multivariable scalar functions.

>[!defn] Definition: local extrema
>Consider the function $f\colon D\subseteq\mathbb{R}^{n}\rightarrow\mathbb{R}$ and the [[2.1 From 1 to many dimensions#^ceea9b|interior point]] $\mathbf{x}_{0}\in D$:
>1) it is said that $f$ has a **local maximum** at $\mathbf{x}_{0}$ if there is $\epsilon>0$ such that $f(\mathbf{x}_{0})>f(\mathbf{x})$ for all $\mathbf{x}\in B_{\mathbf{x}_{0},\epsilon}$;
>2) it is said that $f$ has a **local minimum** at $\mathbf{x}_{0}$ if there is $\epsilon>0$ such that $f(\mathbf{x}_{0})<f(\mathbf{x})$ for all $\mathbf{x}\in B_{\mathbf{x}_{0},\epsilon}$
>3) it is said that $f$ has a **local extrema** at $\mathbf{x}_{0}$ if $\mathbf{x}_{0}$ is either a local maximum or a local minimum of $f$.

For a multivariable scalar function $f\colon D\subseteq\mathbb{R}^{n}\rightarrow\mathbb{R}$, it is possible to introduce a notion of derivative that takes into account how $f$ changes along an arbitrary direction in $\mathbb{R}^{n}$ determined by the vector $\mathbf{v}\in\mathbb{R}^{n}$. This notion of derivative is called **directional derivative** and is defined below.

>[!defn] Definition: directional derivative
>Consider the function $f\colon D\subseteq\mathbb{R}^{n}\rightarrow\mathbb{R}$, an interior point $\mathbf{x}_{0}\in D$, and the vector $\mathbf{v}\in\mathbb{R}^{n}$. The **directional derivative** $\mathbf{D}_{\mathbf{v}}f(\mathbf{x}_{0})$ of $f$ at $\mathbf{x}_{0}$ along the direction $\mathbf{v}$ is defined as
>$$
>\mathbf{D}_{\mathbf{v}}f(\mathbf{x}_{0}):=\lim_{t\rightarrow 0}\,\frac{f(\mathbf{x}_{0} + t\mathbf{v}) - f(\mathbf{x}_{0})}{t}=\frac{\mathrm{d} f(\mathbf{x}_{0} + t\mathbf{v})}{\mathrm{d}t}(0) ,
>$$
>where the right-hand side is thought of as a vector function of one variable $t\in\mathbb{R}$. A moment of thought reveals that the directional derivative of $f$ at $\mathbf{x}_{0}$ along the direction $\mathbf{e}_{j}$ of the standard basis coincides with the $j$-th partial derivative of $f$ at $\mathbf{x}_{0}$ as defined [[2.3 Derivatives and Taylor's theorem for multivariable functions#^c8b7dc|here]]. 

Focusing on the vector function $\gamma\colon D'\subseteq \mathbb{R} \rightarrow \mathbb{R}^{n}$ given by
$$
\gamma(t)=\mathbf{x}_{0} + t\mathbf{v},
$$
where $D'$ is determined by the condition $\gamma(D')\subseteq D$, a direct application of the [[2.3 Derivatives and Taylor's theorem for multivariable functions#^9ceaa7|chain rule]] for the function $f\circ \gamma$ allows to prove the following proposition that gives a direct link between directional derivatives and the gradient vector.

>[!important] Proposition: directional derivatives and the gradient 
>Let $f\colon D\subseteq\mathbb{R}^{n}\rightarrow \mathbb{R}$ be differentiable in $D$, and $\mathbf{x}_{0}\in D$ be an interior point. Then, it holds 
>$$
> \mathbf{D}_{\mathbf{v}}f(\mathbf{x}_{0})= \mathbf{v}\,\cdot\,\nabla f(\mathbf{x}_{0}) ,
>$$
>where $\cdot$ is the scalar product in $\mathbb{R}^{n}$.

>[!rem] Remark: gradient vector as direction of fastest increase
For a differentiable function $(D,  \mathbb{R}^{n},f, \mathbb{R})$, since the scalar product is maximum when $\mathbf{v}$ is parallel to $\nabla f(\mathbf{x}_{0})$, we conclude that the gradient vector points in the direction along which $f$ increases the fastest.

Motivated by the previous remark, we introduce the notion of **critical point** in analogy with the [[1.6 Derivatives and extrema of scalar functions#^d2fcd0|one-variable scalar case]], and we state how it is connected to local extrema.

>[!defn] Definition: critical point
>Given the function $(D,\mathbb{R}^{n},f,\mathbb{R})$, an **interior point** $\mathbf{x}_{0}\in D$ is called a **critical point** if either $\mathbf{D}f(\mathbf{x}_{0})=\nabla f(\mathbf{x}_{0})=0$ or it does not exist.

>[!important] Proposition: local extrema and critical points
>If $(D,\mathbb{R}^{n},f, \mathbb{R})$ has a local extremum at $\mathbf{x}_{0}\in D$ then $\mathbf{x}_{0}$ is a **critical point** of $f$.

>[!attention] Remark
>The converse of the previous proposition is not true, already for [[1.6 Derivatives and extrema of scalar functions#^f4bbab|scalar functions of one variable]].

In the genuinely multivariable case (_i.e._, when the output space is $\mathbb{R}^{n}$ with $n>1$), it is difficult to establish if a critical point is a local extrema. Intuitively speaking, this is due to the fact that a local extrema $\mathbf{x}_{0}$ has to be an extrema along every direction pointing to it, and this is a very demanding request. For instance, let us consider the function $f\colon \mathbb{R}^{2}\rightarrow\mathbb{R}$ given by
$$
f(x,y)=x^{2} - y^{2} .
$$
It is immediate to check that the partial derivatives are
$$
f_{x}=2x,\qquad f_{y}=-2y,
$$
so that $(0,0)$ is a critical point because $\nabla f(0,0)=(0,0)$. However, if we investigate the behaviour of $f$ along the $x$-axis determined by $y=0$ we find $f(x,0)=x^{2}$ which has a global minimum at $x=0$, while the behaviour of $f$ along the $y$-axis gives back $f(0,y)=-y^{2}$ which has a global maximum at $y=0$. Consequently, $(0,0)$ can not be a local extrema for $f$. 

This situation is very typical for multivariable scalar functions, and, as it is clear from the example above, it applies also to very simple functions.

A possible way to investigate the nature of a critical point is associated with the notion of [[2.3 Derivatives and Taylor's theorem for multivariable functions#^1b4383|second order partial derivative]]. In particular, multivariable scalar functions that are $C^{2}$ are important because of the following proposition.

>[!important] Proposition 
>Let $f\colon D\subseteq \mathbb{R}^{n}\rightarrow\mathbb{R}$ be $C^{2}$ on $A\subseteq D$, and let $\mathbf{x}_{0}\in A$ be a critical point (which can only mean $\nabla f(\mathbf{x}_{0})=\mathbf{0}$ because all the partial derivatives exist when $f$ is $C^{2}$). If the multivariable scalar function  $Hf_{\mathbf{x}_{0}}\colon\mathbb{R}^{n}\rightarrow\mathbb{R}$ given by
>$$
>Hf_{\mathbf{x}_{0}}(\mathbf{h}):=(h^{1}\,\cdots\, h^{n})\left(\begin{matrix}f_{11}|_{\mathbf{x}_{0}}& \cdots & f_{1n}|_{\mathbf{x}_{0}} \\ & & \\ \vdots & &\vdots \\ & & \\ f_{n1}|_{\mathbf{x}_{0}} & \cdots & f_{nn}|_{\mathbf{x}_{0}}\end{matrix}\right)\,\left(\begin{matrix}h^{1} \\ \\\vdots\\ \\h^{n} \end{matrix}\right)
>$$
>is strictly positive for all $\mathbf{0}\neq\mathbf{h}\in\mathbb{R}^{n}$ then $\mathbf{x}_{0}$ is a **local minimum**; if it is strictly negative for all $\mathbf{0}\neq\mathbf{h}\in\mathbb{R}^{n}$ then $\mathbf{x}_{0}$ is a **local maximum**; if it takes both positive and negative values, and the Hessian matrix $Hf_{\mathbf{x}_{0}}$ is [invertible](https://en.wikipedia.org/wiki/Invertible_matrix), then $\mathbf{x}_{0}$ is a **saddle point**.

^36f25b

>[!rem] Remark
>To check which type of local extrema is $\mathbf{x}_{0}$, we can also investigate the determinants of all the square sub-matrices of the Hessian matrix containing the first element. If they are all positive, then $\mathbf{x}_{0}$ is a **local minimum**. If they alternate starting with a negative one, then $\mathbf{x}_{0}$ is a local minimum. If they are all different then $0$, but do not follow the previous conditions, then $\mathbf{x}_{0}$ is a **saddle point**. 

^99ac31

# Global extrema

Let us start with the definition of global extrema.

>[!defn] Definition: global extrema
>Consider the function $f\colon D\subseteq\mathbb{R}^{n}\rightarrow\mathbb{R}$ and $\mathbf{x}_{0}\in D$:
>1) it is said that $f$ has a **global maximum** at $\mathbf{x}_{0}$ if $f(\mathbf{x}_{0})>f(\mathbf{x})$ for all $\mathbf{x}\in D$;
>2) it is said that $f$ has a **global minimum** at $\mathbf{x}_{0}$ if $f(\mathbf{x}_{0})<f(\mathbf{x})$ for all $\mathbf{x}\in D$;
>3) it is said that $f$ has a **local extrema** at $\mathbf{x}_{0}$ if $\mathbf{x}_{0}$ is either a global maximum or a global minimum of $f$.

Understanding the behaviour of arbitrary multivariable scalar functions is incredibly difficult. We need to make regularity assumptions on the type of functions we consider in order to be able to attack the problem. At this purpose, multivariable scalar functions that are continuous are particularly important because they admit global extrema when they are defined on sets that are closed and bounded.

>[!defn] Definition: bounded and closed sets
>The set $A\subseteq \mathbb{R}^{n}$ is called **bounded** if there is $M\in\mathbb{R}$ such that $||\mathbf{x}||\leq M$ for every $\mathbf{x}\in A$. 
>The set $A\subseteq \mathbb{R}^{n}$ is called **closed** if it contains all its boundary points.

^cf7a0c

>[!important] Theorem: [Extreme value theorem](https://math.stackexchange.com/a/881569/86146)
>Given a closed and bounded set $D\subset\mathbb{R}^{n}$, a continuous function $f\colon D\subset\mathbb{R}^{n}\rightarrow\mathbb{R}$ admits global maximum and global minimum.

^d586b6

>[!attention] Remark: the level sets of continuous functions are closed
>Fixing $c\in\mathbb{R}$, it can be proved that the level set 
>$$
>S=\left\{\mathbf{x}\in\mathbb{R}^{n}\,|\quad g(\mathbf{x})=c \right\}
>$$
>of the continuous function $g\colon D\subseteq\mathbb{R}^{n}\rightarrow\mathbb{R}$ is always closed in $\mathbb{R}^{n}$. 

In the following, we discuss two possible ways to determine global extrema for continuous functions defined on a closed and bounded domain whose boundary is of a particular type. To be able to discuss more general cases would require a level of sophistication that is not compatible with the little time we have at our disposal for the course. However, if you are interested, a first approach is to read carefully the whole chapter 3 of [M-T](https://www.macmillanlearning.com/college/ca/product/Vector-Calculus/p/1429215089), the whole chapter 16 of [S-E-H](https://www.wiley.com/en-us/Calculus:+One+and+Several+Variables,+10th+Edition-p-9780471698043), and then consult the references and external links in the [Wikipedia page](https://en.wikipedia.org/wiki/Multivariable_calculus) for "Multivariable calculus". 
# Global extrema in 2D: parametrizable boundary

Here, we discuss how to find the global extrema of a continuous function $(D,\mathbb{R}^{2},f,\mathbb{R})$ when $D$ is closed and bounded, and the boundary $\partial D$ is the union of finitely many smooth curves (_e.g._,  polygons, ellipses, and similar things). 

Instead of trying to describe a general theory, we deal with a specific example that allows to understand the general structure. We consider the function $(D,\mathbb{R}^{2},f,\mathbb{R})$, with $f(x,y)=x^{2} + y^{2}-2x$, and with $D$ the triangle with vertices $A\equiv(0,0)$, $B\equiv(2,0)$, and $C\equiv(0,2)$.

Since $f$ is continuous and $D$ is a closed and bounded set, there will be global extrema, and we have to first look for local extrema inside $D$, and then investigate the behaviour at the boundary.

The local extrema inside $D$ can be found and classified according to the theory we [[2.4 Extrema of multivariable functions#^36f25b|developed before]]. We obtain the critical point $(1,0)$ which lies on the boundary of the triangle. Since $f$ is a polynomial function, it is $C^2$, and the Hessian matrix at $(1,0)$ reads
$$\mathbf{H}f_{(1,0)}=\left(\begin{matrix}2 & 0 \\ 0 & 2\end{matrix}\right).$$
According to the [[2.4 Extrema of multivariable functions#^36f25b|proposition]] and [[2.4 Extrema of multivariable functions#^99ac31|remark]] above, we conclude that $(1,0)$ is a local minimum.

Concerning the behavior on the boundary, we consider the open segment $AB$, the open segment $BC$, the open segment $AC$, and then the three vertices.

The segment $AB$ can be parametrized by the function $t\mapsto(t,0)$ with $t\in(0,2)$, so that the original function on this segment looks like $f(t,0)=t^{2}-2t$. Following what we learned for [[1.6 Derivatives and extrema of scalar functions|scalar functions of one variable]], we obtain a minimum at $t=1$, that is, at the point $(1,0)$, which is compatible with what we found before.

The segment $AC$ can be parametrized by the function $t\mapsto (0,t)$ with $t\in(0,2)$, so that the original function on this segment looks like $f(0,t)=t^{2}$. This function has a minimum at $t=0$, which is also a corner point.

The segment $BC$ can be parametrized by $t\mapsto(2t,2(1-t))$ with $t\in(0,1)$, and the original function on the segment looks like $f(2t,2(1-t))= 4t^{2} +4 +4t^{2}-8t-4t=8t^{2}-12t+4$. This function has a minimum at $t=\frac{3}{4}$, that is, at the point $(\frac{3}{2},\frac{1}{2})$.

Arrived here, to find the global extrema of $f$ in $D$, we have to evaluate the function at all the local extrema in the interior of $D$ (in this case, there is no such point), and at the boundary $\partial D$ (including the vertices):
$$
\begin{split}
f(0,0) & =0 \\ & \\
f(2,0) & = 0 \\ & \\
f(0,2) & = 4 \\ & \\
f(1,0) & = -1 \\ & \\
f\left(\frac{3}{2},\frac{1}{2}\right)& =-\frac{1}{2},
\end{split}
$$
and we conclude that $(1,0)$ is the global minima, while $(0,2)$ is the global maxima.
# Global extrema: Lagrange multipliers

Here, we discuss the method of Lagrange multipliers to find the global extrema on $D\subset A$ of a continuous function $(A,\mathbb{R}^{n},f,\mathbb{R})$ with $A$ an open set,  $D$ closed, bounded, with open interior, and the boundary $\partial D$ regular enough in a sense specified below.

The first condition we impose on $\partial D$ is that it is the level set of a smooth function $(A,\mathbb{R}^{n},g,\mathbb{R})$ with $A$ open, that is, we assume:
$$
\partial D=\left\{\mathbf{x}\in \mathbb{R}^{n}\;|\quad g(\mathbf{x})=c\right\},
$$
with $c\in\mathbb{R}$.

>[!important] Proposition: Lagrange multipliers and level sets
>Let $A\subseteq \mathbb{R}^{n}\rightarrow \mathbb{R}$ be open, and let $f,g \colon A\rightarrow\mathbb{R}$ be $C^{1}$ on $A$. Let $c\in\mathbb{R}$, let $S$ be the level set of $c$ through $g$:
>$$
>S=\left\{\mathbf{x}\in \mathbb{R}^{n}\,|\quad g(\mathbf{x})=c\right\},
>$$
>and assume $\nabla g(\mathbf{x}_{0})\neq\mathbf{0}$ at $\mathbf{x}_{0}\in S$.
>
>If the restriction of $f$ to $S$, usually denoted as $f|_{S}$, has a local maximum or local minimum at $\mathbf{x}_{0}$ then
>$$
>\nabla f(\mathbf{x}_{0})=\lambda \nabla g (\mathbf{x}_{0})
>$$
>for some $\lambda\in \mathbb{R}$ (which can also be $0$).

^df7f57

>[!note] Definition: Lagrange multipliers and critical points on level sets
>The number $\lambda$ in the previous proposition is known as **Lagrange multiplier**, while a point $\mathbf{x}_{0}\in S$ for which the equation 
>$$
>\nabla f(\mathbf{x}_{0})=\lambda \nabla g (\mathbf{x}_{0})
>$$
>holds is referred to as a **critical point** of $f$ on $S$.

When the level set $S$ in the previous proposition is the set of [[2.1 From 1 to many dimensions#^ceea9b|boundary points]]  $\partial D$ of a [[2.4 Extrema of multivariable functions#^cf7a0c|closed, bounded]], and connected (meaning it can not be partitioned into the disjoint union of non-empty open sets) set $D$, then we can find the global maxima and global minima of the $C^{1}$ function $f\colon A\subseteq\mathbb{R}^{n}\rightarrow \mathbb{R}$ on $D\subset A$ as follows:

1) we look for critical points in the interior of $D$ using the gradient vector $\nabla f$;
2)  we look for critical points on $\partial D$ using Lagrange multipliers;
3) we compute $f$ at all the points we found and select those giving maximum and minimum values.

>[!attention] Remark
>This procedure works because $D$ is closed an bounded, and $f$ is continuous on $D$ (because it is $C^{1}$ on $A\supset D$). If even one of the previous assumptions falls, the procedure outlined above will fail!

>[!exmp] Example
>Let $f\colon\mathbb{R}^{2}\rightarrow \mathbb{R}$ be given by
>
>$$
>f(x,y)=x^{2} - y^{2},
>$$
>
>and let $D$ be the closed disk with unit radius centered in $(0,0)$. The only critical point in the interior of $D$ is $(0,0)$. However, following the [[2.4 Extrema of multivariable functions#^36f25b|Hessian matrix criteria]], we conclude the origin is neither a local minima, nor a local maxima.
>Concerning the boundary $\partial D$, we can write it as the level set of the function $g(x,y)=x^{2} + y^{2}$ corresponding to the value $1$. Following the proposition on [[2.4 Extrema of multivariable functions#^df7f57|Lagrange mutipliers]], we have to solve the system
>$$
>\left\{\begin{matrix}\nabla f(x,y)=\lambda \nabla g(x,y) \\ x^{2} + y^{2} = 1\end{matrix}\right.
>$$
>with respect to $(x,y)$ and $\lambda$. A direct computation shows that the previous system is equivalent to
>$$
>\left\{\begin{matrix} x=\lambda x \\ y=-\lambda y \\ x^{2} + y^{2} = 1\end{matrix}\right. .
>$$
>We solve this system by first considering $y=0$ and then we solve it for $y\neq 0$. If $y=0$, then the last equation forces $x=\pm 1$ so that the first one implies $\lambda =1$. Therefore, we obtain two solutions: $(x=1,y=0;\lambda=1)$ and $(x=-1,y=0;\lambda=1)$. When $y\neq 0$, the second equation implies $\lambda=-1$ so that the first equation forces $x=0$, but then the third equation implies $y=\pm 1$. Therefore, we obtain other two solutions: $(x=0, y=1;\lambda=-1)$ and $(x=0,y=-1;\lambda=-1)$. 
>These four points are the only candidates to be global extrema for $f$ on $\overline{D}$. If we evaluate $f$ on them we discover that the first two points are global maxima while the second ones are global minima.

As a last remark, we note that the method of Lagrange multipliers can be used also in another context. Specifically, we may be interested in maximizing or minimizing a given function $(D,\mathbb{R}^{n},f,\mathbb{R})$ subject to a _constraint_ encoded in the equation $g(\mathbf{x})=c$. In this case, however, we are not sure the maxima or minima exist because we are not dealing, in general, with a continuous function on a closed and bounded domain. Therefore, _ad hoc_ considerations are necessary to determine whether the points selected by the method of Lagrange multipliers are indeed suitable maxima or minima.

>[!exmp] Example: minimization under a constraint
>We want to find, if possile, the minimum of $\mathbb{R},\mathbb{R}^{2},f,\mathbb{R}$ with
>$$f(x,y)=x^{2} + (y-2)^{2}$$
>subject to the constraint
>$$
>g(x,y)=x^{2}-y^{2}.$$
>Applying the method of Lagrange multipliers, we obtain the system
>$$\left\{\begin{matrix}x& = & \lambda x \\ (y-2) & = & -\lambda y \\ x^{2}-y^{2}& = &1.\end{matrix}\right.$$
>The constraint implies $x\neq 0$, and thus we obtain the solutions $(x=\pm\sqrt{2},y=1,\lambda=1)$. 
>Since the constraint $g(x,y)=x^{2} - y^{2}=1$ is an hyperbola, which is not bounded, and thus is not the boundary of a closed and bounded region, we do not know if either of the two points found above is a minimum. That is, we do not know if there is a solution to our problem.
>However, we note that $f(x,y)$ is nothing but the square of the distance of the point $(x,y)$ from the point $(0,2)$, so that our problem can be stated as the search for the point on the hyperbola $x^{2}-y^{2}=1$ which is the closest to the point $(0,2)$. Of course, because of geometrical considerations, this problem **must** have solution, and both the points found with the method of Lagrange multipliers are viable solutions.





