---
dg-home: false
dg-publish: true
---
# Limits multivariable vector functions 

The idea behind the notion of limit for multivariable vector functions is the same as for [[1.2 Limits of scalar functions|scalar functions]]. Essentially, the limit exists when we can stay as close as we want to an output point (the limit itself) by suitably controlling how the input is closer to a fixed point. As before, to make this intuition precise, we need $\epsilon$ and $\delta$.

>[!defn] Definition: $(\epsilon,\delta)$-definition of limit
>Consider the function $(D,\mathbb{R}^{n},f,\mathbb{R}^{m})$, and the point $\mathbf{x}_{0}\in\mathbb{R}^{n}$. Assume either $\mathbf{x}_{0}$ is an interior point of $D$, or it is a [[2.1 From 1 to many dimensions|boundary point]] of $D$.
>We say that 
>$$
>\lim_{\mathbf{x}\rightarrow \mathbf{x}_{0}}\,f(\mathbf{x})= \mathbf{L}
>$$
>if, for every $\epsilon >0$, there is $\delta_{\epsilon}>0$ such that  
>$$
>d(f(\mathbf{x}),\mathbf{L})=||f(\mathbf{x}) - \mathbf{L}||< \epsilon
>$$
>for every $\mathbf{x}\in D$ such that
>$$
>d(\mathbf{x},\mathbf{x}_{0})=||\mathbf{x} - \mathbf{x}_{0}||< \delta .
>$$

The notion of limit is well-posed because of the following result, whose proof represents an instructive exercise (but can also be found as theorem 2 in section 2.2 in [M-T](https://www.macmillanlearning.com/college/ca/product/Vector-Calculus/p/1429215089)) .

>[!important] Theorem: uniqueness of limit
>If $\lim_{\mathbf{x}\rightarrow \mathbf{x}_{0}}f(\mathbf{x})=\mathbf{L}$ and  $\lim_{\mathbf{x}\rightarrow \mathbf{x}_{0}}f(\mathbf{x})=\mathbf{M}$, then $\mathbf{L}=\mathbf{M}$.

>[!exmp] Example
>Let   $(D,\mathbb{R}^{n},f,\mathbb{R}^{n})$ be given by $f(\mathbf{x})=\mathbf{x}$, and let $\mathbf{x}_{0}$ be either an [[2.1 From 1 to many dimensions#^ceea9b|interior point or a boundary point]] of $D$. Then 
>$$
>\lim_{\mathbf{x}\rightarrow \mathbf{x}_{0}}\,f(\mathbf{x})=\mathbf{x}_{0}
>$$
>because
>$$
>||f(\mathbf{x}) - \mathbf{x}_{0}||=||\mathbf{x} - \mathbf{x}_{0}|| 
>$$
>and thus, fixing $\delta_{\epsilon}<\epsilon$, we obtain that $||\mathbf{x} - \mathbf{x}_{0}||<\delta_{\epsilon}$ implies $||f(\mathbf{x}) - \mathbf{x}_{0}||<\epsilon$. If we write $\mathbf{x}=(x^{1},\cdots, x^{n})$, we can consider the projection on the $j$-th factor as the function $(D,\mathbb{R}^{n},f^{j},\mathbb{R})$ given by $f^{j}(\mathbf{x})=x^{j}$. Then, it should be easy to prove that 
>$$
>\lim_{\mathbf{x}\rightarrow \mathbf{x}_{0}}\,f^{j}(\mathbf{x})=x_{0}^{j}
>$$
>where $\mathbf{x}_{0}=(x^{1}_{0},\cdots, x^{n}_{0})$.

# Properties of limits

In analogy with the [[1.2 Limits of scalar functions#^f50bb0|scalar case]], limits are often computed exploiting previously known limits and the following algebraic manipulations of limits (theorem 3 in chapter 2 of [M-T](https://www.macmillanlearning.com/college/ca/product/Vector-Calculus/p/1429215089)).

>[!important] Proposition: Algebraic manipulations of limits of multivariable scalar functions
>Let $\mathbf{x}_{0}\in\mathbb{R}^{n}$, $(\alpha_{1},\cdots\alpha_{m})$ be a finite sequence of real numbers, and $(f_{1},\cdots,f_{m})$ a finite sequence of multivariable scalar functions. Suppose that
>
>$$
>\lim_{\mathbf{x}\rightarrow \mathbf{x}_{0}}f_{j}(\mathbf{x})=\mathbf{L}_{j}\in \mathbb{R}
>$$
>
>for every $j=1,...,n$. Then, it holds
>
>$$
>\begin{split}
>1)\quad & \lim_{\mathbf{x}\rightarrow \mathbf{x}_{0}} \sum_{j=1}^{m}\,\alpha_{j}\,f_{j}(\mathbf{x}) =\sum_{j=1}^{m}\,\lim_{\mathbf{x}\rightarrow \mathbf{x}_{0}} \left(\alpha_{j}\,f_{j}(\mathbf{x})\right) = \sum_{j=1}^{m}\,\alpha_{j}\,\mathbf{L}_{j} \\ & \\
>2) \quad &\lim_{\mathbf{x}\rightarrow \mathbf{x}_{0}}\prod_{j=1}^{m} \,\alpha_{j}\,f_{j}(\mathbf{x}) =\prod_{j=1}^{m}\,\lim_{\mathbf{x}\rightarrow \mathbf{x}_{0}} \left(\alpha_{j}\,f_{j}(\mathbf{x})\right) = \prod_{j=1}^{m}\,\alpha_{j}\,\mathbf{L}_{j} .
>\end{split}
>$$
>
>Moreover, if $m=2$ and $\alpha_{2},\mathbf{L}_{2}\neq 0$, it holds
>
>$$
>\lim_{\mathbf{x}\rightarrow \mathbf{x}_{0}}\,\frac{\alpha_{1}f_{1}(\mathbf{x})}{\alpha_{2}f_{2}(\mathbf{x})}=\frac{\alpha_{1}\mathbf{L}_{1}}{\alpha_{2} \mathbf{L}_{2}}.
>$$

>[!rem] Remark: multivariable polynomials
>Using the algebraic manipulations of limits mentioned above, we can prove that
>$$ \lim_{\mathbf{x}\rightarrow\mathbf{x}_{0}} P_{n}(\mathbf{x})= P_{n}(\mathbf{x}_{0})$$
>for every $\mathbf{x}_{0}\in\mathbb{R}^{n}$, where $P_{n}$ is a polynomial of order $n$ in the variables $x_{1},\cdots x_{n}$ making up $\mathbf{x}=(x_{1},\cdots,x_{n})\in\mathbb{R}^{n}$. In particular, every polynomial of this type is [[2.2 Limits of multivariable functions#^eef27f|continuous]].

Often, the multivariable vector case can be reduced to the multivariable scalar case according to the following result (theorem 3 in chapter 2 of [M-T](https://www.macmillanlearning.com/college/ca/product/Vector-Calculus/p/1429215089)).

>[!important] Proposition
>If $f\colon D\subseteq\mathbb{R}^{n}\rightarrow \mathbb{R}^{m}$ can be written as 
>
>$$
>f(\mathbf{x})=\left(f_{1}(\mathbf{x}),\cdots,f_{m}(\mathbf{x})\right)
>$$ 
>
>with $f_{j}\colon D\subseteq \mathbb{R}^{n}\rightarrow \mathbb{R}$ for every $j=1,\cdots, m$, then
>
>$$
>\lim_{\mathbf{x}\rightarrow\mathbf{x}_{0}}\,f(\mathbf{x})=\mathbf{b}=(b_{1},\cdots,b_{m})
>$$
>
>if and only if 
>
>$$
>\lim_{\mathbf{x}\rightarrow\mathbf{x}_{0}}\,f_{j}(\mathbf{x})=b_{j}
>$$
>
>for every $j=1,...,m$.

For a function $(D,\mathbb{R},f,\mathbb{R})$, there are only two ways we can approach a point $x_{0}$ in the input space when evaluating the limit $x\rightarrow x_{0}$. Specifically, we can approach $x_{0}$ either from the left, or from the right.

The same is no longer true for multivariable vector functions. For instance, for $(D,\mathbb{R}^{2},f,\mathbb{R})$, when we evaluate the limit for $(x,y)\rightarrow (0,0)$, we can approach the origin along infinitely many different paths (_e.g._, all lines passing through the origin, all parabolas passing through the origin, and, more generally, all curves in the plane passing through the origin). 

This instance can be helpful to prove that a given limit does not exist by showing that we get different values when evaluating the limit along different trajectories.

>[!exmp] Example
>Let us consider the function $f\colon\mathbb{R}^{2}-\{(0,0)\}\rightarrow \mathbb{R}$ given by
>
>$$
>f(x,y)=\frac{x^{2} - y^{2}}{x^{2} + y^{2}} ,
>$$
>
>and let us investigat the limit for $(x,y)\rightarrow (0,0)$. If we approach the origin along the $x$-axis $y=0$, we obtain 
>
>$$
>f(x,0)=\frac{x^{2}}{x^{2}}\stackrel{x\rightarrow 0}{\longrightarrow} 1 .
>$$
>
>However, if we approach the origin along the $y$-axis $x=0$, we obtain
>
>$$
>f(0,y)=\frac{-y^{2}}{y^{2}}\stackrel{y\rightarrow 0}{\longrightarrow} -1 .
>$$
>
>Moreover, if we approach the limit along the line $x=y$, we obtain
>
>$$
>f(x,x)=\frac{0}{2x^{2}}\stackrel{x\rightarrow 0}{\longrightarrow} 0 .
>$$
>
>We thus see that the result depends on the path we take to "compute the limit", and thus we conclude that the limit of $f$ for $(x,y)\rightarrow (0,0)$ does not exist.

Let us now consider another instructive example that should prevent you to make a very common mistake, namely, to think that switching to polar coordinates is enough to compute a limit.

>[!exmp] Example (Limits and polar coordinates)
>Let $f\colon \mathbb{R}^{2} - \{(0,0)\}\rightarrow \mathbb{R}$ be given by
>$$
>f(x,y)=\frac{x^{2}\,y}{x^{4} + y^{2}},
>$$
>
>and let us investigate the limit
>
>$$
>\lim_{(x,y)\rightarrow(0,0)}\,f(x,y) .
>$$
>
>If we pass to polar coordinates, we have that $(x,y)\rightarrow (0,0)$ implies $r\rightarrow 0$, and we immediately compute
>
>$$
>\lim_{r\rightarrow 0}\,f(r\cos(\theta),r\sin(\theta))=\lim_{r\rightarrow 0}\,\frac{r^{3}\cos^{2}(\theta)\sin(\theta)}{r^{2}(r^{2}\cos^{4}(\theta) + \sin^{2}(\theta))}=0 .
>$$
>
>[It may happen](https://math.stackexchange.com/questions/753381/limit-fracx2yx4y2-is-found-using-polar-coordinates-but-it-is-not-supp) that you think that the previous limit forces the result
>
>$$
>\lim_{(x,y)\rightarrow(0,0)}\,f(x,y) =0 ,
>$$
>
>but this is definitely wrong. Indeed, if we evaluate the limit along the parabola $y=x^{2}$ we obtain
>
>$$
>\lim_{(x,y)\rightarrow(0,0)}\,f(x,x^{2}) = \,\lim_{(x,y)\rightarrow(0,0)}\, \frac{x^{4}}{2x^{4}}=\frac{1}{2} ,
>$$
>
>thus showing that the original limit actually does not exist. 

Just as in the [[1.2 Limits of scalar functions#^ec717d|scalar case]], the squeeze theorem holds for multivariable scalar functions and makes our lives better.

>[!important] Theorem: Squeeze (pinching) theorem
> Let $f,g,h$ be multivariable scalar functions with the same domain $D\subseteq \mathbb{R}^{n}$, and let $x_{0}$ be an interior point for $D$. If $h(\mathbf{x})\leq f(\mathbf{x})\leq g(\mathbf{x})$ for all $\mathbf{x}\in D$, and 
> $$
> \lim_{\mathbf{x}\rightarrow \mathbf{x}_{0} }h(\mathbf{x})=\lim_{\mathbf{x}\rightarrow \mathbf{x}_{0} }g(\mathbf{x})=L\in\mathbb{R},
> $$
> then it follows that
> $$
> \lim_{\mathbf{x}\rightarrow \mathbf{x}_{0} }f(\mathbf{x})=L.
> $$


# Continuity of multivariable vector functions

Just as in the [[1.2 Limits of scalar functions#^91cff8|scalar case]], from an intuitive point of view, a scalar function is **continuous** if its graph can be drawn without lifting the pen/pencil from the paper, and the following definition provides a rigorous mathematical formulation of this idea.

>[!defn] Definition: 
>The function $f\colon D\subseteq \mathbb{R}^{n}\rightarrow \mathbb{R}^{m}$ is said to be **continuous** at $\mathbf{x}_{0}\in D$ if
>
>$$
>\lim_{\mathbf{x}\rightarrow \mathbf{x}_{0}}f(\mathbf{x})=f(\mathbf{x}_{0})
>$$
>
>The function $f$ is said to be **continuous** if it is continuous at $\mathbf{x}_{0}$ for every $\mathbf{x}_{0}\in D$.

^eef27f

Some important results about continuous functions are collected below (and their proofs can be found, for instance, in chapter 2 of [M-T](https://www.macmillanlearning.com/college/ca/product/Vector-Calculus/p/1429215089)).

>[!important] Proposition: algebraic manipulations of continuous functions
>Let $f,g\colon D\subseteq \mathbb{R}^{n}\rightarrow \mathbb{R}^{m}$. Then:
>1) if $f,g$ are continuous at $\mathbf{x}_{0}$ and $\alpha,\beta\in\mathbb{R}$ then $\alpha f + \beta g$ is continuous at $\mathbf{x}_{0}$;
>2) if $m=1$, $f$ and $g$ are continuous at $\mathbf{x}_{0}$, and $\alpha,\beta\in\mathbb{R}$, then $(\alpha f)(\beta g)$ is continuous at $\mathbf{x}_{0}$;
>3) if $m=1$, $f$ and $g$ are continuous at $\mathbf{x}_{0}$, and $\alpha,\beta\in\mathbb{R}$ then $\frac{\alpha f}{\beta g}$ is continuous at $\mathbf{x}_{0}$ provided that $\beta,g(\mathbf{x}_{0})\neq 0$;
>4) if $f(\mathbf{x})=(f_{1}(\mathbf{x}),\cdots,f_{m}(\mathbf{x}))$ with $f_{j}\colon\mathbb{R}^{n}\rightarrow \mathbb{R}$ for every $j=1,...,m$, then $f$ is continuous at $\mathbf{x}_{0}$ if and only if each $f_{j}$ is continuous at $\mathbf{x}_{0}$;
>5) if $m=n=1$, and $f$ is invertible and continuous on an open interval $I$ then its inverse $f^{-1}$ is also continuous.

>[!important] Proposition: composition of continuous functions
>Let $g\colon D\subseteq \mathbb{R}^{n}\rightarrow \mathbb{R}^{m}$ and $f\colon B\subseteq \mathbb{R}^{m}\rightarrow \mathbb{R}^{k}$. Suppose that $g(D)\subseteq B$ so that $f\circ g$ is defined on $D$. If $g$ is continuous at $\mathbf{x}_{0}$ and $f$ is continuous at $\mathbf{y}_{0}=g(\mathbf{x}_{0})$, then $f\circ g$ is continuous at $\mathbf{x}_{0}$.

^7370dd

>[!exmp] Example
>The limit of composite functions may be used to prove that 
>$$ \lim_{\mathbf{x}\rightarrow\mathbf{0}} \frac{\sin(\Vert\mathbf{x}\Vert^{2})}{\Vert\mathbf{x}\Vert^{2}} = 1.$$
>Indeed, we know that $\frac{\sin(t)}{t}$ can be extended to be continuous for every $t\in \mathbb{R}$ setting it equal to $1$ when $t=0$, and we also know that $\Vert \mathbf{x}\Vert^{2}=x_{1}^{2}+x_{2}^{2}+\cdots + x_{n}^{2}$ is continuous for every $\mathbf{x}=(x_{1},\cdots,x_{n})\in\mathbb{R}^{n}$. Therefore, a direct application of the [[2.2 Limits of multivariable functions#^7370dd|previous proposition]] gives the result.