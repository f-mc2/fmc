---
dg-home: false
dg-publish: true
---
# Derivatives of scalar functions

The derivative of the function $f$ at a point $x_{0}$ is nothing but a particular limit associated with $f$ and $x_{0}$. A way to understand it is to think to the following physical instance. There is an object $O$ moving on a line. Its position may be thought of as a function $\gamma\colon I\rightarrow\mathbb{R}$ where $I=(-t^{*},t^{*})$ is a time interval, and $\gamma(t)$ is the distance between $O$ and a reference point at time $t$. 

Associated with $\gamma$ and a choice of initial time $t_{0}$, there is the function 
$$
\Delta \gamma (t):=\frac{ \gamma(t)-\gamma(t_{0})}{t-t_{0}}.
$$
This function tells us the rate of change of the position of $O$ in the time interval $\Delta t=t-t_{0}$. Obviously, if $\Delta t$ is fixed, the bigger $\Delta \gamma$ the further $O$ has moved from the origin point $\gamma(t_{0})$. Consequently, $\Delta \gamma$ seems like a nice candidate to mathematically represent what we intuitively picture as velocity/rapidity.

However, upon closer examination, $\Delta \gamma$ can only inform us about velocity/rapidity in the time interval $\Delta t=t-t_{0}$, and we conclude $\Delta \gamma$ is more like an average velocity in the time interval $\Delta t$ rather than an instantaneous at $t=t_{0}$.  

What happens when $t=t_{0}$? 

Looking at the very definition of $\Delta \gamma$, we immediately realize that $t=t_{0}$ brings $0$ in the denominator.  We are thus led to a meaningless expression unless we remember that we introduced [[1.2 Limits of scalar functions#^cbf378|limits]] precisely to deal with these type of situations. Once limits enter the game, we obtain the notion of (left/right) derivative of $\gamma$ at the point $t=t_{0}$, and instantaneous velocities suddenly make sense!

>[!defn] Definition: derivative of a scalar function
>Let $f\colon D\rightarrow \mathbb{R}$ be a scalar function and let $x_{0}\in D$ be a [[1.2 Limits of scalar functions#^772289|left accumulation point]].  Consider the function $\Delta f\colon (x_{0},x_{0}+c)\subset D\rightarrow \mathbb{R}$ given by
>$$
>\Delta f(h):=\frac{f(x_{0}+h) - f(x_{0})}{h} .
>$$
>If the right limit 
>$$
>\lim_{h\rightarrow 0^{+}}\;\Delta f = l
>$$
>exists with $l\in\mathbb{R}$, then $l$ is called the right derivative of $f$ at $x_{0}$ and $f$ is said to be right differentiable at $x_{0}$. Starting from a right accumulation point, we obtain the notion of left derivative and left differentiability at $x_{0}$. If both the left and right derivatives  at $x_{0}$ exist and are equal, then we say that $f$ is differentiable at $x_{0}$ and write
>$$
>f'(x_{0})\equiv \lim_{h\rightarrow 0}\;\Delta f .
>$$
>We also write 
>$$
>\frac{\mathrm{d}f}{\mathrm{d} x}(x_{0})\equiv f'(x_{0}),
>$$
>and refer to this expression as the _Leibniz_ notation for derivatives.
>
>The function associated with the rule $x\mapsto f'(x)$ is called the **derivative** of $f$.  Its domain need not coincide with the domain of $f$.

^971f59

Both [[1.2 Limits of scalar functions#^979f48|continuity]] and differentiability are defined through limits, and it turns out there is a hierarchy between these two properties.

>[!important] Proposition: differentiability implies continuity
>If the function$f\colon D\rightarrow\mathbb{R}$ is (left/right) differentiable at $x_{0}\in D$, then it is also (left/right) continuous at $x_{0}$. 

The converse of the previous proposition does not hold, and the typical example of function which is continuous but not differentiable is the absolute value function given by
$$
|x|=\sqrt{x^{2}}=\left\{\begin{matrix}x & \mbox{ if } x\geq 0 \\ & \\ -x & \mbox{ if } x<0 .\end{matrix}\right.
$$
Indeed, we have
$$
\begin{split}
\lim_{x\rightarrow 0^{+}}\Delta |x|&=\lim_{x\rightarrow 0^{+}}\frac{x-0}{x}=1 \\ & \\
\lim_{x\rightarrow 0^{-}}\Delta |x|&=\lim_{x\rightarrow 0^{-}}\frac{-x-0}{x}=-1 ,
\end{split}
$$
which means that the left and right derivatives at $0$ exist but are different.

>[!exmp] Examples
>The first example of derivative we consider is that of a constant function $f(x)=c$ with $c\in \mathbb{R}$. Following the very definition of  derivative given above, we have
>$$
>f'(x_{0})=\lim_{h\rightarrow 0} \frac{f(x_{0}+h)-f(x_{0})}{h}=0
>$$
>for every $x_{0}\in\mathbb{R}$. 
>
>The second example we consider is that of the function $f(x)=x$:
>$$
>f'(x_{0})=\lim_{h\rightarrow 0} \frac{f(x_{0}+h)-f(x_{0})}{h}=\lim_{h\rightarrow 0} \frac{x_{0} + h -x_{0}}{h}=1
>$$
>for every $x_{0}\in\mathbb{R}$.
>
>The third example we consider is that of the function $f(x)=x^{2}$:
>$$
>\begin{split}
>f'(x_{0})&=\lim_{h\rightarrow 0} \frac{f(x_{0}+h)-f(x_{0})}{h}= \\ &=\lim_{h\rightarrow 0} \frac{(x_{0} + h)^{2} -x_{0}^{2}}{h}= \\ &\lim_{h\rightarrow 0} \frac{2x_{0}h + h^{2}}{h}=2x_{0}
>\end{split}
>$$
>for every $x_{0}\in\mathbb{R}$.

From an intuitive geometrical point of view, the derivative $f'(x_{0})$ of $f$ at $x_{0}$ may be interpreted as the value $\tan(\theta_{0})$ of a certain angle $\theta_{0}$  (refer to the figure adapted from [here](https://commons.wikimedia.org/wiki/File:Derivative_-_geometric_meaning.svg)):

![[geometric_derivative.png|500]]

Considering the square triangle with vertices $A\equiv(x_{0},f(x_{0}))$, $B\equiv(x_{0}+h,f(x_{0}))$, and $C\equiv(x_{0}+h,f(x_{0}+h))$, and denoting with $\theta_{h}$ the angle between the vectors $\vec{AB}$ and $\vec{AC}$, some  trigonometric considerations lead us to the equality
$$
\frac{f(x_{0}+h) - f(x_{0})}{h}=\tan(\theta_{h}).
$$
Then, as $h\rightarrow 0$, the segment $\vec{AC}$ becomes closer and closer to the line $L$ tangent to the graph of $f$ passing through the point $(x_{0},f(x_{0}))$, and the resulting limit is the derivative $f'(x_{0})=\tan(\theta_{0})$, with $\theta_{0}$ the angle between the horizontal axis and the tangent line $L$.

%%
We can visualize in which sense the derivative is related to the tangent line as follows ([source](https://commons.wikimedia.org/wiki/File:Graph_of_sliding_derivative_line_no_text.gif)):

![[Graph_of_sliding_derivative_line_no_text.gif]]
%%
Intuitively speaking, the line tangent to the graph of a function at a point is the limit of the secant lines connecting said point with other points on the graph. [Here](https://www.geogebra.org/m/n8ztr6qs) is an interactive visualization of this fact:

<iframe
  id="inlineFrameExample"
  title="Inline Frame Example"
  width="700"
  height="500"
  src="https://www.geogebra.org/m/n8ztr6qs">
</iframe>

>[!defn] Definition: $C^{1}$-functions
>The function $f\colon D\rightarrow\mathbb{R}$ is said to be differentiable in $D$ if it is (left/right) differentiable at all [[1.2 Limits of scalar functions#^772289|(left/right) accumulation points]] in $D$. The function $f'\colon D\rightarrow \mathbb{R}$ is called the **derivative** function.
>The function $f$ is said to be $C^{1}$ in $D$ if it is differentiable in $D$ and its derivative function is [[1.2 Limits of scalar functions#^979f48|continuous]] in $D$.

^c07f2b

The intuitive idea of derivative as rate of change may be easy to grasp because it relies on the idea of velocity which the vast majority of humans can understand (even if only in a non-rigorous way). However, this interpretation is not very helpful when the input of the functions considered is no longer a single number (which can always be thought of as being the "time parameter" labeling something that changes). 

A perspective which is suitable to multivariable functions is that of thinking of the derivative $f'(x_{0})$ as providing the best linear approximation of the function near $x_{0}$. Specifically, we can define $f'(x_{0})$ as the number (if it exists) such that
$$ \lim_{x\rightarrow x_{0}}\frac{\mid f(x) - [f(x_{0}) +f'(x_{0})\,(x-x_{0})]\mid}{\mid x-x_{0}\mid}=0. $$
Roughly speaking, the previous limit is telling us that the difference between $f(x)$ and $[f(x_{0}) +f'(x_{0})\,(x-x_{0})]$ gets smaller as $x$ approaches $x_{0}$, and it gets smaller "faster" than how $x$ approaches $x_{0}$ (because the limit with $x-x_{0}$ in the denominator is finite).

Consequently, when the derivative at $x_{0}$ exists, we may approximate reasonably well $f(x_{0})$ with the linear function $[f(x_{0}) + f'(x_{0})\,(x-x_{0})]$ as long as $x$ is close to $x_{0}$. 

Once we understand how the derivative is connected with linear approximation, we may go back to the interpretation of the derivative as velocity, and we can try to understand velocity as being connected with the best linear approximation of the position for small times.


# Properties of derivatives

^f3bc1e

Just as it happens for limits, the actual computation of derivatives may often be carried without directly computing the limit in its definition by suitably exploiting powerful results.

The first result is related with derivatives of sums and products.

>[!important] Proposition: Algebraic manipulations of derivatives
>Let $(\alpha_{1},\cdots\alpha_{n})$ be a finite sequence of real numbers and $(f_{1},\cdots,f_{n})$ a finite sequence of scalar functions. Suppose that $f_{j}'(x)$ exists for every $j=1,...,n$. Then, it holds
>$$
>\begin{split}
> \left(\sum_{j=1}^{n}\,\alpha_{j}\,f_{j}\right)'(x) &=\sum_{j=1}^{n}\, \alpha_{j}\,f_{j}'(x)   \\ & \\
> \left(\prod_{j=1}^{n} \,\alpha_{j}\,f_{j}\right)'(x) &=\sum_{j=1}^{n}\,\alpha_{j}\,f_{j}'(x)\,\left(\prod_{k\neq j}\alpha_{k}\,f_{k}(x)\right) .
>\end{split}
>$$
>In particular, if $n=2$ and $\alpha_{1}=\alpha_{2}=1$, the derivative of the product takes on the very well-known form (**Leibniz rule**)
>$$
>\left(f_{1}f_{2}\right)'(x)=f_{1}'(x)\,f_{2}(x) + f_{1}(x)\,f_{2}'(x).
>$$
>Moreover, if $n=2$, $f_{2}'(x)\neq0$ and $\alpha_{1}=\alpha_{2}=1$, the derivative of the quotient reads
>$$
>\left(\frac{ f_{1}}{f_{2}}\right)'(x)=\frac{f_{1}'(x)f_{2}(x) - f_{1}(x)\,f_{2}'(x)}{(f_{2}(x))^{2}}.
>$$

^5d20ad

Next, we consider the composition of functions and obtain the important **chain rule** for derivatives.

>[!important] Proposition: **Chain rule for derivatives**
>Let $f,g$ be scalar functions such that $g$ is differentiable at the point $x_{0}$ and $f$ is differentiable at the point $y_{0}=g'(x_{0})$. The composite function $f\circ g$ is differentiable at $x_{0}$ and its derivative reads
>$$
>(f\circ g)'(x_{0})=(f'\circ g)(x_{0})\,g'(x_{0}).
>$$
>If we introduce the new variable $y=g(x)$ we can exploit the _Leibniz_ notation for derivatives to write the chain rule above as
>$$
>\frac{\mathrm{d}f}{\mathrm{d}x}=\frac{\mathrm{d}f}{\mathrm{d}y}\frac{\mathrm{d}y}{\mathrm{d}x}.
>$$
>This expression is particularly useful if we want to extend the chain rule to the composition of more than two functions.

^c87d27

Finally, if a function admits an inverse, then their derivatives are connected to one another.

>[!important] Proposition: Differentiability of the inverse function 
>Let $g$ be a scalar function which is invertible and differentiable, and let $f$ denote its inverse. If $g(a)=b$ and $g'(a)\neq 0$ then $f$ is differentiable at $b=g(a)$ and it holds 
>$$
>f'(b)= \frac{1}{g'(a)} .
>$$

^05d051

A good discussion of all the subtleties involved in the previous proposition can be found in the section "Inverse function" of Michael Spivak's "Calculus" book.

# L'Hôpital's rule and indeterminate forms

Derivatives can help us in computing the limit of some quotients of functions in the indeterminate cases $\frac{0}{0}$ or $\frac{\pm\infty}{\pm\infty}$.

>[!important] Theorem: L'Hôpital's rule
>Let $x_{0}\in\overline{\mathbb{R}}:=\mathbb{R}\cup\{+\infty,-\infty\}$ and let $I$ be an open interval containing $x_{0}$ if $x_{0}\neq \pm\infty$, or having $x_{0}$ as an endpoint if $x_{0}=\pm\infty$. Let $f,g$ be scalar functions differentiable in $I$ except possibly at $x_{0}$. If 
>$$
>\lim_{x\rightarrow x_{0}}f(x)=\lim_{x\rightarrow x_{0}}g(x)=0\,\mbox{ or }\; \pm \infty,
>$$
>if $g'(x)\neq 0$ for every $x\in I$ such that $x\neq x_{0}$, and if 
>$$
>\lim_{x\rightarrow x_{0}}\frac{f'(x)}{g'(x)} = l \in\mathbb{R},
>$$
>then 
>$$
>\lim_{x\rightarrow x_{0}}\frac{f(x)}{g(x)} =\lim_{x\rightarrow x_{0}}\frac{f'(x)}{g'(x)} .
>$$
>Whenever even **one** of the previous assumptions fail, the result is not true!

Note that L'Hôpital's rule may be applied more than one time if necessary. For instance to compute

$$1)\quad \lim_{x\rightarrow 0} \frac{ax^{4}+bx^{3}+cx^{2}}{b'x^{3}+c'x^{2}} = \frac{c}{c'} ,$$

with  $(c,c')\neq (0,0)$.

Indeed, if we apply L'Hôpital's rule once, we get

$$2)\quad \lim_{x\rightarrow 0} \frac{4ax^{3}+3bx^{2}+2cx}{3b'x^{2}+2c'x} \stackrel{?}{=} \frac{0}{0}, $$

which means we can not solve 1) with 2). However, we can look at 2) on its own, an apply L'Hôpital's rule to it, obtaining

$$ 3)\quad \lim_{x\rightarrow 0} \frac{12ax^{2}+6bx +2c }{6b'x +2c'} = \frac{c}{c'}. $$

Since we assumed $(c,c')\neq (0,0)$, the right hand side of 3) makes sense, and thus L'Hôpital's rule can be applied to conclude that the result of 2) is $\frac{c}{c'}$. However, once we are here, we know that the limit in 2) makes sense, and we can  L'Hôpital's rule applies again to conclude that the result of 1) is $\frac{c}{c'}$.


L'Hôpital's rule is very powerful and useful, but it is easy to abuse it or apply it incorrectly. In particular, it is relevant to mention the case 
$$
\lim_{x\rightarrow 0} \frac{\sin(x)}{x}=1
$$

^77ad27

even if we did not yet introduced the trigonometric function $\sin(x)$. Suppose trigonometric functions are introduced just making reference to the geometry of angles as it is often done. If we want to apply L'Hôpital's rule to check the previous limit, we need to compute the derivative of $\sin(x)$ at $x=0$. Assuming we know that $\sin(0)=0$, by the very definition of derivative, we get
$$
\sin'(0)=\lim_{h\rightarrow 0} \frac{\sin(0+h) -\sin(0)}{h}=\lim_{h\rightarrow 0}\frac{\sin(h)}{h}.
$$
It is thus clear that we reach a circular situation because evaluating $\sin'(x)$ is equivalent to using L'Hôpital's rule to solve the indeterminate limit! 

This instance should make clear that L'Hôpital's rule should never be applied without really understanding what is going on at a deeper level. 

>[!rem] Remark
>We [[1.5 Power series, trascendental functions, and Taylor series#^ed7011|will see]] that L'Hôpital's rule can be applied to solve the previous limit if we introduce trigonometric functions using power series.  
# Second and higher order derivatives

By definition, if the scalar function $f$ is $C^{1}$ in $D$, its derivative $f'$ is a continuous function in $D$. Therefore, it makes sense to consider $f'$ as a scalar function on its own, say $g\equiv f'$, and investigate its differentiability properties. If everything works fine, we obtain the **second derivative** $f''$ of $f$. In the $\frac{\mathrm{d}}{\mathrm{d}x}$-notation for derivatives introduced above, the second derivative is written as
$$
\frac{\mathrm{d}^{2}f}{\mathrm{d}x^{2}} =  \frac{\mathrm{d}}{\mathrm{d}x}\left(\frac{\mathrm{d}f}{\mathrm{d}x}\right)=f''.
$$
>[!exmp] Examples
>It should not be surprising that the second derivative of the constant function $f(x)=c$ identically vanishes because $f'(x)=0$.
>
>Quite similarly, the second derivative of $f(x)=x$ also vanishes. Indeed, since $f'(x)=1$, setting $g(x)=f'(x)=1$, we obtain that $f''(x)=g'(x)=0$ because $g'(x)$ is a constant function.
>
>On the other hand, if we consider the function $f(x)=x^{2}$, we know from the examples above that $f'(x)=2x$, so that, setting $g(x)=f'(x)$, it immediately follows that $f''(x)=g'(x)=2$.

As long as things make sense, we can take derivatives as many times as we want, thus reaching the third derivative $f'''=f^{(3)}$, or the fourth derivative $f^{(4)}$, or even the 26453271935261-th derivative $f^{(26453271935261)}$.

>[!defn] Definition: $C^{k}$ and smooth functions
>Given $k\in\mathbb{N}-\{0\}$, a function $f\colon D \rightarrow \mathbb{R}$ is said to be $C^{k}$ on $D$ if its $k$-th order derivative exists and is continuous on $D$. A continuous function is also said to be $C^{0}$. If $f$ is $C^{k}$ for all $k\in\mathbb{K}$, then it is called $C^{\infty}$ or smooth.

^a6b2f2


