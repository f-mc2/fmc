[
  {
    "objectID": "publications/misc.html",
    "href": "publications/misc.html",
    "title": "Miscellanea",
    "section": "",
    "text": "Various works on various (sparse) topics.\nThe list is ordered according to the date of publication/arXiv submission, and not the date of creation.\n\n\nP. Aniello, F.M. Ciaglia, F. Di Cosmo, G. Marmo, and J.M. Pérez-Pardo: Time, classical and quantum: Annals of physics 373: 2016-10: doi: 10.1016/j.aop.2016.08.001. Available at: https://arxiv.org/abs/1605.03534\n\n\nM. Asorey, M. Ciaglia, F. Di Cosmo, and A. Ibort: Covariant brackets for particles and fields: Modern physics letters a 32(19): 2017-06: doi: 10.1142/S0217732317501000. Available at: https://arxiv.org/abs/1705.00235\n\n\nM. Asorey, F.M. Ciaglia, F. Di Cosmo, and A. Ibort: Erratum: “Covariant brackets for particles and fields”: Modern physics letters a 32(22): 2017-07: doi: 10.1142/S021773231792002X\n\n\nM. Asorey, F.M. Ciaglia, F. Di Cosmo, A. Ibort, and G. Marmo: Covariant Jacobi brackets for test particles: Modern physics letters a 32(23): 2017-07: doi: 10.1142/S021773231750122X\n\n\nF.M. Ciaglia, F.D. Cosmo, M. Laudato, and G. Marmo: Differential calculus on manifolds with boundary applications: International journal of geometric methods in modern physics 14(08): 2017-08: doi: 10.1142/S0219887817400035. Available at: https://arxiv.org/abs/1705.02974\n\n\nF.M. Ciaglia, F. Di Cosmo, A. Ibort, M. Laudato, and G. Marmo: Dynamical Vector Fields on the Manifold of Quantum States: Open systems & information dynamics 24(03): 2017-09: doi: 10.1142/S1230161217400030. Available at: https://www.worldscientific.com/doi/abs/10.1142/S1230161217400030\n\n\nF.M. Ciaglia, F. Di Cosmo, A. Ibort, and G. Marmo: Dynamical aspects in the quantizer–dequantizer formalism: Annals of physics 385: 2017-10: doi: 10.1016/j.aop.2017.08.025\n\n\nF.M. Ciaglia, H. Cruz, and G. Marmo: Contact manifolds and dissipation, classical and quantum: Annals of physics 398: 2018-11: doi: 10.1016/j.aop.2018.09.012. Available at: https://linkinghub.elsevier.com/retrieve/pii/S0003491618302574\n\n\nF.M. Ciaglia, A. Ibort, and G. Marmo: Differential Geometry of Quantum States, Observables and Evolution: Quantum Physics and Geometry: 2019: doi: 10.1007/978-3-030-06122-7_7. Available at: http://link.springer.com/10.1007/978-3-030-06122-7_7\n\n\nF.M. Ciaglia, G. Marmo, and L. Schiavone: From Classical Trajectories to Quantum Commutation Relations: Classical and Quantum Physics: 2019: Available at: http://doi.org/10.1007/978-3-030-24748-5_9\n\n\nF.M. Ciaglia, A. Ibort, and G. Marmo: On the Notion of Composite System: Geometric Science of Information: 2019: doi: 10.1007/978-3-030-26980-7_67. Available at: http://link.springer.com/10.1007/978-3-030-26980-7_67\n\n\nD. Chruściński, F.M. Ciaglia, A. Ibort, G. Marmo, and F. Ventriglia: Stratified manifold of quantum states, actions of the complex special linear group: Annals of physics 400: 2019-01: doi: 10.1016/j.aop.2018.11.015. Available at: https://linkinghub.elsevier.com/retrieve/pii/S0003491618303002\n\n\nF.M. Ciaglia, F. Di Cosmo, A. Ibort, and G. Marmo: Descriptions of Relativistic Dynamics with World Line Condition: Quantum reports 1(2): 2019-10: doi: 10.3390/quantum1020016. Available at: https://www.mdpi.com/2624-960X/1/2/16\n\n\nF.M. Ciaglia, F. Di Cosmo, G. Marmo, and L. Schiavone: Evolutionary equations and constraints: Maxwell equations: Journal of mathematical physics 60(11): 2019-11: doi: 10.1063/1.5109087. Available at: http://aip.scitation.org/doi/10.1063/1.5109087\n\n\nF.M. Ciaglia, F. Di Cosmo, A. Figueroa, V.I. Man’ko, G. Marmo, L. Schiavone, F. Ventriglia, and P. Vitale: Nonlinear Dynamics from Linear Quantum Evolutions: Annals of physics 411: 2019-12: doi: 10.1016/j.aop.2019.167957. Available at: http://arxiv.org/abs/1908.03699\n\n\nF.M. Ciaglia, F. Di Cosmo, A. Ibort, G. Marmo, L. Schiavone, and A. Zampini: Lagrangian description of Heisenberg and Landau–von Neumann equations of motion: Modern physics letters a 35(19): 2020-06: doi: 10.1142/S0217732320501618. Available at: https://www.worldscientific.com/doi/abs/10.1142/S0217732320501618\n\n\nF.M. Ciaglia, F. Di Cosmo, A. Ibort, G. Marmo, and L. Schiavone: Covariant reduction of classical Hamiltonian Field Theories: From D’Alembert to Klein–Gordon and Schrödinger: Modern physics letters a 35(23): 2020-07: doi: 10.1142/S0217732320502144. Available at: https://www.worldscientific.com/doi/abs/10.1142/S0217732320502144\n\n\nF.M. Ciaglia, F. Di Cosmo, A. Ibort, G. Marmo, and L. Schiavone: Covariant Variational Evolution and Jacobi brackets: Fields: Modern physics letters a 35(23): 2020-07: doi: 10.1142/S0217732320502065. Available at: https://www.worldscientific.com/doi/abs/10.1142/S0217732320502065\n\n\nF.M. Ciaglia, F. Di Cosmo, A. Ibort, G. Marmo, and L. Schiavone: Covariant variational evolution and Jacobi brackets: Particles: Modern physics letters a 35(23): 2020-07: doi: 10.1142/S0217732320200011. Available at: https://www.worldscientific.com/doi/abs/10.1142/S0217732320200011\n\n\nF.M. Ciaglia, F. Di Cosmo, A. Ibort, G. Marmo, L. Schiavone, and A. Zampini: Symmetries and Covariant Poisson Brackets on Presymplectic Manifolds: Symmetry 14(1): 2022-01: doi: 10.3390/sym14010070. Available at: https://www.mdpi.com/2073-8994/14/1/70\n\n\nF.M. Ciaglia, F. Di Cosmo, A. Ibort, G. Marmo, L. Schiavone, and A. Zampini: The Geometry of the solution space of first order Hamiltonian field theories I: From particle dynamics to free Electrodynamics: 2022-08: Available at: http://arxiv.org/abs/2208.14136\n\n\nF.M. Ciaglia, F. Di Cosmo, A. Ibort, G. Marmo, L. Schiavone, and A. Zampini: The Geometry of the solution space of first order Hamiltonian field theories II: Non-Abelian gauge theories: 2022-08: Available at: http://arxiv.org/abs/2208.14155\n\n\nF.M. Ciaglia and F. Di Cosmo: Some Remarks on the Notion of Transition: Geometric Science of Information: 2023: doi: 10.1007/978-3-031-38299-4_42\n\n\nF.M. Ciaglia, F. Di Cosmo, A. Ibort, G. Marmo, L. Schiavone, and A. Zampini: The geometry of the solution space of first order Hamiltonian field theories I: From particle dynamics to free electrodynamics: Journal of geometry and physics 204: 2024-10: doi: 10.1016/j.geomphys.2024.105279. Available at: https://www.sciencedirect.com/science/article/pii/S0393044024001803",
    "crumbs": [
      "Home",
      "Publications",
      "Miscellanea"
    ]
  },
  {
    "objectID": "publications/grpd.html",
    "href": "publications/grpd.html",
    "title": "Groupoidal picture of Quantum Mechanics",
    "section": "",
    "text": "This project aims to investigate the use of groupoids in the context of quantum mechanics, quantum information theory, and quantum field theory. The idea of associating a groupoid with a quantum system may be traced back to Schwinger’s seminal work.\nThe list is ordered according to the date of publication/arXiv submission, and not the date of creation.\n\n\nF.M. Ciaglia, A. Ibort, and G. Marmo: A gentle introduction to Schwinger’s formulation of quantum mechanics: The groupoid picture: Modern physics letters a 33(20): 2018-06: doi: 10.1142/S0217732318501225. Available at: https://arxiv.org/abs/1807.00519\n\n\nF.M. Ciaglia, A. Ibort, and G. Marmo: Schwinger’s picture of quantum mechanics I: Groupoids: International journal of geometric methods in modern physics 16(08): 2019-08: doi: 10.1142/S0219887819501196. Available at: https://arxiv.org/abs/1905.12274\n\n\nF.M. Ciaglia, A. Ibort, and G. Marmo: Schwinger’s picture of quantum mechanics II: Algebras and observables: International journal of geometric methods in modern physics 16(09): 2019-09: doi: 10.1142/S0219887819501366. Available at: https://arxiv.org/abs/1907.03883\n\n\nF.M. Ciaglia, A. Ibort, and G. Marmo: Schwinger’s picture of quantum mechanics III: The statistical interpretation: International journal of geometric methods in modern physics 16(11): 2019-11: doi: 10.1142/S0219887819501652. Available at: https://arxiv.org/abs/1909.07265\n\n\nF.M. Ciaglia, F. Di Cosmo, A. Ibort, and G. Marmo: Schwinger’s picture of quantum mechanics: International journal of geometric methods in modern physics 17(04): 2020-03: doi: 10.1142/S0219887820500541. Available at: https://arxiv.org/abs/2002.09326\n\n\nF.M. Ciaglia, F. Di Cosmo, A. Ibort, and G. Marmo: Schwinger’s picture of quantum mechanics IV: Composition and independence: International journal of geometric methods in modern physics 17(04): 2020-03: doi: 10.1142/S0219887820500589. Available at: https://arxiv.org/abs/2004.02472\n\n\nF.M. Ciaglia, F.D. Di Cosmo, A. Ibort, and G. Marmo: Evolution of Classical and Quantum States in the Groupoid Picture of Quantum Mechanics: Entropy 22(11): 2020-11: doi: 10.3390/e22111292. Available at: https://arxiv.org/abs/2012.10284\n\n\nF.M. Ciaglia, F. Di Cosmo, A. Ibort, G. Marmo, L. Schiavone, and A. Zampini: A quantum route to the classical Lagrangian formalism: Modern physics letters a 36(15): 2021-05: doi: 10.1142/S0217732321500917. Available at: https://arxiv.org/abs/2106.00998\n\n\nF.M. Ciaglia, F. Di Cosmo, A. Ibort, G. Marmo, L. Schiavone, and A. Zampini: Feynman’s propagator in Schwinger’s picture of Quantum Mechanics: Modern physics letters a 36(26): 2021-08: doi: 10.1142/S021773232150187X. Available at: https://arxiv.org/abs/2109.05756\n\n\nF.M. Ciaglia, F.D. Cosmo, A. Ibort, G. Marmo, and L. Schiavone: Schwinger’s picture of quantum mechanics: 2-groupoids and symmetries: Journal of geometric mechanics 13(3): 2021-09: doi: 10.3934/jgm.2021008. Available at: https://arxiv.org/abs/2104.13880\n\n\nF.M. Ciaglia, F. Di Cosmo, A. Ibort, G. Marmo, L. Schiavone, and A. Zampini: Causality in Schwinger’s Picture of Quantum Mechanics: Entropy 24(1): 2022-01: doi: 10.3390/e24010075. Available at: https://www.mdpi.com/1099-4300/24/1/75\n\n\nF.M. Ciaglia, F. Di Cosmo, A. Ibort, and G. Marmo: Quantum tomography and Schwinger’s picture of quantum mechanics: Journal of physics a: Mathematical and theoretical 55(27): 2022-07: doi: 10.1088/1751-8121/ac7591. Available at: https://arxiv.org/abs/2205.00170\n\n\nF.M. Ciaglia, F. Di Cosmo, A. Ibort, and G. Marmo: Dynamical Maps and Symmetroids: Open systems & information dynamics 28(04): 2022-07: doi: 10.1142/S1230161221500190. Available at: https://arxiv.org/abs/2205.06734\n\n\nF.M. Ciaglia, F. Di Cosmo, P. Facchi, A. Ibort, A. Konderak, and G. Marmo: Groupoid and algebra of the infinite quantum spin chain: Journal of geometry and physics 191: 2023-06: doi: 10.1016/j.geomphys.2023.104901. Available at: https://arxiv.org/abs/2302.01050\n\n\nF.M. Ciaglia, F. Di Cosmo, A. Ibort, and G. Marmo: The groupoidal picture of quantum mechanics: Journal of geometry and physics 197: 2024-01: doi: 10.1016/j.geomphys.2023.105095. Available at: https://doi.org/10.1016/j.geomphys.2023.105095",
    "crumbs": [
      "Home",
      "Publications",
      "Groupoidal picture of Quantum Mechanics"
    ]
  },
  {
    "objectID": "publications/articles.html",
    "href": "publications/articles.html",
    "title": "Articles",
    "section": "",
    "text": "The list is ordered according to the date of publication, and not the date of creation.\n\n\nP. Aniello, F.M. Ciaglia, F. Di Cosmo, G. Marmo, and J.M. Pérez-Pardo: Time, classical and quantum: Annals of physics 373: 2016-10: doi: 10.1016/j.aop.2016.08.001. Available at: https://arxiv.org/abs/1605.03534\n\n\nM. Asorey, M. Ciaglia, F. Di Cosmo, and A. Ibort: Covariant brackets for particles and fields: Modern physics letters a 32(19): 2017-06: doi: 10.1142/S0217732317501000. Available at: https://arxiv.org/abs/1705.00235\n\n\nF.M. Ciaglia, F. Di Cosmo, D. Felice, S. Mancini, G. Marmo, and J.M. Pérez-Pardo: Hamilton-Jacobi approach to potential functions in information geometry: Journal of mathematical physics 58(6): 2017-06: doi: 10.1063/1.4984941. Available at: https://arxiv.org/abs/1608.06584\n\n\nM. Asorey, F.M. Ciaglia, F. Di Cosmo, and A. Ibort: Erratum: “Covariant brackets for particles and fields”: Modern physics letters a 32(22): 2017-07: doi: 10.1142/S021773231792002X\n\n\nM. Asorey, F.M. Ciaglia, F. Di Cosmo, A. Ibort, and G. Marmo: Covariant Jacobi brackets for test particles: Modern physics letters a 32(23): 2017-07: doi: 10.1142/S021773231750122X\n\n\nF.M. Ciaglia, F.D. Cosmo, M. Laudato, and G. Marmo: Differential calculus on manifolds with boundary applications: International journal of geometric methods in modern physics 14(08): 2017-08: doi: 10.1142/S0219887817400035. Available at: https://arxiv.org/abs/1705.02974\n\n\nF.M. Ciaglia, F. Di Cosmo, A. Ibort, M. Laudato, and G. Marmo: Dynamical Vector Fields on the Manifold of Quantum States: Open systems & information dynamics 24(03): 2017-09: doi: 10.1142/S1230161217400030. Available at: https://www.worldscientific.com/doi/abs/10.1142/S1230161217400030\n\n\nF.M. Ciaglia, F. Di Cosmo, A. Ibort, and G. Marmo: Dynamical aspects in the quantizer–dequantizer formalism: Annals of physics 385: 2017-10: doi: 10.1016/j.aop.2017.08.025\n\n\nF.M. Ciaglia, A. Ibort, and G. Marmo: Geometrical structures for classical and quantum probability spaces: International journal of quantum information 15(08): 2017-12: doi: 10.1142/s021974991740007x. Available at: https://arxiv.org/abs/1711.09774\n\n\nF.M. Ciaglia, F.D. Cosmo, D. Felice, S. Mancini, G. Marmo, and J.M. Pérez-Pardo: Aspects of Geodesical Motion with Fisher-Rao Metric: Classical and Quantum: Open systems & information dynamics 25(01): 2018-03: doi: 10.1142/s1230161218500051. Available at: https://arxiv.org/abs/1608.06105\n\n\nF.M. Ciaglia, A. Ibort, and G. Marmo: A gentle introduction to Schwinger’s formulation of quantum mechanics: The groupoid picture: Modern physics letters a 33(20): 2018-06: doi: 10.1142/S0217732318501225. Available at: https://arxiv.org/abs/1807.00519\n\n\nF.M. Ciaglia, F. Di Cosmo, M. Laudato, G. Marmo, F.M. Mele, F. Ventriglia, and P. Vitale: A pedagogical intrinsic approach to relative entropies as potential functions of quantum metrics: The q – z family: Annals of physics 395: 2018-08: doi: 10.1016/j.aop.2018.05.015. Available at: https://arxiv.org/abs/1807.00519\n\n\nF.M. Ciaglia, H. Cruz, and G. Marmo: Contact manifolds and dissipation, classical and quantum: Annals of physics 398: 2018-11: doi: 10.1016/j.aop.2018.09.012. Available at: https://linkinghub.elsevier.com/retrieve/pii/S0003491618302574\n\n\nD. Chruściński, F.M. Ciaglia, A. Ibort, G. Marmo, and F. Ventriglia: Stratified manifold of quantum states, actions of the complex special linear group: Annals of physics 400: 2019-01: doi: 10.1016/j.aop.2018.11.015. Available at: https://linkinghub.elsevier.com/retrieve/pii/S0003491618303002\n\n\nF.M. Ciaglia, G. Marmo, and J.M. Pérez-Pardo: Generalized potential functions in differential geometry and information geometry: International journal of geometric methods in modern physics 16(01): 2019-02: doi: 10.1142/s0219887819400024. Available at: https://arxiv.org/abs/1804.10414\n\n\nF.M. Ciaglia, A. Ibort, and G. Marmo: Schwinger’s picture of quantum mechanics I: Groupoids: International journal of geometric methods in modern physics 16(08): 2019-08: doi: 10.1142/S0219887819501196. Available at: https://arxiv.org/abs/1905.12274\n\n\nF.M. Ciaglia, A. Ibort, and G. Marmo: Schwinger’s picture of quantum mechanics II: Algebras and observables: International journal of geometric methods in modern physics 16(09): 2019-09: doi: 10.1142/S0219887819501366. Available at: https://arxiv.org/abs/1907.03883\n\n\nF.M. Ciaglia, F. Di Cosmo, A. Ibort, and G. Marmo: Descriptions of Relativistic Dynamics with World Line Condition: Quantum reports 1(2): 2019-10: doi: 10.3390/quantum1020016. Available at: https://www.mdpi.com/2624-960X/1/2/16\n\n\nF.M. Ciaglia, F. Di Cosmo, G. Marmo, and L. Schiavone: Evolutionary equations and constraints: Maxwell equations: Journal of mathematical physics 60(11): 2019-11: doi: 10.1063/1.5109087. Available at: http://aip.scitation.org/doi/10.1063/1.5109087\n\n\nF.M. Ciaglia, A. Ibort, and G. Marmo: Schwinger’s picture of quantum mechanics III: The statistical interpretation: International journal of geometric methods in modern physics 16(11): 2019-11: doi: 10.1142/S0219887819501652. Available at: https://arxiv.org/abs/1909.07265\n\n\nF.M. Ciaglia, A. Ibort, J. Jost, and G. Marmo: Manifolds of classical probability distributions and quantum density operators in infinite dimensions: Information geometry 2(2): 2019-12: doi: 10.1007/s41884-019-00022-1. Available at: https://arxiv.org/abs/1907.00732\n\n\nF.M. Ciaglia, F. Di Cosmo, A. Figueroa, V.I. Man’ko, G. Marmo, L. Schiavone, F. Ventriglia, and P. Vitale: Nonlinear Dynamics from Linear Quantum Evolutions: Annals of physics 411: 2019-12: doi: 10.1016/j.aop.2019.167957. Available at: http://arxiv.org/abs/1908.03699\n\n\nF.M. Ciaglia, F.D. Cosmo, A. Figueroa, G. Marmo, and L. Schiavone: Geometry from divergence functions and complex structures: International journal of quantum information 18(01): 2020-02: doi: 10.1142/s021974991941020x. Available at: https://arxiv.org/abs/2002.02891\n\n\nF.M. Ciaglia, F. Di Cosmo, A. Ibort, and G. Marmo: Schwinger’s picture of quantum mechanics: International journal of geometric methods in modern physics 17(04): 2020-03: doi: 10.1142/S0219887820500541. Available at: https://arxiv.org/abs/2002.09326\n\n\nF.M. Ciaglia, F. Di Cosmo, A. Ibort, and G. Marmo: Schwinger’s picture of quantum mechanics IV: Composition and independence: International journal of geometric methods in modern physics 17(04): 2020-03: doi: 10.1142/S0219887820500589. Available at: https://arxiv.org/abs/2004.02472\n\n\nF.M. Ciaglia: Quantum states, groups and monotone metric tensors: The european physical journal plus 135(6): 2020-06: doi: 10.1140/epjp/s13360-020-00537-y. Available at: https://arxiv.org/abs/2006.10595\n\n\nF.M. Ciaglia, F. Di Cosmo, A. Ibort, G. Marmo, L. Schiavone, and A. Zampini: Lagrangian description of Heisenberg and Landau–von Neumann equations of motion: Modern physics letters a 35(19): 2020-06: doi: 10.1142/S0217732320501618. Available at: https://www.worldscientific.com/doi/abs/10.1142/S0217732320501618\n\n\nF.M. Ciaglia, J. Jost, and L. Schwachhöfer: From the Jordan Product to Riemannian Geometries on Classical and Quantum States: Entropy 22(6): 2020-06: doi: 10.3390/e22060637. Available at: https://arxiv.org/abs/2005.02023\n\n\nF.M. Ciaglia, F. Di Cosmo, A. Ibort, G. Marmo, and L. Schiavone: Covariant reduction of classical Hamiltonian Field Theories: From D’Alembert to Klein–Gordon and Schrödinger: Modern physics letters a 35(23): 2020-07: doi: 10.1142/S0217732320502144. Available at: https://www.worldscientific.com/doi/abs/10.1142/S0217732320502144\n\n\nF.M. Ciaglia, F. Di Cosmo, A. Ibort, G. Marmo, and L. Schiavone: Covariant Variational Evolution and Jacobi brackets: Fields: Modern physics letters a 35(23): 2020-07: doi: 10.1142/S0217732320502065. Available at: https://www.worldscientific.com/doi/abs/10.1142/S0217732320502065\n\n\nF.M. Ciaglia, F. Di Cosmo, A. Ibort, G. Marmo, and L. Schiavone: Covariant variational evolution and Jacobi brackets: Particles: Modern physics letters a 35(23): 2020-07: doi: 10.1142/S0217732320200011. Available at: https://www.worldscientific.com/doi/abs/10.1142/S0217732320200011\n\n\nF.M. Ciaglia, F.D. Di Cosmo, A. Ibort, and G. Marmo: Evolution of Classical and Quantum States in the Groupoid Picture of Quantum Mechanics: Entropy 22(11): 2020-11: doi: 10.3390/e22111292. Available at: https://arxiv.org/abs/2012.10284\n\n\nF.M. Ciaglia, J. Jost, and L. Schwachhöfer: Differential Geometric Aspects of Parametric Estimation Theory for States on Finite-Dimensional C\\(\\ast\\)-Algebras: Entropy 22(11): 2020-11: doi: 10.3390/e22111332. Available at: https://arxiv.org/abs/2010.14394\n\n\nF.M. Ciaglia, F. Di Cosmo, A. Ibort, G. Marmo, L. Schiavone, and A. Zampini: A quantum route to the classical Lagrangian formalism: Modern physics letters a 36(15): 2021-05: doi: 10.1142/S0217732321500917. Available at: https://arxiv.org/abs/2106.00998\n\n\nF.M. Ciaglia, S. Mancini, and M. Ha Quang: Focus point: Classical and quantum information geometry: The european physical journal plus 136(5): 2021-05: doi: 10.1140/epjp/s13360-021-01541-6\n\n\nF.M. Ciaglia, F. Di Cosmo, A. Ibort, G. Marmo, L. Schiavone, and A. Zampini: Feynman’s propagator in Schwinger’s picture of Quantum Mechanics: Modern physics letters a 36(26): 2021-08: doi: 10.1142/S021773232150187X. Available at: https://arxiv.org/abs/2109.05756\n\n\nF.M. Ciaglia, F.D. Cosmo, A. Ibort, G. Marmo, and L. Schiavone: Schwinger’s picture of quantum mechanics: 2-groupoids and symmetries: Journal of geometric mechanics 13(3): 2021-09: doi: 10.3934/jgm.2021008. Available at: https://arxiv.org/abs/2104.13880\n\n\nF.M. Ciaglia, F. Di Cosmo, A. Ibort, G. Marmo, L. Schiavone, and A. Zampini: Causality in Schwinger’s Picture of Quantum Mechanics: Entropy 24(1): 2022-01: doi: 10.3390/e24010075. Available at: https://www.mdpi.com/1099-4300/24/1/75\n\n\nF.M. Ciaglia, F. Di Cosmo, A. Ibort, G. Marmo, L. Schiavone, and A. Zampini: Symmetries and Covariant Poisson Brackets on Presymplectic Manifolds: Symmetry 14(1): 2022-01: doi: 10.3390/sym14010070. Available at: https://www.mdpi.com/2073-8994/14/1/70\n\n\nF.M. Ciaglia, F. Di Cosmo, A. Ibort, and G. Marmo: Quantum tomography and Schwinger’s picture of quantum mechanics: Journal of physics a: Mathematical and theoretical 55(27): 2022-07: doi: 10.1088/1751-8121/ac7591. Available at: https://arxiv.org/abs/2205.00170\n\n\nF.M. Ciaglia, F. Di Cosmo, A. Ibort, and G. Marmo: Dynamical Maps and Symmetroids: Open systems & information dynamics 28(04): 2022-07: doi: 10.1142/S1230161221500190. Available at: https://arxiv.org/abs/2205.06734\n\n\nF.M. Ciaglia and F. Di Nocera: Group Actions and Monotone Quantum Metric Tensors: Mathematics 10(15): 2022-07: doi: 10.3390/math10152613. Available at: https://arxiv.org/abs/2206.10394\n\n\nF.M. Ciaglia, F. Di Cosmo, P. Facchi, A. Ibort, A. Konderak, and G. Marmo: Groupoid and algebra of the infinite quantum spin chain: Journal of geometry and physics 191: 2023-06: doi: 10.1016/j.geomphys.2023.104901. Available at: https://arxiv.org/abs/2302.01050\n\n\nF.M. Ciaglia, F. Di Cosmo, laura González-Bravo, A. Ibort, and G. Marmo: The categorical foundations of quantum information theory: Categories and the Cramér–Rao inequality: Mod. Phys. Lett. a 38(16 & 17): 2023-08: doi: 10.1142/S0217732323500852. Available at: https://arxiv.org/abs/2309.10428\n\n\nF.M. Ciaglia, J. Jost, and L.J. Schwachhöfer: Information geometry, jordan algebras, and a coadjoint orbit-like construction: SIGMA 19: 2023-10: doi: 10.3842/SIGMA.2023.078. Available at: https://arxiv.org/abs/2112.09781\n\n\nF.M. Ciaglia, F. Di Cosmo, A. Ibort, and G. Marmo: G-dual Teleparallel Connections in Information Geometry: Information geometry 7(1): 2024-01: doi: 10.1007/s41884-023-00117-w. Available at: https://arxiv.org/abs/2207.08694\n\n\nF.M. Ciaglia, F. Di Cosmo, A. Ibort, and G. Marmo: The groupoidal picture of quantum mechanics: Journal of geometry and physics 197: 2024-01: doi: 10.1016/j.geomphys.2023.105095. Available at: https://doi.org/10.1016/j.geomphys.2023.105095\n\n\nF.M. Ciaglia, F. Di Nocera, J. Jost, and L. Schwachhöfer: Parametric models and information geometry on W*-algebras: Information geometry 7(1): 2024-01: doi: 10.1007/s41884-022-00094-6. Available at: https://arxiv.org/abs/2207.09396\n\n\nF.M. Ciaglia, F. Di Cosmo, F. Di Nocera, and P. Vitale: Monotone metric tensors in quantum information geometry: International journal of geometric methods in modern physics 21(10): 2024-09: doi: 10.1142/S0219887824400048. Available at: https://arxiv.org/abs/2203.10857\n\n\nF.M. Ciaglia, F. Di Cosmo, A. Ibort, G. Marmo, L. Schiavone, and A. Zampini: The geometry of the solution space of first order Hamiltonian field theories I: From particle dynamics to free electrodynamics: Journal of geometry and physics 204: 2024-10: doi: 10.1016/j.geomphys.2024.105279. Available at: https://www.sciencedirect.com/science/article/pii/S0393044024001803\n\n\nF.M. Ciaglia, S. Jiang, J. Jost, and L. Schwachhöfer: A coadjoint orbit–like construction for Jordan superalgebras: Journal of geometry and physics 209: 2025-03: doi: 10.1016/j.geomphys.2024.105404. Available at: https://arxiv.org/abs/2311.01333",
    "crumbs": [
      "Home",
      "Publications",
      "Articles"
    ]
  },
  {
    "objectID": "publications/index.html",
    "href": "publications/index.html",
    "title": "Publications",
    "section": "",
    "text": "All my publications are available either on my arXiv page or in open access on the publisher page.",
    "crumbs": [
      "Home",
      "Publications"
    ]
  },
  {
    "objectID": "publications/index.html#list-of-publications-by-topics",
    "href": "publications/index.html#list-of-publications-by-topics",
    "title": "Publications",
    "section": "List of publications by topics",
    "text": "List of publications by topics\n\nInformation Geometry\nGroupoid picture of Quantum Mechanics\nMiscellanea",
    "crumbs": [
      "Home",
      "Publications"
    ]
  },
  {
    "objectID": "publications/index.html#list-of-publications-by-types",
    "href": "publications/index.html#list-of-publications-by-types",
    "title": "Publications",
    "section": "List of publications by types",
    "text": "List of publications by types\n\nPreprints\nArticles\nProceedings",
    "crumbs": [
      "Home",
      "Publications"
    ]
  },
  {
    "objectID": "publications/index.html#editorials",
    "href": "publications/index.html#editorials",
    "title": "Publications",
    "section": "Editorials",
    "text": "Editorials\n\nI guest-edited the research topic on “Advances in Information Geometry: beyond the conventional approach” in Frontiers in Physics.\nI guest-edited the focus point “Classical and Quantum Information Geometry” in the European Physical Journal Plus.",
    "crumbs": [
      "Home",
      "Publications"
    ]
  },
  {
    "objectID": "teaching/calculus-uc3m-robotics/1.4 Sequences and series.html",
    "href": "teaching/calculus-uc3m-robotics/1.4 Sequences and series.html",
    "title": "Limits of numerical sequences",
    "section": "",
    "text": "Limits of numerical sequences\nIntuitively speaking, numerical sequences are a discrete analogue of scalar functions of the form \\((D,\\mathbb{R},f,\\mathbb{R})\\). Following this idea, we will develop a way to discuss limits of numerical sequences in analogy with [[1.2 Limits of scalar functions|limits of scalar functions]].\n\n[!defn] Definition A numerical sequence is a function \\(a\\colon \\mathbb{N}\\rightarrow\\mathbb{R}\\). A numerical sequence is also written as \\(\\{a_{n}\\}_{n\\in\\mathbb{N}}\\) (or just \\(\\{a_{n}\\}\\), depending on the degree of laziness) where \\(a_{n}=a(n)\\). A numerical sequence is called: - increasing if \\(a_{n}&lt;a_{n+1}\\) for all \\(n\\in\\mathbb{N}\\); - non-decreasing if \\(a_{n}\\leq a_{n+1}\\) for all \\(n\\in\\mathbb{N}\\); - decreasing if \\(a_{n}&gt;a_{n+1}\\) for all \\(n\\in\\mathbb{N}\\); - non-increasing if \\(a_{n}\\geq a_{n+1}\\) for all \\(n\\in\\mathbb{N}\\); - monotonic if it satisfies any of the previous conditions; - bounded from above if there exists \\(B\\in\\mathbb{R}\\) such that \\(a_{n}\\leq B\\) for every \\(n\\in\\mathbb{N}\\); - bounded from below if there exists \\(B\\in\\mathbb{R}\\) such that \\(a_{n}\\geq B\\) for every \\(n\\in\\mathbb{N}\\); - bounded if it is both bounded from above and below.\nA numerical sequence \\(\\{b_{n}\\}_{n\\in\\mathbb{N}}\\) is a subsequence of the numerical sequence \\(\\{a_{m}\\}_{m\\in\\mathbb{N}}\\) if \\(b_{n}=a_{n}\\) for all \\(n\\in \\mathbb{N}\\).\n\nObvious examples of numerical sequences are given by\n\\[\na_{n}= n, \\quad a_{n}=n^{2},\\quad a_{n}=n + (n-4)^{2}.\n\\]\n\n[!rem] Remark: numerical sequences and scalar functions It is clear that every scalar function \\((D,\\mathbb{R},f,\\mathbb{R})\\) such that \\([0,+\\infty)\\subseteq D\\) gives rise to a numerical sequence setting \\[\na_{n}=f(n).\n\\] However, the converse is not true. The prototypical example of numerical sequence that can not be obtained from a scalar function is \\[\na_{n}=n!= \\left\\{\\begin{matrix}1 & \\mbox{ if } n=0,1 \\\\  n(n-1)(n-2)\\cdots 2 & \\mbox{ if } n&gt;1 \\end{matrix}\\right.,  \n\\] where \\(n!\\) denotes the factorial of \\(n\\), that is, the possible distinct sequences of \\(n\\) distinct objects. Another important example is the Fibonacci sequence \\[\n\\left \\{ \\begin{matrix} a_{0}=a_{1}=1 &  \\\\ a_{n+1}= a_{n} + a_{n-1} & \\mbox{ for } n&gt;1 . \\end{matrix} \\right.  \n\\]\n\nThe notion of [[1.2 Limits of scalar functions#^cbf378|limit]] for scalar functions requires the notion of [[1.2 Limits of scalar functions#^772289|accumulation point]]. In the case of numerical sequences, the only possible (right) accumulation point is \\(+\\infty\\). Apart from that, the definition of limits for sequences is very similar to that of scalar functions.\n\n[!defn] Definition: \\(\\epsilon\\)-\\(\\delta\\) definition of limit of a numerical sequence Consider the numerical sequence \\(\\{a_{n}\\}_{n\\in\\mathbb{N}}\\). We say that \\[\n\\lim_{n\\rightarrow +\\infty} a_{n}=l\n\\] with \\(l\\in\\mathbb{R}\\) if for every \\(\\epsilon&gt;0\\) there is a \\(N\\in\\mathbb{N}\\) such that \\[\nn&gt;N \\Longrightarrow |a_{n}-l|&lt;\\epsilon .\n\\]\n\n^0cefdc\n\n[!defn] Definition: unbounded limit of a numerical sequence Consider the numerical sequence \\(\\{a_{n}\\}_{n\\in\\mathbb{N}}\\). We say that \\[\n\\lim_{n\\rightarrow +\\infty} a_{n}=+\\infty\n\\] if for every \\(K&gt;0\\) there is a \\(N\\in\\mathbb{N}\\) such that \\[\nn&gt;N \\Longrightarrow a_{n}&gt; K.\n\\] Similarly, we say that \\[\n\\lim_{n\\rightarrow +\\infty} a_{n}=-\\infty\n\\] if for every \\(K&gt;0\\) there is a \\(N\\in\\mathbb{N}\\) such that \\[\nn&gt;N \\Longrightarrow a_{n}&lt;- K.\n\\]\n\n^5a7f61\n\n[!important] Proposition: uniqueness of limits of numerical sequences If the limit for \\(n\\rightarrow +\\infty\\) of the sequence \\(\\{a_{n}\\}_{n\\in\\mathbb{N}}\\) exists (being it finite or \\(\\pm\\infty\\)), then it is unique.\n\n\n[!important] Proposition: limits and shift Let \\(\\{a_{n}\\}_{n\\in\\mathbb{N}}\\) be a sequence converging to the limit \\(L\\). Define \\(\\{b_{n}\\}_{n\\in\\mathbb{N}}\\) setting \\(b_{n}=a_{n+k}\\) for some \\(k\\in\\mathbb{N}\\). Then, it holds \\[\\lim_{n\\rightarrow+\\infty}b_{n}=\\lim_{n\\rightarrow+\\infty}a_{n}=L.\\]\n\n^655116\n\n\nProperties of limits\nThe similarity with the notions of limit for scalar functions should be apparent, and could lead you to conjecture that we can actually exploit what we already know to compute limits of sequences. If so, you are on the right track.\n\n[!important] Proposition Let \\(\\{a_{n}\\}_{n\\in\\mathbb{N}}\\) be a numerical sequence. Suppose there is a scalar function \\(f\\) and \\(N\\in\\mathbb{N}\\) such that \\(a_{n}=f(n)\\) for all \\(n&gt;N\\). It then follows that \\[\n\\lim_{x\\rightarrow +\\infty}f(x)=l\\quad \\Longrightarrow \\quad \\lim_{n\\rightarrow +\\infty} a_{n}= l\n\\] with \\(l\\in \\overline{\\mathbb{R}}=\\mathbb{R}\\cup\\{+\\infty,-\\infty\\}\\).\n\n\n[!attention] Remark The existence of the limit for the function \\(f\\) is mandatory. Indeed, once we introduce [[1.5 Power series, trascendental functions, and Taylor series|tracendental functions]], we have that the numerical sequence \\(\\{a_{n}\\}_{n\\in\\mathbb{N}}\\) with \\(a_{n}=\\sin(n\\pi)\\) vanishes identically so that its limit for \\(n\\rightarrow +\\infty\\) is obviously \\(0\\), while the limit for \\(x\\rightarrow +\\infty\\) of the scalar function \\(f(x)=\\sin(x\\pi)\\) does not exist!\n\nThe [[1.2 Limits of scalar functions#^ec717d|squeeze theorem]], as well as all the [[1.2 Limits of scalar functions#^55a3be|algebraic manipulations]] valid for limits of scalar functions have their appropriate counterpart in the context of numerical sequences. Two important additional results are recalled below:\n\n[!important] Bolzano-Weierstrass theorem Every bounded numerical sequence has at least one convergent subsequence.\n\n\n[!important] Monotone convergence theorem Every non-decreasing/non-increasing numerical sequence which is bounded from above/below converges.\n\n^6e7aa2\nSometimes, it may be useful to know if a sequence converges, even if we do not know to which limit.\n\n[!defn] Cauchy sequences A sequence \\(\\{a_{n}\\}_{n\\in\\mathbb{N}}\\) is said to be a Cauchy sequence if, for every \\(\\epsilon&gt;0\\) there is \\(N\\in \\mathbb{N}\\) such that \\(n,m&gt;N\\) implies \\(\\mid a_{n} -a_{m}\\mid&lt;\\epsilon\\).\n\n\n[!important] Proposition A sequence \\(\\{a_{n}\\}_{n\\in \\mathbb{N}}\\) converges to a real number (different from \\(\\pm\\infty\\)!) if and only if it is a Cauchy sequence.\n\n\n[!exmp] Example We now prove that the sequence \\(\\{a_{n}\\}_{n\\in\\mathbb{N}}\\) given by \\(a_{n}=c^{n}\\) converges to \\(1\\) when \\(c=1\\), to \\(0\\) when\\(|c|&lt;1\\), and to \\(+\\infty\\) when \\(|c|&gt;1\\). The first case is obvious. For the second case, we note that the sequence is bounded from above and below, and that it is decreasing when \\(c&gt;0\\), and increasing when \\(c&lt;0\\). Therefore, the [[1.4 Sequences and series#^6e7aa2|monotone coovergence theorem above]] implies the sequence has a limit, say \\(L\\). Introducing the sequence \\(\\{b_{n}\\}_{n\\in\\mathbb{N}}\\) setting \\(b_{n}=a_{n+1}\\), the proposition about [[1.4 Sequences and series#^655116|limits and shift above]] implies \\[L=\\lim_{n\\rightarrow+\\infty}a_{n}=\\lim_{n\\rightarrow+\\infty}b_{n}=\\lim_{n\\rightarrow+\\infty}\\frac{1}{c^{n+1}}=\\frac{1}{c}\\lim_{n\\rightarrow+\\infty}\\frac{1}{c^{n}}=\\lim_{n\\rightarrow+\\infty}a_{n}=\\frac{L}{c}.\\] Consequently, \\(L\\left(1-\\frac{1}{c}\\right)=0\\), which means \\(L=0\\) because \\(c\\neq 1\\). The third case is proved noting that \\(1&lt;|c|=\\frac{1}{|b|}\\) for some \\(|b|&lt;1\\), and then using the algebraic manipulation of limits.\n\n^157012\n\n\nNumerical series\nLet us start manipulating how to express the beautiful number \\(2\\):\n\\[\n\\begin{split}\n2& =1+1=1+\\frac{1}{2} + \\frac{1}{2}= \\\\\n& = 1+ \\frac{1}{2}  +  \\frac{1}{4} + \\frac{1}{4} = \\\\\n&= \\sum_{n=0}^{2}\\frac{1}{2^{n}}  + \\frac{1}{2^{2}}.\n\\end{split}\n\\]\nThe procedure can clearly continue for every \\(n\\) up to an arbitrary integer \\(N\\):\n\\[\n2=\\sum_{n=0}^{N}\\frac{1}{2^{n}}+ \\frac{1}{2^{N}}.\n\\]\nCan we extend it to infinity? This is equivalent to give sense to the equality \\[\n2=\\sum_{n=0}^{+\\infty}\\frac{1}{2^{n}}.\n\\]\nIt turns out that Mathematics allows us to deal with infinite sums!\n\n[!defn] Definition: limits of series Given a numerical sequence \\(\\{a_{n}\\}_{n\\in\\mathbb{N}}\\) we define the numerical sequence of partial sums \\(\\{s_{k}\\}_{k\\in\\mathbb{N}}\\) as follows: \\[ s_{k}=\\sum_{n=0}^{k}\\,a_{n}. \\]\nIf \\(\\{s_{k}\\}_{k\\in\\mathbb{N}}\\) converges to \\(-\\infty&lt;L&lt;+\\infty\\), we write \\[\n\\sum_{n=0}^{+\\infty}a_{n}=L\n\\]\nand say that the numerical series determined by \\(\\{a_{n}\\}\\) is convergent. If \\(L=\\pm\\infty\\), we say the series diverges.\nThe series \\(S\\) is said to be absolutely convergent if the series\n\\[\nS'=\\sum_{n=0}^{+\\infty}|a_{n}|\n\\] is convergent.\n\n^32251b\n\n[!important] Proposition: necessary condition for convergence If the numerical series determined by \\(\\{a_{n}\\}_{n\\in\\mathbb{N}}\\) converges then \\[\n\\lim_{n\\rightarrow +\\infty}a_{n}=0.\n\\]\n\n^ba1349\n\n[!important] Proposition: absolute convergence rules Every absolutely convergent series is convergent.\n\n\n[!exmp] Example: the geometric series The geometric series is associated with the numerical sequence \\[\na_{n}= x^{n}\n\\] with \\(x\\in\\mathbb{R}\\). Consider the numerical sequence of partial sums \\[\ns_{k}=\\sum_{n=0}^{k} x^{n}.\n\\] When \\(x=1\\) it is \\(s_{k}= k + 1\\), which means the series diverges. When \\(x=-1\\), it is \\(s_{k}=1\\) if \\(k\\) is even, and \\(s_{k}=0\\) if \\(k\\) is odd, and thus the sequence of partial sums does not have limit, and the series neither converge nor diverge. On the other hand, when \\(x\\neq \\pm1\\), it holds \\[\nx\\,s_{k}=x\\sum_{n=0}^{k}x^{n}=x + x^{2} + \\cdots + x^{k} + x^{k+1} = s_{k} -1 + x^{k+1},\n\\] which means \\[\n(x-1)s_{k}=x^{k} -1\\;\\Longrightarrow\\; s_{k}= \\frac{x^{k+1} -1}{x-1}.\n\\] If \\(x&gt;1\\) the sequence \\(s_{k}\\) diverges to \\(+\\infty\\), while if \\(x&lt;-1\\) the sequence does not have a limit since its values are increasingly big negative numbers for \\(k+1\\) even and increasingly big positive for \\(k+1\\) odd. The case \\(|x|&lt;1\\) is the only case in which \\(s_{k}\\) converges and it holds \\[\n\\sum_{n=0}^{+\\infty}a_{n} = \\lim_{k\\rightarrow +\\infty}s_{k} = \\frac{1}{1-x} .\n\\]\n\n^bf33d9\n\n\nProperties and convergence tests for numerical series\nSome important results are collected in the proposition below (and their proofs can be found, for instance, in Chapter 11 of S-E-H).\n\n[!important] Proposition Let \\(S_{a}\\) be the series determined by \\(\\{a_{n}\\}_{n\\in\\mathbb{N}}\\) and \\(S_{b}\\) be the series determined by \\(\\{b_{n}\\}_{n\\in\\mathbb{N}}\\):\n\nif \\(S_{a}\\) converges to \\(L\\) and \\(S_{b}\\) converges to \\(M\\) then \\(S_{a+b}\\) converges to \\(L+M\\);\nif \\(a_{n}\\geq 0\\) for all \\(n\\in\\mathbb{N}\\), then \\(S_{a}\\) converges if and only if the sequence of partial sums is bounded;\nevery rearrangement of an absolutely convergent series converges to the same limit while a rearrangement of a convergent series can do everything (Riemann’s theorem 1867).\n\n\nFinding the explicit value of infinite sums is very difficult, and we often only need to know if the series converges or not.\n\n[!important] Proposition: (limit) comparison test Given two series \\(S_{a},S_{b}\\) determined by \\(\\{a_{n}\\}\\) and \\(\\{b_{n}\\}\\), respectively, if \\(0\\leq a_{n}\\leq b_{n}\\) for all \\(n\\in\\mathbb{N}\\) greater than some fixed \\(N\\), then \\[ \\sum_{n=0}^{+\\infty}b_{n}&lt;+\\infty \\quad \\Rightarrow \\quad \\sum_{n=0}^{+\\infty}a_{n}&lt;+\\infty \\] and \\[ \\sum_{n=0}^{+\\infty}a_{n}=+\\infty \\quad \\Rightarrow \\quad \\sum_{n=0}^{+\\infty}b_{n}=+\\infty. \\]\nWhen \\(0\\leq a_{n}, b_{n}\\) but the terms of \\(\\{b_{n}\\}\\) do not majorize those of \\(\\{a_{n}\\}\\) for all \\(n&gt;N\\) as above, it holds that \\[ \\lim_{n\\rightarrow +\\infty} \\frac{a_{n}}{b_{n}} = L\\] implies: 1) both \\(S_{a}\\) and \\(S_{b}\\) either converge or diverge if \\(0&lt;L&lt;+\\infty\\); 2) \\(S_{a}\\) converges if so does \\(S_{b}\\) and \\(L=0\\); 3) \\(S_{b}\\) diverges if so does \\(S_{a}\\) and \\(L=+\\infty\\).\n\n\n[!important] Proposition: root test Let \\(S_{a}\\) be a series determined by \\(\\{a_{n}\\}\\), with \\(0\\leq a_{n}\\) for all \\(n\\in\\mathbb{N}\\) and \\[\n\\lim_{n\\rightarrow +\\infty}\\sqrt[n]{a_{n}} =L.\n\\] Then: 1. \\(S_{a}\\) converges if \\(0\\leq L&lt;1\\); 2. \\(S_{a}\\) diverges if \\(L&gt;1\\); 3. if \\(L=1\\) then the test is inconclusive.\n\n^3a9a60\n\n[!important] Proposition: ratio test Let \\(S_{a}\\) be the series determined by \\(\\{a_{n}\\}\\), with \\(0\\leq a_{n}\\) for all \\(n\\in\\mathbb{N}\\) and \\[ \\lim_{n\\rightarrow+\\infty}\\frac{a_{n+1}}{a_{n}}=L. \\] Then: 1. \\(S_{a}\\) converges if \\(0\\leq L&lt;1\\); 2. \\(S_{a}\\) diverges if \\(L&gt;1\\); 3. if \\(L=1\\) then the test is inconclusive.\n\n^b44184\nA series of the form \\[\n\\sum\\limits_{n=0}^{\\infty}(-1)^{n}a_{n},\n\\] with \\(a_{n}\\geq 0\\), is called alternating.\n\n[!important] Proposition: Leibniz’s test An alternating series converges if \\(0\\leq a_{n+1}\\leq a_{n}\\) for \\(n&gt;N\\) with \\(N\\) fixed, and \\(\\{a_{n}\\}\\) converges to \\(0\\).",
    "crumbs": [
      "Home",
      "Teaching",
      "Calculus - Robotics Engineering - UC3M",
      "Limits of numerical sequences"
    ]
  },
  {
    "objectID": "teaching/calculus-uc3m-robotics/3.1 Antiderivatives.html",
    "href": "teaching/calculus-uc3m-robotics/3.1 Antiderivatives.html",
    "title": "Anti-derivatives of scalar functions",
    "section": "",
    "text": "Anti-derivatives of scalar functions\nThe anti-derivative (or primitive) of a scalar function is a sort of algebraic inverse of the derivative.\n\n[!defn] Definition: anti-derivative function Let \\(([a,b],\\mathbb{R},f, \\mathbb{R})\\) be a continuous function. A continuous function \\(( [a,b],\\mathbb{R},F, \\mathbb{R})\\) is called an anti-derivative (or primitive) function for \\(f\\) if it is differentiable in \\((a,b)\\) and it satisfies \\[ F'(x)=f(x) \\] for every \\(x\\in (a,b)\\).\n\n\n[!rem] Remark It can be proved that every continuous function \\(([a,b],\\mathbb{R},f, \\mathbb{R})\\) admits an anti-derivative. This anti-derivative is not unique, but the difference between any two anti-derivatives is a constant. Specifically, if \\(([a,b],\\mathbb{R},F, \\mathbb{R})\\) and \\(([a,b],\\mathbb{R},G, \\mathbb{R})\\) are anti-derivatives of \\(f\\) then there exists \\(c\\in\\mathbb{R}\\) for which \\[ G(x)=F(x) +c\\] for all \\(x\\in (a,b)\\).\n\nThe act of taking anti-derivatives is depicted as \\[\nf\\mapsto \\; \\int f\\,\\mathrm{d}x ,\n\\] where it is implicitly assumed that \\(x\\) is the variable upon which \\(f\\) depends.\nGiven the intimate link between anti-derivatives and definite integrals we will discuss below, the act of taking the anti-derivative is also referred to as indefinite integration.\nThe operation of taking anti-derivatives satisfies a linearity property that essentially follows from the one we already saw for [[1.3 Derivatives of scalar functions#^5d20ad|derivatives]].\n\n[!important] Proposition Let \\(f,g\\colon I\\subseteq \\mathbb{R}\\rightarrow \\mathbb{R}\\) be continuous functions and \\(a,b\\in\\mathbb{R}\\). Then, it follows \\[\n\\int(a\\,f(x) + b\\,g(x))\\,\\mathrm{dx}=a\\,\\int f(x)\\,\\mathrm{dx}+ b\\,\\int g(x)\\,\\mathrm{dx}.\n\\]\n\n\n[!exmp] Example Consider the function \\(f\\colon \\mathbb{R}\\rightarrow\\mathbb{R}\\) given by \\[\nf(x)= a_{n}x^{n} + \\cdots a_{1}x + a_{0} .\n\\] Recalling the linearity property of the anti-derivative operation and the derivative of a polynomial we immediately get \\[\n\\int f(x) \\,\\mathrm{d}x =\\frac{a_{n}}{n}x^{n+1}\\cdots\\frac{a_{1}}{2}x^{2} + a_{0}x + c ,\n\\] where \\(c\\in \\mathbb{R}\\) characterizes all possible anti-derivatives of \\(f\\).\n\nWe can also exploit other properties of derivatives in order to obtain their counterparts for anti-derivatives, as it’s done below.\nLet us end this section noting that computing antiderivatives is a craft that requires a lot of practice and a lot of study. There are a lot of different techniques for a lot of different problems, and it is impossible for us to discuss even a little fraction of these methods, given the time at our disposal. However, for those willing to explore this beautiful corner of the mathematical landscape, I suggest starting from chapter 8 of this book.\n\n\nChange of variables\n^93db8a\nFrom the [[1.3 Derivatives of scalar functions#^c87d27|chain rule]] we obtain \\[\n\\int f(g(x))\\,g'(x)\\,\\mathrm{d}x = f(g(x)) + c ,\n\\] which immediately leads to \\[\n\\begin{split}\n1) \\qquad & \\int \\frac{f'(x)}{f(x)}\\,\\mathrm{d}x = \\ln(|g(x)|) + c \\\\ & \\\\\n2) \\qquad & \\int f'(x) (f(x))^{\\alpha}\\,\\mathrm{d}x = \\frac{(f(x))^{\\alpha +1}}{\\alpha + 1}  + c\\qquad (\\alpha\\neq 1) \\\\ & \\\\\n3) \\qquad & \\int \\frac{f'(x)}{1 + (f(x))^{2}}\\,\\mathrm{d}x = \\arctan((f(x))  + c  \\\\ & \\\\\n4) \\qquad & \\int \\frac{f'(x)}{\\sqrt{1 - (f(x))^{2}}}\\,\\mathrm{d}x = \\arcsin((f(x))  + c  .\n\\end{split}\n\\]\nWe can also look at this application of the chain rule from a slightly different perspective. Specifically, instead of looking at the composite function \\(f\\circ g\\) from scratch, we look at the function \\(F\\) which is an anti-derivative of \\(f\\): \\[\nF(x) = \\int f(x) \\mathrm{d}x \\quad \\Longleftrightarrow\\quad F'(x) =f(x) .\n\\] Then, we want to investigate what happens if we apply a change of variable through the function \\(g\\). Clearly, we have \\[\nF(x) \\quad \\Rightarrow \\quad F(x(t))=F(g(t))=(F\\circ g)(t),\n\\] so that the chain rule implies \\[\n(F\\circ g)'(t)=(F'\\circ g)(t)\\;g'(t) = (f\\circ g)(t)\\;g'(t).\n\\] Consequently, since taking the anti-derivative is a kind of inverse of taking the derivative, it also holds \\[\nF(x(t))=(F\\circ g)(t)=\\int (f\\circ g)(t) \\;g'(t)\\mathrm{d}t .\n\\]\nTherefore, we can either take the anti-derivative of \\(f(x)\\) with respect to \\(x\\) directly, or first take the anti-derivative of the composite function \\((f\\circ g)(t)\\,g'(t)\\) with respect to the auxiliary variable \\(t\\) and then write the result in terms of \\(x\\) by exploiting \\(t=g^{-1}(x)\\). This procedure is known as the change of variable for anti-derivatives, and it is often written as \\[\n\\int f(x)\\;\\mathrm{d}x = \\left[\\int (f\\circ g)(t)\\;g'(t)\\,\\mathrm{d}t\\right]_{t=g^{-1}(x)},\n\\] where the last change of variable \\(t\\mapsto t=g^{-1}(x)\\) for the right-hand-side is often dropped to simplify notation, at the expense of clarity and rigour, unfortunately implicitly considered. The change of variable technique is particularly useful to find anti-derivative, but, unfortunately, there is no general rule to build the change of variable \\(x=g(t)\\). Only experience can help you build your intuition.\n\n[!exmp] Example Let us compute the anti-derivative \\[\n\\int \\frac{1}{\\sqrt[3]{(1-2x)^{2}} - \\sqrt{1-2x}}\\,\\mathrm{d}x .\n\\] It is clear that the dependence from \\(x\\) is only through the function \\(1-2x\\). Moreover, this function appears either to the power \\(\\frac{3}{2}\\) or to the power \\(\\frac{1}{2}\\). Therefore, we guess it would be useful to introduce an auxiliary variable of the type \\(t^{m}=1-2x\\), where \\(m\\) takes into account the fact that \\(1-2x\\) appears always elevated to some power. What specific value of \\(m\\) should we take? Every number that makes all the roots disappear would be a wise choice. For instance, we can take \\(m=6\\) so that \\[\n\\frac{1}{\\sqrt[3]{(1-2x)^{2}} - \\sqrt{1-2x}}\\quad\\mapsto\\quad \\frac{1}{\\sqrt[3]{(t^{6})^{2}} - \\sqrt{t^{6}}} =\\frac{1}{t^{4} - t^{3}},\n\\] but even \\(m=12\\) would be fine. At this point, we determined the function \\(t(x)=g^{-1}(x)=\\sqrt[6]{1-2x}\\) because we defined \\(t\\) in terms of \\(x\\). However, we note that \\[\nt^{6} = 1-2x \\quad \\Longrightarrow \\quad t^{6} - 1 = -2x\\quad\\Longrightarrow \\quad \\frac{1-t^{6}}{2}= x,\n\\] and we obtain the change of variable function \\(x=g(t)=\\frac{1-t^{6}}{2}\\) with its derivative \\(g'(t)=-3t^{5}\\). Therefore, applying the change of variable formula we get \\[\n\\int \\frac{1}{\\sqrt[3]{(1-2x)^{2}} - \\sqrt{1-2x}}\\,\\mathrm{d}t = \\int \\frac{-3t^{5}}{t^{4} - t^{3}}\\,\\mathrm{d}t= -3\\int \\frac{t^{2}}{t -1}\\,\\mathrm{d}t .\n\\] At this point, we note that \\((t-1)(t+1)=t^{2} - 1\\) so that we can also write \\[\n\\frac{t^{2}}{t -1}=\\frac{t^{2}-1 + 1}{t -1}= \\frac{t^{2}-1}{t -1} + \\frac{1}{t -1}= t-1 + \\frac{1}{t -1},\n\\] and also \\[\n-3\\int \\frac{t^{2}}{t -1}\\,\\mathrm{d}t =  -3\\int t-1 + \\frac{1}{t -1}\\,\\mathrm{d}t= -\\frac{3}{2}(t-1)^{2} - 3\\ln|t-1| +c.\n\\] Consequently, applying the change of variable \\(t=g^{-1}(t)=\\sqrt[6]{1-2x}\\) we obtain \\[\n\\int \\frac{1}{\\sqrt[3]{(1-2x)^{2}} - \\sqrt{1-2x}}\\,\\mathrm{d}x=  -\\frac{3}{2}(\\sqrt[6]{1-2x}-1)^{2} - 3\\ln|\\sqrt[6]{1-2x}-1| +c .\n\\]\n\n\n[!attention] Remark A useful tool to guess the change of variable is to remember that, given \\(x=g(t)\\), it holds \\[\n\\frac{\\mathrm{d}x}{\\mathrm{d}t}=g'(t)\n\\] which we formally write as \\[\n\\mathrm{d}x = g'(t) \\mathrm{d}t.\n\\] Quite similarly, we also have \\[\n\\frac{\\mathrm{d}t}{\\mathrm{d}x}=(g^{-1})'(x)=\\frac{1}{g'(g^{-1}(x))} \\quad \\longrightarrow \\quad \\mathrm{d}t=(g^{-1})'(x)\\mathrm{d}x .\n\\]\n\n\n[!exmp] Example Let us compute the anti-derivative \\[\n\\int f(x)\\,\\mathrm{d}x=\\int\\frac{(x+1)^{3}}{\\sqrt{1-(x+1)^{2}}}\\,\\mathrm{d}x .\n\\] It may “feel” natural to set \\(t=g^{-1}(x)=(x+1)^{2}\\) as a reasonable change of variable. It then follows that \\(x=g(t)=\\sqrt{t} -1\\) so that \\(g'(t)=\\frac{1}{2\\sqrt{t}}\\) and \\[\n\\int\\frac{(x+1)^{3}}{\\sqrt{1-(x+1)^{2}}}\\,\\mathrm{d}x = \\int \\frac{t}{2\\sqrt{1-t}} \\,\\mathrm{d}t .\n\\]\nThis anti-derivative is not immediate, but can be computed using the procedure known as “integration by part” that is discussed below. However, there is another possibile change of variable that, while looking more complex, simplifies the problem. Specifically, we set \\[\nt=\\sqrt{1-(x+1)^{2}},\n\\] from which it follows that \\[\n1-t^{2} = (x+1)^{2} .\n\\] Moreover, following what is written in the previous remark, we also have \\[\n\\mathrm{d}t =- \\frac{(1+x)}{\\sqrt{1- (x+1)^{2}}}\\mathrm{d}x ,\n\\] which means that \\[\n\\int\\frac{(x+1)^{3}}{\\sqrt{1-(x+1)^{2}}}\\,\\mathrm{d}x =\\int t^{2}-1 \\mathrm{d}t,\n\\] where, as usual, the change of variable \\(t\\mapsto t(x)\\) in the right-hand-side is implicit. The anti-derivative on the right-hand-side is immediate being it the anti-derivative of a polynomial function: \\[\n\\int t^{2} -1 \\mathrm{d}t = \\frac{t^3}{3} -t,\n\\] and thus we obtain \\[\n\\int\\frac{(x+1)^{3}}{\\sqrt{1-(x+1)^{2}}}\\,\\mathrm{d}x = \\frac{(1-(x+1)^{2})^{\\frac{3}{2}}}{3} - \\sqrt{1-(x+1)^{2}}.\n\\]\n\n\n[!rem] Remark It is useful to remember the following useful changes of variables:\n\nwhen we have \\(\\sqrt{1-x^{2}}\\) we can apply \\(x=g(t)=\\sin(t)\\) so that \\(\\sqrt{1-x^2}=\\cos(t)\\) and \\(g'(t)=\\cos(t)\\);\nwhen we have \\(\\sqrt{1+x^{2}}\\) we can apply \\(x=g(t)=\\tan(t)\\) so that \\(\\sqrt{1+x^2}=\\frac{1}{\\cos(t)}\\) and \\(g'(t)=\\frac{1}{\\cos^{2}(t)}\\);\nwhen we have \\(\\sqrt{1+x^{2}}\\) we can also apply \\(x=g(t)=\\sinh(t)\\) so that \\(\\sqrt{1+x^2}=\\cosh(t)\\) and \\(g'(t)=\\cosh(t)\\);\nwhen we have \\(\\sqrt{x^{2}-1}\\) we can apply \\(x=g(t)=\\frac{1}{\\cos(t)}\\) so that \\(\\sqrt{x^{2} -1}=\\tan(t)\\) and \\(g'(t)=\\frac{\\sin(t)}{\\cos^{2}(t)}\\);\nwhen we have \\(\\sqrt{x^{2}-1}\\) we can apply \\(x=g(t)=\\cosh(t)\\) so that \\(\\sqrt{x^2 - 1}=\\sinh(t)\\) and \\(g'(t)=\\sinh(t)\\);\nwhen we have rational functions of trigonometric functions, we can exploit \\(t=g^{-1}(x)=\\tan(x/2)\\) so that \\(\\sin(x)=\\frac{2t}{1+t^{2}}\\), \\(\\cos(x)=\\frac{1-t^{2}}{1+t^{2}}\\), and \\(g'(t)=\\frac{2}{1 + t^{2}}\\).\n\n\n\n\nIntegration by parts\n^b5075f\nIf we exploit the so-called [[1.3 Derivatives of scalar functions#^5d20ad|Leibniz rule]] for computing the derivative of the product function \\[\n\\frac{\\mathrm{d}}{\\mathrm{d}x} (gh)(x)= g'(x) h(x) + g(x) h'(x),\n\\] we obtain \\[\n\\int\\left(\\frac{\\mathrm{d}}{\\mathrm{d}x} (gh)(x)\\right)\\,\\mathrm{d}x = \\int\\left( g'(x) h(x) + g(x) h'(x)\\right)\\,\\mathrm{d}x\n\\] which means \\[\n\\int  g'(x) h(x) \\,\\mathrm{d}x= gh(x) - \\int  g(x) h'(x) \\,\\mathrm{d}x ,\n\\] which is known as the formula for integration by parts.\n\n[!exmp] Example Consider the continuous function \\(f\\colon [a,b]\\rightarrow \\mathbb{R}\\) given by \\(f(x)=x\\mathrm{e}^{x}\\). We compute its anti-derivatives exploiting the rule of integration by parts. Specifically, we look at \\(\\mathrm{e}^{x}\\) as the derivative function of \\(g(x)=\\mathrm{e}^{x}\\) so that\n\\[\n\\int f(x)\\,\\mathrm{d}x=\\int x\\,\\mathrm{e}^{x}\\,\\mathrm{d}x = x \\mathrm{e}^{x} - \\int \\mathrm{e}^{x} \\,\\mathrm{d}x=(x-1)\\mathrm{e}^{x} .\n\\]\nA direct computation of the derivative of \\((x-1)\\mathrm{e}^{x}\\) shows that we did good.",
    "crumbs": [
      "Home",
      "Teaching",
      "Calculus - Robotics Engineering - UC3M",
      "Anti-derivatives of scalar functions"
    ]
  },
  {
    "objectID": "teaching/calculus-uc3m-robotics/index.html",
    "href": "teaching/calculus-uc3m-robotics/index.html",
    "title": "Calculus - Robotics Engineering - UC3M",
    "section": "",
    "text": "This digital garden contains the notes for the course of Calculus for Robotics Engineering at the UC3M taught by me in the academic years 2022/2023, 2023/2024, and 2024/2025. The course is quite an unusual one because it aims at presenting the content of a Calculus I course (~ scalar functions) together with the content of a Calculus II course (~ multivariable functions) in a single course. Accordingly, none of the topics discussed here are explored with the care and focus they usually receive in more usual courses. Therefore, I collected some useful additional resources below ([[Home#References|here]] and [[Home#Online resources|here]]).\nI am having fun with this course, and I think I will keep updating these notes even when I will stop teaching the course.\nPlease, notify me about comments/remarks/mistakes/questions by creating an issue in the associated Github repo, or sending me an email using my institutional address (which coincides with the address on the latest entry of my arXiv page).",
    "crumbs": [
      "Home",
      "Teaching",
      "Calculus - Robotics Engineering - UC3M"
    ]
  },
  {
    "objectID": "teaching/calculus-uc3m-robotics/index.html#references",
    "href": "teaching/calculus-uc3m-robotics/index.html#references",
    "title": "Calculus - Robotics Engineering - UC3M",
    "section": "References",
    "text": "References\nThe following are the references which I believe provides the basic understanding necessary to pass the exam:\n\nSalas, Etgen, Hille - Calculus: One and Several Variables (tenth edition);\nMarsden, Tromba - Vector Calculus (sixth edition).\n\nAdditional references, for a deeper understanding and an overall better enjoyment of the beautiful subject Calculus (or better, Analysis) is, are:\n\nRudin - Principle of Mathematical Analysis (third edition);\nSpivak - Calculus (third/fourth edition).\nTao - Analysis I (fourth edition)\nTao - Analysis II (fourth edition)",
    "crumbs": [
      "Home",
      "Teaching",
      "Calculus - Robotics Engineering - UC3M"
    ]
  },
  {
    "objectID": "teaching/calculus-uc3m-robotics/index.html#wikipedia-in-english",
    "href": "teaching/calculus-uc3m-robotics/index.html#wikipedia-in-english",
    "title": "Calculus - Robotics Engineering - UC3M",
    "section": "Wikipedia (in English)",
    "text": "Wikipedia (in English)\nIt is almost impossible not to start this list from Wikipedia (in English). Whenever I encounter something new or I have a little doubt on something I should know, I consult Wikipedia together with the sources and further references it provides. However, I would not recommend to rely on Wikipedia as your primary and sole reference for studying.",
    "crumbs": [
      "Home",
      "Teaching",
      "Calculus - Robotics Engineering - UC3M"
    ]
  },
  {
    "objectID": "teaching/calculus-uc3m-robotics/index.html#mathematics-stack-exchange",
    "href": "teaching/calculus-uc3m-robotics/index.html#mathematics-stack-exchange",
    "title": "Calculus - Robotics Engineering - UC3M",
    "section": "MATHEMATICS Stack Exchange",
    "text": "MATHEMATICS Stack Exchange\nA useful website to consult in order to get additional explanations or help with exercises in basically all fields of Mathematics is MATHEMATICS Stack Exchange. However, before asking a question there, make sure to search in the website for questions similar to the one you want to ask. Indeed, chances are that other people have already asked the same question, and they already received good answers you may profit from. Moreover, if asking for help with an exercise, make sure to explain what you did and why you are stucked. That is, always try to solve exercises on your own and then ask for help. Bear in mind that you need to know some basics of LaTeX to properly write mathematical expressions on MATHEMATICS Stack Exchange.",
    "crumbs": [
      "Home",
      "Teaching",
      "Calculus - Robotics Engineering - UC3M"
    ]
  },
  {
    "objectID": "teaching/calculus-uc3m-robotics/index.html#pauls-online-notes",
    "href": "teaching/calculus-uc3m-robotics/index.html#pauls-online-notes",
    "title": "Calculus - Robotics Engineering - UC3M",
    "section": "Paul’s Online Notes",
    "text": "Paul’s Online Notes\nThis amazing website can be consulted for additional explanations/exercises, as a source of additional studying material, as well as for the simple fact that it is very nice to read. Moreover, among other things, you can find in it some very useful cheatsheets on algebra, trigonometry, and calculus.",
    "crumbs": [
      "Home",
      "Teaching",
      "Calculus - Robotics Engineering - UC3M"
    ]
  },
  {
    "objectID": "teaching/calculus-uc3m-robotics/index.html#libretexts-mathematics",
    "href": "teaching/calculus-uc3m-robotics/index.html#libretexts-mathematics",
    "title": "Calculus - Robotics Engineering - UC3M",
    "section": "LibreTexts Mathematics",
    "text": "LibreTexts Mathematics\nThis website contains some textbooks on Mathematics that are completely free to access, contains exercises, and can be freely downloaded!",
    "crumbs": [
      "Home",
      "Teaching",
      "Calculus - Robotics Engineering - UC3M"
    ]
  },
  {
    "objectID": "teaching/calculus-uc3m-robotics/index.html#youtube-videos",
    "href": "teaching/calculus-uc3m-robotics/index.html#youtube-videos",
    "title": "Calculus - Robotics Engineering - UC3M",
    "section": "Youtube videos",
    "text": "Youtube videos\n\nIn general in your life, you are strongly encouraged to check 3Blue1Brown’s channel because it contains beautiful videos explaining different concepts related to mathematics(on monsters and group theory), physics (a primer on Heisenberg’s uncertainty principle) and other things (how does AI work?, what is Bitcoin?), in a clear and engaging way, using amazing graphical animations that help conveying the message. In relation to the course, I suggest the playlist essence of Calculus for a nice introduction to one-variable calculus.\nI did not have time to explore in detail Trefor Bazett’s Calculus playlists, but I think it is another good place to start for having more graphical explanations of the arguments discussed in the course.",
    "crumbs": [
      "Home",
      "Teaching",
      "Calculus - Robotics Engineering - UC3M"
    ]
  },
  {
    "objectID": "teaching/calculus-uc3m-robotics/index.html#fossee-animations-mathematics",
    "href": "teaching/calculus-uc3m-robotics/index.html#fossee-animations-mathematics",
    "title": "Calculus - Robotics Engineering - UC3M",
    "section": "FOSSEE Animations Mathematics",
    "text": "FOSSEE Animations Mathematics\nThis website provides interesting (open source) visualizations of concepts from multivariable calculus.",
    "crumbs": [
      "Home",
      "Teaching",
      "Calculus - Robotics Engineering - UC3M"
    ]
  },
  {
    "objectID": "teaching/calculus-uc3m-robotics/index.html#tools-to-help-with-exercises",
    "href": "teaching/calculus-uc3m-robotics/index.html#tools-to-help-with-exercises",
    "title": "Calculus - Robotics Engineering - UC3M",
    "section": "Tools to help with exercises",
    "text": "Tools to help with exercises\nSometimes, it could be helpful to check if some limit/derivative/integral we computed is actually correct. At this purpose, you can refer to the following online tools:\n\nHere you can compute derivatives, integrals, limits, sum of series and other things concerning scalar functions of one variable;\n\nHere you find also the option to deal with multivariable calculus (move the mouse over “Calculus”, a window-menu will open and you’ll find “multivariable calculus” there) and with local extrema;\n\nHere you find Lagrange multipliers.\n\nOf course, if you rely entirely on these computational tools (some of them offer the possibility to pay to get not only the answer but also the procedure), you will learn essentially nothing! Your own active hard work is the only thing that really helps you learning something.",
    "crumbs": [
      "Home",
      "Teaching",
      "Calculus - Robotics Engineering - UC3M"
    ]
  },
  {
    "objectID": "teaching/calculus-uc3m-robotics/2.3 Derivatives and Taylor's theorem for multivariable functions.html",
    "href": "teaching/calculus-uc3m-robotics/2.3 Derivatives and Taylor's theorem for multivariable functions.html",
    "title": "Derivatives and differentiability of multivariable functions",
    "section": "",
    "text": "Derivatives and differentiability of multivariable functions\nWe discuss here how the notion of [[1.3 Derivatives of scalar functions|derivative]] can be extended to the multivariable vector case.\n\n[!defn] Definition (Matrix of partial derivatives) Consider the multivariable vector function \\(f\\colon D \\subseteq \\mathbb{R}^{k}\\rightarrow \\mathbb{R}^{n}\\) given by \\[\nf(x^{1},\\cdots , x^{k})\\equiv f(\\mathbf{x})=\\left(\\begin{matrix} \\;f^{1}(\\mathbf{x}) \\;\\\\ \\; f^{2}(\\mathbf{x}) \\; \\\\ \\;\\vdots \\; \\\\ \\; f^{n}(\\mathbf{x}) \\;\\end{matrix}\\right),\n\\] and let \\(\\mathbf{x}_{0}\\) be an interior point of \\(D\\). The matrix of partial derivatives of \\(f\\) at \\(\\mathbf{x}_{0}\\) is the matrix \\(\\mathbf{D}f(\\mathbf{x}_{0})\\) whose \\((i,j)\\)-th element reads\n\\[\n(\\mathbf{D}f)^{i}_{j}(\\mathbf{x}_{0})\\equiv f_{j}^{i}|_{\\mathbf{x}_{0}}\\equiv \\left.\\frac{\\partial f^{i}}{\\partial x^{j}}\\right|_{\\mathbf{x}_{0}} :=\\lim_{h\\rightarrow 0} \\frac{f^{i}(\\mathbf{x}_{0} + h\\mathbf{e}_{j})-f^{i}(\\mathbf{x}_{0})}{h} .\n\\] The matrix of partial derivatives \\(\\mathbf{D}f(\\mathbf{x}_{0})\\) is also called the Jacobian of \\(f\\), and, when \\(n=1\\), the Jacobian becomes a horizontal vector known as the gradient vector of \\(f\\) at \\(\\mathbf{x}_{0}\\), and it is denoted by \\(\\nabla f(\\mathbf{x}_{0})\\).\n\n^c8b7dc\n\n[!rem] Remark: computation of partial derivatives Given \\(f\\colon D\\subseteq\\mathbb{R}^{k}\\rightarrow\\mathbb{R}^{n}\\), to compute the partial derivative \\(\\left.\\frac{\\partial f^{i}}{\\partial x^{j}}\\right|_{\\mathbf{x}_{0}}\\), we first compute the derivative of \\(f^{i}\\) with respect to \\(x^{j}\\) using the rules we learned for [[1.3 Derivatives of scalar functions|scalar functions]] by considering all variables except \\(x^{j}\\) to be constants. If the result is well-defined, that is the partial derivative we were looking for. If not, we must use the original definition of partial derivative in terms of limits.\n\n\n[!exmp] Example We now describe an example in which it is not possible to use the rules we learned for [[1.3 Derivatives of scalar functions|scalar functions]] to compute the partial derivative at a given point, and we must use the definition in terms of limit given above. Consider the function \\(f\\colon \\mathbb{R}^{2}\\rightarrow \\mathbb{R}\\) given by \\[\nf(x,y)=\\sqrt[3]{x\\,y}\n\\] and the point \\((0,0)\\in\\mathbb{R}\\). If we take the derivative of \\(f\\) with respect to \\(x\\) considering \\(y\\) as a constant, we immediately obtain the function \\[\nF(x,y)=\\frac{\\sqrt[3]{y}}{3(x)^{\\frac{2}{3}}}\n\\] which is not defined at \\((0,0)\\). Similarly, if we take the derivative of \\(f\\) with respect to \\(y\\) as if \\(x\\) was a constant, we immediately obtain the function \\[\nG(x,y)=\\frac{\\sqrt[3]{x}}{3(y)^{\\frac{2}{3}}}\n\\] which is not defined at \\((0,0)\\). However, if we exploit the definition of partial derivative given above, we obtain \\[\n\\mathbf{D}_{x}f(0,0)=\\lim_{h\\rightarrow 0} \\frac{f(0+h,0)-f(0,0)}{h}=0\n\\] and similarly for \\(\\mathbf{D}_{y}f(0,0)\\). Therefore, we conclude that both partial derivatives of \\(f\\) at \\((0,0)\\) exist and we have \\(\\mathbf{D}f(0,0)=\\nabla f(0,0)=\\mathbf{0}\\).\n\nIn general, the existence of partial derivatives is not enough to guarantee a suitable notion of differentiability because, among other things, it does not guarantee the validity of the differentiability of composite functions (see [[2.5 Exercises#^fbb3a9|this exercise]]).\n\n[!defn] Definition: differentiability of multivariable vector functions Let \\(f\\colon D\\subseteq \\mathbb{R}^{k}\\rightarrow \\mathbb{R}^{n}\\). It is differentiable at \\(\\mathbf{x}_{0}\\in D\\) if the matrix \\(\\mathbf{D}f(\\mathbf{x}_{0})\\) of partial derivatives at \\(\\mathbf{x}_{0}\\) exists and\n\\[\n\\lim_{\\mathbf{x}\\rightarrow\\mathbf{x}_{0}}\\,\\frac{||f(\\mathbf{x}) - f(\\mathbf{x}_{0}) - \\mathbf{D}f(\\mathbf{x}_{0})(\\mathbf{x} - \\mathbf{x}_{0})||}{||\\mathbf{x} - \\mathbf{x}_{0}||} = 0\n\\]\nwhere \\(\\mathbf{D}f(\\mathbf{x}_{0})(\\mathbf{x} - \\mathbf{x}_{0})\\) denotes the matrix product between the \\((k\\times n)\\) matrix \\(\\mathbf{D}f(\\mathbf{x}_{0})\\) and the \\((n\\times 1)\\) matrix \\((\\mathbf{x} - \\mathbf{x}_{0})\\).\nGiven \\(A\\subseteq D\\), the function \\(f\\) is said to be differentiable in \\(A\\) if the matrix \\(\\mathbf{D}f\\) of partial derivatives exists for all \\(\\mathbf{x}\\in A\\).\nThe function \\(f\\) is said to be \\(C^{1}\\) in \\(A\\subseteq D\\) if it is differentiable in \\(A\\) and all its partial derivatives \\(\\frac{\\partial f^{i}}{\\partial x^{j}}\\) are continuous in \\(A\\).\n\n^801be3\n\n[!exmp] Example Let \\(f\\colon \\mathbb{R}^{3}\\rightarrow \\mathbb{R}\\) be given by \\[\nf(x,y,z)=x^{2} + y^{2} + z^{2} .\n\\] A direct computation shows that \\[\n\\nabla f=(2x, 2y, 2z)\n\\] which is well-defined for every \\(\\mathbf{x}=(x,y,z)\\in\\mathbb{R}^{3}\\). In order for \\(f\\) to be differentiable at \\(\\mathbf{x}_{0}\\), we have to check that \\[\n\\lim_{\\mathbf{x}\\rightarrow \\mathbf{x}_{0}} \\frac{x^{2} + y^{2} + z^{2} - x_{0}^{2} - y_{0}^{2} - z_{0}^{2} -2(x - x_{0})x_{0} - 2(y-y_{0})y_{0} - 2(z-z_{0})z_{0}}{\\sqrt{(x - x_{0})^{2} + (y - y_{0})^{2} +(z - z_{0})^{2}}} =0 .\n\\] Some simple algebra shows that, in the left-hand-side of the expression above, the numerator is precisely the square of the denominator so that the limit becomes\n\\[\n\\lim_{\\mathbf{x}\\rightarrow \\mathbf{x}_{0}} \\sqrt{(x - x_{0})^{2} + (y - y_{0})^{2} +(z - z_{0})^{2}} =0 ,\n\\]\nwhich holds because of the very definition of [[2.2 Limits of multivariable functions|limits of multivariable scalar functions]].\n\n\n[!exmp] Example Consider the function \\(f\\colon\\mathbb{R}^{3}\\rightarrow\\mathbb{R}^{2}\\) given by\n\\[\nf(x,y,z)=\\left(\\begin{matrix} x^{2} + y^{2} + z^{2} \\\\ x + y + z  \\end{matrix}\\right).\n\\]\nFollowing the strategy described in the remark before the previous proposition, the matrix of partial derivatives at \\((x_{0},y_{0},z_{0})\\) turns out to be the \\((2\\times 3)\\) matrix given by\n\\[\nDf(x_{0},y_{0},z_{0})=\\left(\\begin{matrix} \\left.\\frac{\\partial f^{1}}{\\partial x}\\right|_{(x_{0},y_{0},z_{0})} & \\left.\\frac{\\partial f^{1}}{\\partial y}\\right|_{(x_{0},y_{0},z_{0})}  & \\left.\\frac{\\partial f^{1}}{\\partial z}\\right|_{(x_{0},y_{0},z_{0})}  \\\\ & & \\\\\\left.\\frac{\\partial f^{2}}{\\partial x}\\right|_{(x_{0},y_{0},z_{0})}  & \\left.\\frac{\\partial f^{2}}{\\partial y}\\right|_{(x_{0},y_{0},z_{0})}  & \\left.\\frac{\\partial f^{2}}{\\partial z}\\right|_{(x_{0},y_{0},z_{0})}  \\end{matrix} \\right) = \\left(\\begin{matrix}  2 x & 2y &2z \\\\ & & \\\\ 1 & 1 & 1  \\end{matrix} \\right).\n\\]\nA direct computation shows that\n\\[\nDf(\\mathbf{x}_{0})(\\mathbf{x} - \\mathbf{x}_{0})= \\left(\\begin{matrix} 2x(x-x_{0}) + 2y(y-y_{0}) + 2z(z-z_{0}) \\\\ 0  \\end{matrix}\\right),\n\\]\nwhich means that\n\\[\n\\begin{split}\n||f(\\mathbf{x}) - f(\\mathbf{x}_{0}) - Df(\\mathbf{x}_{0})(\\mathbf{x} - \\mathbf{x}_{0})||&=\\left|\\left| \\left(\\begin{matrix} - (x-x_{0})^{2} - (y-y_{0})^{2} - (z-z_{0})^{2}\\\\ 0 \\end{matrix}\\right) \\right|\\right| \\\\& =||\\mathbf{x} - \\mathbf{x}_{0}||^{2}.\n\\end{split}\n\\]\nConsequently, the limit\n\\[\n\\lim_{\\mathbf{x}\\rightarrow\\mathbf{x}_{0}}\\,\\frac{||f(\\mathbf{x}) - f(\\mathbf{x}_{0}) - Df(\\mathbf{x}_{0})(\\mathbf{x} - \\mathbf{x}_{0})||}{||\\mathbf{x} - \\mathbf{x}_{0}||}=\\lim_{\\mathbf{x}\\rightarrow\\mathbf{x}_{0}}\\, ||\\mathbf{x} - \\mathbf{x}_{0}|| =0\n\\]\nis satisfied and we conclude that \\(f\\) is differentiable everywhere.\n\n\n[!defn] Definition: tangent hyperplane of a multivariable scalar function Given \\((D,\\mathbb{R}^{n},f,\\mathbb{R})\\) and a point \\(\\mathbf{x}_{0}\\in D\\) at which all partial derivatives exist, the equation \\[f(\\mathbf{x}_{0})+\\mathbf{D}f(\\mathbf{x}_{0})(\\mathbf{x} - \\mathbf{x}_{0})=0\\] defines an hyperplane in \\(\\mathbb{R}^{n}\\) called the tangent hyperplane to \\(f\\) at \\(\\mathbf{x}_{0}\\). When \\(f\\) is differentiable at \\(\\mathbf{x}_{0}\\), the tangent hyperplane clearly exists and gives a good approximation to \\(f\\) around \\(\\mathbf{x}_{0}\\).\n\n\n\nProperties of derivatives of multivariable vector functions\nSome important properties of multivariable vector functions that are differentiable are collected below. Note that when \\(k=n=1\\), the results in this subsection reduce to those we already encounter for [[1.3 Derivatives of scalar functions#^f3bc1e|scalar functions]].\n\n[!important] Proposition Let \\(f\\colon A\\subseteq \\mathbb{R}^{k}\\rightarrow \\mathbb{R}^{n}\\) be differentiable at \\(\\mathbf{x}_{0}\\). Then \\(f\\) is continuous at \\(\\mathbf{x}_{0}\\).\n\n\n[!important] Proposition Let \\(f\\colon A\\subseteq \\mathbb{R}^{k}\\rightarrow \\mathbb{R}^{n}\\) be such that all its partial derivatives exist and are continuous in an open ball centered at \\(\\mathbf{x}_{0}\\) with radius \\(\\epsilon&gt;0\\). Then \\(f\\) is differentiable at \\(\\mathbf{x}_{0}\\).\n\n\n[!rem] Remark Always remember that:\ncontinuity of partial derivatives \\(\\Longrightarrow\\) differentiability \\(\\Longrightarrow\\) existence of partial derivatives\nwhile all converse implications are false!\n\n\n[!exmp] Example: partial derivatives exist but the function is not differentiable Consider the function \\(f\\colon \\mathbb{R}^{2}\\rightarrow \\mathbb{R}\\) given by \\[\nf(x,y)=\\sqrt[3]{x\\,y}\n\\] and the point \\((0,0)\\in\\mathbb{R}\\). Following one of the examples discussed above, we obtain \\[\n\\mathbf{D}f(0,0)=\\nabla f(0,0)=\\mathbf{0}.\n\\] To check the differentiability of \\(f\\) at \\((0,0)\\), we should verify the limit \\[\n\\lim_{(x,y)\\rightarrow (0,0)}\\,\\frac{|f(x,y) - f(0,0) - \\nabla f(0,0)\\cdot\\mathbf{x}|}{||\\mathbf{x}||}=\\lim_{(x,y)\\rightarrow (0,0)}\\,\\frac{\\sqrt[3]{xy}}{\\sqrt{x^{2} + y^{2}}}=0 .\n\\] When \\(y=0\\) we obtain \\[\n\\lim_{x\\rightarrow 0}\\,\\frac{|f(x,0) - f(0,0) - \\nabla f(0,0)\\cdot\\mathbf{x}|}{||\\mathbf{x}||}=\\lim_{x\\rightarrow 0}\\,0=0 .\n\\] However, when \\(y=x^{2}\\), we have \\[\n\\lim_{x\\rightarrow 0}\\,\\frac{|f(x,x^{2}) - f(0,0) - \\nabla f(0,0)\\cdot\\mathbf{x}|}{||\\mathbf{x}||}=\\lim_{x\\rightarrow 0}\\,\\frac{x}{\\sqrt{x^{2}+x^{4}}}=\\lim_{x\\rightarrow 0}\\,\\frac{x}{|x|\\sqrt{1+x^{2}}}=\\pm 1\n\\] depending on whether \\(x\\rightarrow 0^{+}\\) or \\(x\\rightarrow 0^{-}\\). Therefore, we must conclude that the function is not differentiable at \\((0,0)\\).\n\n\n[!exmp] Example: a differentiable function whose partial derivatives are not continuous Consider the function \\(f\\colon\\mathbb{R}\\rightarrow\\mathbb{R}\\) given by \\[\nf(x)=\\left\\{\\begin{matrix}x^{2}\\,\\sin\\left(\\frac{1}{x}\\right) & x\\neq 0 \\\\ & \\\\ 0 & x=0 \\; .\\end{matrix}\\right.\n\\] The partial derivatives here collapse to the [[1.3 Derivatives of scalar functions|ordinary derivative]] of one-variable scalar functions which reads \\[\nf'(x)=\\left\\{\\begin{matrix}2x\\,\\sin\\left(\\frac{1}{x}\\right) - \\cos\\left(\\frac{1}{x}\\right) & x\\neq 0 \\\\ & \\\\ 0 & x=0 \\; .\\end{matrix}\\right.\n\\] Clearly, \\(f'(x)\\) is not continuous at \\(x=0\\) because \\(\\cos\\left(\\frac{1}{x}\\right)\\) has no limit for \\(x\\rightarrow 0\\). However, it is immediate to check that the limit \\[\n\\lim_{x\\rightarrow 0}\\,\\frac{|f(x)-f(0) - f'(0)x|}{|x|}=\\lim_{x\\rightarrow 0}\\,\\left|x\\sin\\left(\\frac{1}{x}\\right)\\right|= 0\n\\] holds because trigonometric functions are bounded. We thus conclude that \\(f\\) is differentiable at \\(x=0\\) although its partial derivatives are not continuous there.\n\n^bed62d\n\n[!Important] Proposition (Sums, products, quotients) 1) Sum Rule: let \\(f,g \\colon D\\subseteq\\mathbb{R}^{k}\\rightarrow\\mathbb{R}^{n}\\) be differentiable at \\(\\mathbf{x}_{0}\\) and let \\(a,b\\in\\mathbb{R}\\). Then \\(h(\\mathbf{x}) = a f (\\mathbf{x}) + b g (\\mathbf{x})\\) is differentiable at \\(\\mathbf{x}_{0}\\) and \\(\\mathbf{D}h(\\mathbf{x}_{0}) = a\\mathbf{D}f (\\mathbf{x}_{0} ) + b\\mathbf{D}g (\\mathbf{x}_{0} )\\) (sum of matrices). 2) Product Rule: let et \\(f,g \\colon D\\subseteq\\mathbb{R}^{k}\\rightarrow\\mathbb{R}\\) be differentiable at \\(\\mathbf{x}_{0}\\). Then \\(h(\\mathbf{x}) = f (\\mathbf{x})\\,g (\\mathbf{x})\\) is differentiable at \\(\\mathbf{x}_{0}\\) and \\(\\mathbf{D}h(\\mathbf{x}_{0}) = g (\\mathbf{x}_{0} )\\,\\mathbf{D}f (\\mathbf{x}_{0} ) + f (\\mathbf{x}_{0} )\\,\\mathbf{D}g (\\mathbf{x}_{0} )\\). 3) Quotient Rule: with the same hypotheses as in the product rule above, let \\(h(\\mathbf{x}) = \\frac{f (\\mathbf{x})}{g (\\mathbf{x})}\\) and suppose \\(g\\) is never \\(0\\) on \\(D\\). Then \\(h\\) is differentiable at \\(\\mathbf{x}_{0}\\) and\n\\[\n\\mathbf{D}h(\\mathbf{x}_{0})=\\frac{g(\\mathbf{x}_{0})\\,\\mathbf{D}f(\\mathbf{x}_{0})-f(\\mathbf{x}_{0})\\,\\mathbf{D}g(\\mathbf{x}_{0} )}{(g(\\mathbf{x}_{0}))^{2}}.\n\\]\n\n\n[!Important] Proposition (Chain rule) Let \\(D\\subset\\mathbb{R}^{k}\\) and \\(D'\\subseteq\\mathbb{R}^{m}\\) be open sets. Let \\(g\\colon\\mathbb{R}^{k}\\rightarrow\\mathbb{R}^{m}\\) and \\(f\\colon\\mathbb{R}^{m}\\rightarrow\\mathbb{R}^{n}\\) be given functions such that \\(g(D)\\subseteq D'\\), so that \\(f \\circ g\\) is defined. Suppose \\(g\\) is differentiable at \\(\\mathbf{x}_{0}\\) and \\(f\\) is differentiable at \\(\\mathbf{y}_{0} = g(\\mathbf{x}_{0} )\\). Then \\(f \\circ g\\) is differentiable at \\(\\mathbf{x}_{0}\\) and\n\\[\n\\mathbf{D}( f\\circ g)(\\mathbf{x}_{0} ) = \\mathbf{D}f (\\mathbf{y}_{0})\\cdot \\mathbf{D}g(\\mathbf{x}_{0})\n\\]\nwhere \\(\\cdot\\) in the right-hand side is the matrix product of \\(\\mathbf{D}f (\\mathbf{y}_{0})\\) with \\(\\mathbf{D}g(\\mathbf{x}_{0})\\).\n\n^9ceaa7\n\n[!exmp] Example: chain rule Let us consider the function \\(f\\colon \\mathbb{R}^{3}\\rightarrow \\mathbb{R}\\) given by \\[\nf(u,v,w)=u^{2} + v^{2} -w,\n\\] and the function \\(g\\colon\\mathbb{R}^{3}\\rightarrow\\mathbb{R}^{3}\\) given by \\[\ng(x,y,z)=\\left(\\begin{matrix} x^{2}y \\\\ y^{2} \\\\ \\mathrm{e}^{-xz}\\end{matrix}\\right).\n\\] We want to verify the chain rule. At this purpose, we first compute \\[\n\\mathbf{D}f(u,v,z)=\\nabla f(u,v,z)=\\left(2u, 2v, -1\\right),\n\\] and \\[\n\\mathbf{D}g(x,y,z)=\\left(\\begin{matrix}2xy & x^{2} & 0 \\\\ 0 & 2y & 0 \\\\ -z\\mathrm{e}^{xz} & 0 & -x\\mathrm{e}^{-xz}  \\end{matrix}\\right),\n\\] so that \\[\n\\begin{split}\n\\mathbf{D}f(u(x,y,z),v(x,y,z),w(x,y,z))\\cdot \\mathbf{D}g(x,y,z)&=\\left(2u(x,y,z), 2v(x,y,z), -1\\right)\\cdot\\left(\\begin{matrix}2xy & x^{2} & 0 \\\\ 0 & 2y & 0 \\\\ -z\\mathrm{e}^{xz} & 0 & -x\\mathrm{e}^{-xz}  \\end{matrix}\\right)= \\\\ & \\\\ & = \\left(2x^{2}y, 2y^{2}, -1\\right)\\cdot\\left(\\begin{matrix}2xy & x^{2} & 0 \\\\ 0 & 2y & 0 \\\\ -z\\mathrm{e}^{xz} & 0 & -x\\mathrm{e}^{-xz}  \\end{matrix}\\right) = \\\\ & \\\\ & = 4x^{3}y^{2} + z\\mathrm{e}^{-xz},\\;2x^{4}y + 4y^{3}, x\\mathrm{e}^{-xz} .\n\\end{split}\n\\] Then, we compute \\[\nf\\circ g(x,y,z)= f(u(x,y,z),v(x,y,z),w(x,y,z))=f(x^{2}y, y^{2}, \\mathrm{e}^{-xz})= x^{4}y^{2} + y^{4} - \\mathrm{e}^{-xz}\n\\] so that \\[\n\\mathbf{D}(f\\circ g)(x,y,z)=\\nabla (f\\circ g)(x,y,z)=\\left(4x^{3}y^{2} + z\\mathrm{e}^{-xz},\\;2x^{4}y + 4y^{3}, x\\mathrm{e}^{-xz}\\right),\n\\] and we immediately see that the chain rule holds as expected.\n\n\n\nSecond order partial derivatives\n^1b4383\nHere, we will focus on multivariable scalar functions, that is, functions whose input space is \\(\\mathbb{R}^{n}\\) and whose output space is \\(\\mathbb{R}\\).\nWe are interested in understanding the multivariable analogue of the second derivative of a scalar function.\nConsider \\(f\\colon D\\subseteq \\mathbb{R}^{n}\\rightarrow \\mathbb{R}\\) with partial derivatives \\(\\frac{\\partial f}{\\partial x^{j}}\\), with \\(j=1,...,n\\). These partial derivatives can be arranged into a vector.\n\n[!defn] Definition: the gradient vector Given \\(f\\colon D\\subseteq \\mathbb{R}^{n}\\rightarrow \\mathbb{R}\\), we define the gradient vector \\(\\nabla f(\\mathbf{x}_{0})\\) by setting \\[ \\nabla f (\\mathbf{x}_{0})=\\left(\\left.\\frac{\\partial f}{\\partial x^{1}}\\right|_{\\mathbf{x}_{0}},\\cdots,\\left.\\frac{\\partial f}{\\partial x^{n}}\\right|_{\\mathbf{x}_{0}}\\right).\\] Note that the gradient vector defines a multivariable vector function \\(\\nabla f\\colon A\\subseteq\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{n}\\).\n\nEvery \\(\\frac{\\partial f}{\\partial x^{j}}\\) is a multivariable scalar function, and we can thus compute its derivative with respect to any of \\(x^{1},\\cdots,x^{n}\\). Specifically, given \\(f\\colon D\\subseteq \\mathbb{R}^{n}\\rightarrow \\mathbb{R}\\), we can define its \\((i,j)\\)-th second partial derivative at \\(\\mathbf{x}_{0}\\) as \\[\n\\left. f_{ij}\\right|_{\\mathbf{x}_{0}}\\equiv\\left.\\frac{\\partial^{2} f}{\\partial x^{i}\\partial x^{j}}\\right|_{\\mathbf{x}_{0}}:=\\left(\\frac{\\partial}{\\partial x^{i}}\\left(\\frac{\\partial f}{\\partial x^{j}}\\right)\\right)_{\\mathbf{x}_{0}},\n\\]\nwhich means taking the \\(i\\)-th partial derivative of the \\(j\\)-th partial derivative function \\(f_{j}\\equiv \\frac{\\partial f}{\\partial x^{j}}\\).\n\n[!defn] Definition: Hessian matrix Let \\(f\\colon D\\subseteq\\mathbb{R}^{n}\\rightarrow \\mathbb{R}\\) have all second partial derivatives at \\(\\mathbf{x}_{0}\\). The matrix \\[\nHf_{\\mathbf{x}_{0}}=\\left(\\begin{matrix}f_{11}|_{\\mathbf{x}_{0}}& \\cdots & f_{1n}|_{\\mathbf{x}_{0}} \\\\ & & \\\\ \\vdots & &\\vdots \\\\ & & \\\\ f_{n1}|_{\\mathbf{x}_{0}} & \\cdots & f_{nn}|_{\\mathbf{x}_{0}}\\end{matrix}\\right)\n\\] of second partial derivatives at \\(\\mathbf{x}_{0}\\in A\\) is called the Hessian matrix of \\(f\\) at \\(\\mathbf{x}_{0}\\in A\\).\n\nIn principle, the Hessian matrix is not symmetric because, for \\(i\\neq j\\), it may happen that \\[\n\\left.\\frac{\\partial^{2} f}{\\partial x^{i}\\partial x^{j}}\\right|_{\\mathbf{x}_{0}}\\neq \\left.\\frac{\\partial^{2} f}{\\partial x^{j}\\partial x^{i}}\\right|_{\\mathbf{x}_{0}} ,\n\\] that is, the mixed partial derivatives are different.\n\n[!defn] Definition: \\(C^{2}\\) functions If all the possible second order partial derivatives of the function \\(f\\colon D\\subseteq \\mathbb{R}^{n}\\rightarrow \\mathbb{R}\\) exist and are continuous on the open set \\(A\\subseteq D\\), then \\(f\\) is said to be \\(C^{2}\\) on \\(A\\).\n\n\n[!defn] Proposition: equality of mixed partial derivatives If \\(f\\colon D\\subseteq \\mathbb{R}^{n}\\rightarrow \\mathbb{R}\\) is \\(C^{2}\\) on the open set \\(A\\subseteq D\\) then \\[\n\\frac{\\partial^{2} f}{\\partial x^{i}\\partial x^{j}}=f_{ij}=f_{ji}=\\frac{\\partial^{2} f}{\\partial x^{j}\\partial x^{i}}\n\\] for all \\(i,j\\) and for all \\(\\mathbf{x}\\in\\mathbf{A}\\). That is, mixed second partial derivatives are equal.\n\nBecause of the proposition above, the Hessian matrix of a \\(C^{2}\\) function is always a symmetric matrix.\nOnce we have the matrix of second partial derivatives, we can take the partial derivatives of each element in the matrix, and obtain an object (which is neither a vector nor a matrix) encoding information on third order derivatives, and so on.\n\n[!defn] Definition: \\(C^{k}\\) functions If all the possible \\(k\\)-th order partial derivatives of the function \\(f\\colon D\\subseteq \\mathbb{R}^{n}\\rightarrow \\mathbb{R}\\) exist and are continuous on the open set \\(A\\subseteq D\\), then \\(f\\) is said to be \\(C^{k}\\) on \\(A\\).\n\nNote that, for \\(f\\colon D\\subseteq \\mathbb{R}^{n}\\rightarrow\\mathbb{R}\\), we get \\(n^{k}\\) partial derivatives of order \\(k\\), and thus number of computations becomes quickly unmanageable.\n\n\nSecond order Taylor’s theorem\nJust as in the [[1.5 Power series, trascendental functions, and Taylor series#^113a5b|one-variable case]], derivatives help in getting a good approximation of a given function. The accuracy of such an approximation is governed by Taylor’s theorem. For simplicity, we focus on the second order version of the theorem.\n\n[!important] Theorem: second order Taylor’s theorem Let \\(f\\colon D\\subseteq \\mathbb{R}^{n}\\rightarrow\\mathbb{R}\\) be of class \\(C^{3}\\) on the open set \\(A\\subseteq D\\) containing \\(x_{0}\\). Then, we have \\[\nf(\\mathbf{x})=f(\\mathbf{x}_{0})+ \\sum\\limits_{j=1}^{n}\\left.\\frac{\\partial f}{\\partial x^{j}}\\right|_{\\mathbf{x}_{0}}(x^{j}-x_{0}^{j}) + \\frac{1}{2}\\sum\\limits_{j,k=1}^{n}\\left.\\frac{\\partial^{2} f}{\\partial x^{j}\\partial x^{k}}\\right|_{\\mathbf{x}_{0}}\\,(x^{j}-x_{0}^{j})\\,(x^{k}-x_{0}^{k}) + R_{2}(\\mathbf{x},\\mathbf{x}_{0}),\n\\] where the function \\(R_{2}(\\mathbf{x},\\mathbf{x}_{0})\\) is such that \\[\\lim_{\\mathbf{x}\\rightarrow\\mathbf{x}_{0}}\\,\\frac{R_{2}(\\mathbf{x},\\mathbf{x}_{0})}{\\Vert \\mathbf{x}-\\mathbf{x}_{0}\\Vert^{2}}=0,\\] and it’s called the second order remainder.\n\nExploiting matrix notation, Taylor’s formula can be also written as \\[\nf(\\mathbf{x})=f(\\mathbf{x}_{0}) + \\nabla f|_{\\mathbf{x}_{0}}\\cdot\\,(\\mathbf{x}-\\mathbf{x}_{0}) + \\frac{1}{2}(\\mathbf{x}-\\mathbf{x}_{0})^{T}\\,Hf|_{\\mathbf{x}_{0}}\\,(\\mathbf{x}-\\mathbf{x}_{0}),\n\\] where \\(\\nabla f\\) is the gradient vector of \\(f\\), \\(Hf\\) is the Hessian matrix of \\(f\\), and \\(v^{T}\\) denote the transpose of a vector/matrix.\n\n[!important] Form of the 2-nd order remainder In the hypothesis of the 2-nd order Taylor’s theorem, the remainder \\(R_{2}(\\mathbf{x},\\mathbf{x}_{0})\\) can be written as: \\[ \\sum_{j,k,l=1}^{n}\\frac{1}{3!}\\,\\frac{\\partial^{3} f}{\\partial x^{j}\\partial x^{k}\\partial x^{l}}(c_{jkl})(x^{j}-x^{j}_{0})(x^{k}-x^{k}_{0})(x^{l}-x^{l}_{0}),\\] where each \\(c_{jkl}\\) lies somewhere on the line joining \\(\\mathbf{x}_{0}\\) to \\(\\mathbf{x}\\).",
    "crumbs": [
      "Home",
      "Teaching",
      "Calculus - Robotics Engineering - UC3M",
      "Derivatives and differentiability of multivariable functions"
    ]
  },
  {
    "objectID": "teaching/calculus-uc3m-robotics/3.4 Triple integrals.html",
    "href": "teaching/calculus-uc3m-robotics/3.4 Triple integrals.html",
    "title": "Triple integrals",
    "section": "",
    "text": "Triple integrals\nThe triple integral of \\(f\\colon D\\subseteq \\mathbb{R}^{3}\\rightarrow\\mathbb{R}\\) over a closed and bounded region \\(\\Omega\\subseteq D\\) is the three-dimensional analogue of the notion of [[3.2 Integrals|integral of a scalar function]] over \\([a,b]\\subset \\mathbb{R}\\), and of [[3.3 Double integrals|double integral of a scalar function]] over an elementary region \\(A\\subset\\mathbb{R}^{2}\\).\nSpecifically, we consider a function \\(f\\colon D\\subseteq \\mathbb{R}^{3}\\rightarrow \\mathbb{R}\\) and a set \\(\\Omega\\subseteq D\\) on which \\(f&gt;0\\), and introduce the set \\[\nS_{\\Omega}^{f}=\\left\\{(x,y,z)\\in\\mathbb{R}^{3}\\;|\\quad (x,y)\\in\\Omega,\\;z=f(x,y)\\right\\} .\n\\] The triple integral of \\(f\\) over \\(\\Omega\\) is built in order to generalize the notion of 3-d volume to the 4-d case, thus providing a definition of the hyper-volume of the set \\(S_{\\Omega}^{f}\\). Among other things, it is required that the triple integral coincides with the 3-d volume of \\(\\Omega\\) when \\(f(x,y,z)=1\\) on \\(\\Omega\\).\nOf course, the existence of the integral depends on both \\(\\Omega\\) and \\(f\\). In particular, unlike the one-dimensional case, and in analogy with the two-dimensional case, we must be careful in choosing the region \\(\\Omega\\) of integration.\nWhen it exists, the integral of \\(f\\) over \\(\\Omega\\) is denoted as \\[\n\\mathrm{V}(S_{\\Omega}^{f})=\\iiint_{\\Omega} f\\;\\mathrm{d}V.\n\\] It is then possible to extend the notion of triple integral also to functions admitting negative values, this time with little to lose because a direct physical interpretation of an hyper-volume is already non-trivial for positive functions.\nAgain, because of the little time at our disposal, and because of the difficulty of the argument, we do not discuss how to explicitly define the triple integral of \\(f\\) on \\(\\Omega\\) and simply jump to how to compute it in some quite useful particular cases. More details can be found in Paul’s Online Notes, or in chapter 15 of the Mathematics LibreText, or in chapter 5 of M-T.\nIt turns out we can evaluate \\(\\mathrm{V}(S_{\\Omega}^{f})\\) for subsets that are known as elementary regions in \\(\\mathbb{R}^{3}\\).\n\n[!defn] Definition: elementary regions of 3-d space Let \\(D\\) be an [[3.3 Double integrals#^c48dd6|elementary region]] in the \\(xy\\)-plane and let \\(\\phi_{1},\\phi_{2}\\) be continuous scalar functions on \\(D\\) such that \\(\\phi_{2}\\geq\\phi_{1}\\). Then \\[\nW:=\\left\\{(x,y,z)\\in \\mathbb{R}^{3}\\,|\\quad (x,y)\\in D,\\; \\phi_{1}(x,y)\\leq z\\leq \\phi_{2}(x,y)\\right\\}\n\\] is a \\(z\\)-simple region in \\(\\mathbb{R}^{3}\\). The boundary of a \\(z\\)-simple region is composed by at most \\(4\\) planar regions (with boundary). Proceeding by analogy, we can define \\(x\\)-simple and \\(y\\)-simple regions in \\(\\mathbb{R}^{3}\\). Then, a region which is either \\(x\\)-simple or \\(y\\)-simple or \\(z\\)-simple is called elementary.\n\n\n\nTriple integrals over boxes\nThe first case we consider is the case in which \\(\\Omega\\) is a box of the form \\([a,b]\\times[c,d]\\times [p,q]\\subset\\mathbb{R}^{3}\\). In this case, we also write \\(B\\) instead of \\(\\Omega\\).\nIntuitively speaking, \\(\\mathrm{V}(S_{B}^{f})\\) is computed as the limit of its approximation “from below” and “from above” in analogy with the [[3.2 Integrals#^964215|one-dimensional case]] and [[3.3 Double integrals#^050619|two-dimensional case]]. Of course, the integral exists when both approximations converge to the same limit. In this case, we write \\[\n\\mathrm{V}(S_{B}^{f})=\\iiint_{B} f \\,\\mathrm{d}V\n\\] as anticipated above.\nWithout discussing technical details, we write down a magnificent theorem that helps us in computing \\(\\mathrm{V}(S_{B}^{f})\\) for an interesting class of functions.\n\n[!important] Theorem: Fubini’s theorem Let \\(f\\colon B\\subset\\mathbb{R}^{3}\\rightarrow \\mathbb{R}\\) be continuous except on the boundary of an elementary region \\(W\\subseteq B\\). Then \\(\\mathrm{V}(S_{B}^{f})\\) exists. Moreover, if the integral \\[\nF_{[p,q]}(x,y)= \\int_{p}^{q}f(x,y,z)\\;\\mathrm{d}z\n\\] exists for all \\((x,y)\\) in an elementary region \\(D\\) in the \\(xy\\)-plane, then, \\(\\mathrm{V}(S_{B}^{f})\\) is equal to the following iterated integral: \\[\n\\mathrm{V}(S_{B}^{f})=\\iiint_{B}f\\;\\mathrm{d}V=\\iint_{D}F_{[p,q]}(x,y)\\;\\mathrm{d}A,\n\\] whose right-hand-side can be evaluated using [[3.3 Double integrals#^1d3dd7|Fubini’s theorem for double integrals]] obtaining either \\[\n\\mathrm{V}(S_{B}^{f})=\\iiint_{B}f\\;\\mathrm{d}V=\\int_{a}^{b}\\int_{c}^{d}\\int_{p}^{q}f(x,y,z)\\;\\mathrm{d}z\\,\\mathrm{d}y\\,\\mathrm{d}x\n\\] or \\[\n\\mathrm{V}(S_{B}^{f})=\\iiint_{B}f\\;\\mathrm{d}V=\\int_{c}^{d}\\int_{a}^{b}\\int_{p}^{q}f(x,y,z)\\;\\mathrm{d}z\\,\\mathrm{d}x\\,\\mathrm{d}y .\n\\] Similarly, if the integral \\[\nF_{[c,d]}(x,z)= \\int_{c}^{d}f(x,y,z)\\;\\mathrm{d}y\n\\] exists for all \\((x,z)\\) in an elementary region \\(D\\) in the \\(xz\\)-plane, then \\(\\mathrm{V}(S_{B}^{f})\\) is equal to the following iterated integral: \\[\n\\mathrm{V}(S_{B}^{f})=\\iiint_{B}f\\;\\mathrm{d}V=\\iint_{D}F_{[c,d]}(x,z)\\;\\mathrm{d}A,\n\\] whose right-hand-side can be evaluated using [[3.3 Double integrals#^1d3dd7|Fubini’s theorem for double integrals]] obtaining either \\[\n\\mathrm{V}(S_{B}^{f})=\\iiint_{B}f\\;\\mathrm{d}V=\\int_{a}^{b}\\int_{p}^{q}\\int_{c}^{d}f(x,y,z)\\;\\mathrm{d}y\\,\\mathrm{d}z\\,\\mathrm{d}x\n\\]\nor\n\\[\n\\mathrm{V}(S_{B}^{f})=\\iiint_{B}f\\;\\mathrm{d}V=\\int_{p}^{q}\\int_{a}^{b}\\int_{c}^{d}f(x,y,z)\\;\\mathrm{d}y\\,\\mathrm{d}x\\,\\mathrm{d}z .\n\\]\nContinuing, if the integral\n\\[\nF_{[a,b]}(y,z)= \\int_{a}^{b}f(x,y,z)\\;\\mathrm{d}x\n\\]\nexists for all \\((y,z)\\) in an elementary region \\(D\\) in the \\(yz\\)-plane, then \\(\\mathrm{V}(S_{B}^{f})\\) is equal to the following iterated integral:\n\\[\n\\mathrm{V}(S_{B}^{f})=\\iiint_{B}f\\;\\mathrm{d}V=\\iint_{D}F_{[a,b]}(y,z)\\;\\mathrm{d}A,\n\\]\nwhose right-hand-side can be evaluated using [[3.3 Double integrals#^1d3dd7|Fubini’s theorem for double integrals]] obtaining either \\[\n\\mathrm{V}(S_{B}^{f})=\\iiint_{B}f\\;\\mathrm{d}V=\\int_{c}^{d}\\int_{p}^{q}\\int_{a}^{b}f(x,y,z)\\;\\mathrm{d}x\\,\\mathrm{d}z\\,\\mathrm{d}y\n\\] or \\[\n\\mathrm{V}(S_{B}^{f})=\\iiint_{B}f\\;\\mathrm{d}V=\\int_{p}^{q}\\int_{c}^{d}\\int_{a}^{b}f(x,y,z)\\;\\mathrm{d}x\\,\\mathrm{d}y\\,\\mathrm{d}z .\n\\] Finally, if all previous assumptions hold (like when \\(f\\) is continuous over the whole \\(B\\)), we obtain \\[\n\\begin{split}\n\\iiint_{B}f\\;\\mathrm{d}V&=\\int_{a}^{b}\\int_{c}^{d}\\int_{p}^{q}f(x,y,z)\\;\\mathrm{d}z\\,\\mathrm{d}y\\,\\mathrm{d}x= \\int_{c}^{d}\\int_{a}^{b}\\int_{p}^{q}f(x,y,z)\\;\\mathrm{d}z\\,\\mathrm{d}x\\,\\mathrm{d}y=\\\\\n&=\\int_{a}^{b}\\int_{p}^{q}\\int_{c}^{d}f(x,y,z)\\;\\mathrm{d}y\\,\\mathrm{d}z\\,\\mathrm{d}x= \\int_{p}^{q}\\int_{a}^{b}\\int_{c}^{d}f(x,y,z)\\;\\mathrm{d}y\\,\\mathrm{d}x\\,\\mathrm{d}z =\\\\\n&=\\int_{c}^{d}\\int_{p}^{q}\\int_{a}^{b}f(x,y,z)\\;\\mathrm{d}x\\,\\mathrm{d}z\\,\\mathrm{d}y=\\int_{p}^{q}\\int_{c}^{d}\\int_{a}^{b}f(x,y,z)\\;\\mathrm{d}x\\,\\mathrm{d}y\\,\\mathrm{d}z,\n\\end{split}\n\\] and we are free to choose the one we prefer for the sake of computations.\n\n\n[!exmp] Example Let \\(B=[0,1]\\times [-\\frac{1}{2},0]\\times [0,\\frac{1}{3}]\\), and let \\(f(x,y,z) = (x+2y+3z)^2\\). Then we have \\[\n\\begin{split}\n\\iint_{B}(x+2y+3z)^{2}\\,\\mathrm{d}V &=\\int_{0}^{1}\\int_{-\\frac{1}{2}}^{0}\\int_{0}^{\\frac{1}{3}}(x+2y+3z)^{2}\\;\\mathrm{d}z\\,\\mathrm{d}y\\,\\mathrm{d}x= \\\\ & =  \\int_{0}^{1}\\int_{-\\frac{1}{2}}^{0}\\left[\\frac{(x+2y+3z)^{3}}{9}\\right]_{0}^{\\frac{1}{3}}\\;\\mathrm{d}y\\,\\mathrm{d}x = \\\\ &  =  \\frac{1}{9}\\int_{0}^{1}\\int_{-\\frac{1}{2}}^{0}(x+2y+1)^{3} - (x+2y)^{3} \\;\\mathrm{d}y\\,\\mathrm{d}x = \\\\ &  =  \\frac{1}{72}\\int_{0}^{1}\\left[(x+2y+1)^{4} - (x+2y)^{4} \\right]_{-\\frac{1}{2}}^{0}\\; \\mathrm{d}x  = \\\\ &  =  \\frac{1}{72}\\int_{0}^{1}  (x+1)^{4} + (x-1)^{4} -  2x^{4}   \\; \\mathrm{d}x = \\\\ &  =  \\frac{1}{360}\\left[  (x+1)^{5} + (x-1)^{5} -  2x^{5}\\right]_{0}^{1} =\\frac{1}{12}.\n\\end{split}\n\\] Of course, if we exchange the order of integration, we get the same result.\n\n\n\nTriple integrals over elementary regions\nThe definition of triple integrals over elementary regions can be easily accommodated exploiting Fubini’s theorem recalled above.\nAssume \\(W\\) is an elementary region strictly contained in the box \\(B=[a,b]\\times [c,d] \\times [p,q]\\), and assume \\(f\\colon W\\subset\\mathbb{R}^{3}\\rightarrow\\mathbb{R}\\) is continuous. We extend \\(f\\) from \\(W\\) to \\(B\\) setting \\(f(x,y,z)=0\\) for every \\((x,y,z)\\in B\\) but not in \\(W\\).\nThis almost trivial extension of \\(f\\) (denoted again by \\(f\\) with an evident abuse of notation) turns out to be continuous on \\(B\\) except on the boundary of the elementary region \\(W\\). Therefore, the hypothesis of Fubini’s theorem recalled above apply to the (extension of) \\(f\\) and we set \\[\n\\mathrm{V}(S_{W}^{f})\\equiv\\iiint_{W} f\\;\\mathrm{d}V := \\iiint_{B}f\\;\\mathrm{d}V=\\mathrm{V}(S_{B}^{f}).\n\\]\n\n[!attention] Remark As already mentioned above, when \\(f(x,y,z)=1\\) for every \\((x,y,z)\\in W\\), the triple integral of \\(f\\) over \\(W\\), when it exists, coincides with the volume of \\(W\\).\n\nThe explicit computation of \\(\\mathrm{V}(S_{W}^{f})\\) can be performed as follows. First of all, suppose \\(W\\) is \\(z\\)-simple. The integral\n\\[\nF_{[a,b,c,d]}(x,y)=\\int_{p}^{q}f(x,y,z)\\;\\mathrm{d}z\n\\]\nexists because \\(f(x,y,z)\\), seen as a function of \\(z\\), is continuous in \\([p,q]\\) except at a finite number of points (recall the proposition on the [[6.1 Anti-derivative and integrals of scalar functions|interval additivity]] of the one-dimensional integral). Moreover, \\(f(x,y,z)=0\\) for all \\(p\\leq z&lt;\\phi_{1}(x,y)\\) and for all \\(\\phi_{2}(x,y)&lt;z\\leq q\\), so that it holds\n\\[\nF_{[a,b,c,d]}(x,y)=\\underbrace{\\int_{p}^{\\phi_{1}(x,y)}f(x,y,z)\\;\\mathrm{d}z}_{=0} + \\int_{\\phi_{1}(x,y)}^{\\phi_{2}(x,y)}f(x,y,z)\\;\\mathrm{d}z + \\overbrace{\\int_{\\phi_{2}(x,y)}^{q}f(x,y,z)\\;\\mathrm{d}z}^{=0}= \\int_{\\phi_{1}(x,y)}^{\\phi_{2}(x,y)}f(x,y,z)\\;\\mathrm{d}z .\n\\]\nNow, we are left with\n\\[\n\\iiint_{W}f(x,y,z) \\;\\mathrm{d}V=\\iint_{D}F_{a,b,c,d}(x,y)\\;\\mathrm{d}A\n\\]\nwhere \\(D\\) is the region in the \\(xy\\)-plane in terms of which \\(W\\) is defined. Suppose \\(D\\) is \\(x\\)-simple (but the procedure is analogous if \\(D\\) is \\(y\\)-simple). Since \\(F_{a,b,c,d}(x,y)\\) is continuous as a function of \\(y\\) except at most at the boundary of \\(D\\) because of the [[3.2 Integrals#^0523d5|fundamental theorem of calculus]], we have\n\\[\n\\mathrm{V}(S_{W}^{f})\\equiv\\iiint_{W} f(x,y,z)\\;\\mathrm{d}V := \\iint_{D}F_{[a,b,c,d]}(x,y)\\, \\mathrm{d}A = \\int_{a}^{b}\\int_{\\psi_{1}(x)}^{\\psi_{2}(x)}F_{a,b,c,d}(x,y)\\;\\mathrm{d}y\\,\\mathrm{d}x= \\int_{a}^{b}\\int_{\\psi_{1}(x)}^{\\psi_{2}(x)}\\int_{\\phi_{1}(x,y)}^{\\phi_{2}(x,y)}f(x,y,z)\\;\\mathrm{d}z \\;\\mathrm{d}y\\,\\mathrm{d}x,\n\\]\nwhere \\(\\psi_{1}\\) and \\(\\psi_{2}\\) are the functions with respect to which \\(D\\) is \\(x\\)-simple.\nAll other cases (e.g, when \\(W\\) is \\(y\\)-simple or \\(x\\)-simple) can be treated analogously, and when \\(W\\) is both \\(x\\)-simple, \\(y\\)-simple, and \\(z\\)-simple then all the possible expressions for \\(V(S_{W}^{f})\\) are perfectly valid.\nIt turns out that being able to exchange the role of integrals could be quite effective in reducing computational difficulties.\n\n[!important] Proposition: Linearity of the triple integral Let \\(W\\) be an elementary region in \\(\\mathbb{R}^{3}\\) and let \\(f,g\\) be continuous on \\(W\\), except at most on its boundary. Then it holds \\[\n\\iiint_{W}(f+g)\\;\\mathrm{d}V=\\iiint_{W}f\\;\\mathrm{d}V + \\iiint_{W}g\\;\\mathrm{d}V,\n\\] that is, the triple integral is linear.\n\n\n[!exmp] Example Let us compute the volume of the ball \\(\\mathcal{B}\\) of radius \\(1\\) centered in \\((0,0,0)\\). Remember that this ball is determined by the inequality \\[\nx^{2} + y^{2} + z^{2}\\leq 1 ,\n\\] and all we need to show is that it is an elementary region and solve the integral \\[\n\\iiint_{\\mathcal{B}} \\,\\mathrm{d}V .\n\\] First of all, we note that the projection of \\(\\mathcal{B}\\) on the \\(xy\\)-plane is a unit circle in \\(\\mathbb{R}^{2}\\) centered in \\((0,0)\\). This is an elementary region in the \\(xy\\)-plane that can be described by \\(-1\\leq x\\leq 1\\) and \\(-\\sqrt{1-x^{2}}\\leq y \\leq \\sqrt{1-x^{2}}\\). Then, we exploit the inequality above to infer that \\[\n-\\sqrt{1- x^{2} -y^{2}}\\leq z\\leq \\sqrt{1- x^{2} -y^{2}}\\equiv \\phi(x,y) ,\n\\] so that \\[\n\\mathcal{B}=\\left\\{(x,y,z)\\in\\mathbb{R}^{3}\\;|\\quad -1\\leq x\\leq 1,\\; -\\sqrt{1-x^{2}}\\leq y \\leq \\sqrt{1-x^{2}},\\;-\\sqrt{1- x^{2} -y^{2}}\\leq z\\leq \\sqrt{1- x^{2} -y^{2}}\\right\\},\n\\] which shows that \\(\\mathcal{B}\\) is indeed an elementary region. Then, we have \\[\n\\begin{split}\n\\iiint_{\\mathcal{B}} \\,\\mathrm{d}V &=\\int_{-1}^{1}\\int_{-\\sqrt{1-x^{2}}}^{\\sqrt{1-x^{2}}}\\int_{-\\sqrt{1-x^{2}-y^{2}}}^{\\sqrt{1-x^{2}-y^{2}}}\\,\\mathrm{d}z\\,\\mathrm{d}y\\,\\mathrm{d}x= 2\\int_{-1}^{1}\\int_{-\\sqrt{1-x^{2}}}^{\\sqrt{1-x^{2}}}\\sqrt{1-x^{2} -y^{2}}\\;\\mathrm{d}y\\,\\mathrm{d}x .\n\\end{split}\n\\] We now apply the change of variables \\(y = \\sqrt{1-x^{2}}\\sin(t)\\) so that \\[\n\\begin{split}\n\\int_{-\\sqrt{1-x^{2}}}^{\\sqrt{1-x^{2}}}\\sqrt{1-x^{2} -y^{2}}\\;\\mathrm{d}y & =\\int_{-\\frac{\\pi}{2}}^{\\frac{\\pi}{2}}(1-x^{2})\\cos^{2}(t)\\;\\mathrm{d}t= \\\\ & = \\frac{(1-x^{2})}{2}[t + \\sin(t)\\cos(t)]_{-\\frac{\\pi}{2}}^{\\frac{\\pi}{2}} = \\\\ & = (1-x^{2})\\frac{\\pi}{2},\n\\end{split}\n\\] and thus \\[\n\\begin{split}\n\\iiint_{\\mathcal{B}} \\,\\mathrm{d}V &= 2\\int_{-1}^{1}\\int_{-\\sqrt{1-x^{2}}}^{\\sqrt{1-x^{2}}}\\sqrt{1-x^{2} -y^{2}}\\;\\mathrm{d}y\\,\\mathrm{d}x = \\pi\\int_{-1}^{1}(1-x^{2}) \\,\\mathrm{d}x = \\\\ & = \\pi\\left[x -\\frac{x^{3}}{3}\\right]_{-1}^{1}\\,\\mathrm{d}x= \\frac{4}{3}\\pi.\n\\end{split}\n\\]\n\n\n\nTriple integrals and change of variables\nSometimes, it could be useful to transform a triple integral through an appropriate change of variables.\nIn this case, it turns out that the appropriate way to transform the triple integral is through the use of the determinant of the matrix of the partial derivatives of the function implementing the change of variables.\n\n[!important] Proposition Let \\(\\overline{W^{*}},\\overline{W}\\subseteq\\mathbb{R}^{3}\\) be elementary regions, and let \\(W^{*},W\\) denote their open interiors. Let \\(T\\colon \\overline{W^{*}}\\rightarrow\\mathbb{R}^{3}\\) be a one-to-one continuous map which is \\(C^{1}\\) on \\(W^{*}\\), and is such that \\(T(W^{*})=W\\). Let \\(f\\colon \\overline{W}\\rightarrow \\mathbb{R}\\) be continuous so that its triple integral over \\(W\\) exists. Then it holds \\[\n\\iiint_{\\overline{W}}f(x,y,y)\\,\\mathrm{d}V = \\iiint_{\\overline{W^{*}}}f\\circ T(u,v,w)\\,|DT(u,v,w)|\\;\\mathrm{d}V,\n\\] where \\(|DT(u,v,w)|\\) is the determinant of the matrix of partial derivatives of \\(T\\) at the point \\((u,v,w)\\).\n\n\n[!exmp] Example Let \\(W\\) be the unit ball in \\(\\mathbb{R}^{3}\\). Let us use the change of variable formula to evaluate the triple integral \\[\n\\iiint_{W}\\left(\\mathrm{e}^{x^{2} + y^{2} + z^{2}}\\right)^{\\frac{3}{2}}\\,\\mathrm{d}V\n\\] when the transformation \\(T\\) is the one implementing the [[2.1 From 1 to many dimensions#^b5d3f4|change of coordinates from spherical to Cartesian]]: \\[\\begin{split} \\iiint_{W}\\left(\\mathrm{e}^{x^{2} + y^{2} + z^{2}}\\right)^{\\frac{3}{2}}\\,\\mathrm{d}V&= \\iiint_{W}\\mathrm{e}^{r^{3}}\\,|DT(r,\\theta,\\varphi)|\\mathrm{d}V \\\\ & = \\int_{0}^{1}\\int_{0}^{2\\pi}\\int_{0}^{\\pi}\\mathrm{e}^{r^{3}}\\,r^{2}\\,\\mathrm{d}\\varphi\\mathrm{d}\\theta\\mathrm{d}r=2\\pi^{2}\\left[\\frac{\\mathrm{e}^{r^{3}}}{3}\\right]_{0}^{1} .\\end{split}\\]",
    "crumbs": [
      "Home",
      "Teaching",
      "Calculus - Robotics Engineering - UC3M",
      "Triple integrals"
    ]
  },
  {
    "objectID": "teaching/calculus-uc3m-robotics/2.2 Limits of multivariable functions.html",
    "href": "teaching/calculus-uc3m-robotics/2.2 Limits of multivariable functions.html",
    "title": "Limits multivariable vector functions",
    "section": "",
    "text": "Limits multivariable vector functions\nThe idea behind the notion of limit for multivariable vector functions is the same as for [[1.2 Limits of scalar functions|scalar functions]]. Essentially, the limit exists when we can stay as close as we want to an output point (the limit itself) by suitably controlling how the input is closer to a fixed point. As before, to make this intuition precise, we need \\(\\epsilon\\) and \\(\\delta\\).\n\n[!defn] Definition: \\((\\epsilon,\\delta)\\)-definition of limit Consider the function \\((D,\\mathbb{R}^{n},f,\\mathbb{R}^{m})\\), and the point \\(\\mathbf{x}_{0}\\in\\mathbb{R}^{n}\\). Assume either \\(\\mathbf{x}_{0}\\) is an interior point of \\(D\\), or it is a [[2.1 From 1 to many dimensions|boundary point]] of \\(D\\). We say that \\[\n\\lim_{\\mathbf{x}\\rightarrow \\mathbf{x}_{0}}\\,f(\\mathbf{x})= \\mathbf{L}\n\\] if, for every \\(\\epsilon &gt;0\\), there is \\(\\delta_{\\epsilon}&gt;0\\) such that\n\\[\nd(f(\\mathbf{x}),\\mathbf{L})=||f(\\mathbf{x}) - \\mathbf{L}||&lt; \\epsilon\n\\] for every \\(\\mathbf{x}\\in D\\) such that \\[\nd(\\mathbf{x},\\mathbf{x}_{0})=||\\mathbf{x} - \\mathbf{x}_{0}||&lt; \\delta .\n\\]\n\nThe notion of limit is well-posed because of the following result, whose proof represents an instructive exercise (but can also be found as theorem 2 in section 2.2 in M-T) .\n\n[!important] Theorem: uniqueness of limit If \\(\\lim_{\\mathbf{x}\\rightarrow \\mathbf{x}_{0}}f(\\mathbf{x})=\\mathbf{L}\\) and \\(\\lim_{\\mathbf{x}\\rightarrow \\mathbf{x}_{0}}f(\\mathbf{x})=\\mathbf{M}\\), then \\(\\mathbf{L}=\\mathbf{M}\\).\n\n\n[!exmp] Example Let \\((D,\\mathbb{R}^{n},f,\\mathbb{R}^{n})\\) be given by \\(f(\\mathbf{x})=\\mathbf{x}\\), and let \\(\\mathbf{x}_{0}\\) be either an [[2.1 From 1 to many dimensions#^ceea9b|interior point or a boundary point]] of \\(D\\). Then \\[\n\\lim_{\\mathbf{x}\\rightarrow \\mathbf{x}_{0}}\\,f(\\mathbf{x})=\\mathbf{x}_{0}\n\\] because \\[\n||f(\\mathbf{x}) - \\mathbf{x}_{0}||=||\\mathbf{x} - \\mathbf{x}_{0}||\n\\] and thus, fixing \\(\\delta_{\\epsilon}&lt;\\epsilon\\), we obtain that \\(||\\mathbf{x} - \\mathbf{x}_{0}||&lt;\\delta_{\\epsilon}\\) implies \\(||f(\\mathbf{x}) - \\mathbf{x}_{0}||&lt;\\epsilon\\). If we write \\(\\mathbf{x}=(x^{1},\\cdots, x^{n})\\), we can consider the projection on the \\(j\\)-th factor as the function \\((D,\\mathbb{R}^{n},f^{j},\\mathbb{R})\\) given by \\(f^{j}(\\mathbf{x})=x^{j}\\). Then, it should be easy to prove that \\[\n\\lim_{\\mathbf{x}\\rightarrow \\mathbf{x}_{0}}\\,f^{j}(\\mathbf{x})=x_{0}^{j}\n\\] where \\(\\mathbf{x}_{0}=(x^{1}_{0},\\cdots, x^{n}_{0})\\).\n\n\n\nProperties of limits\nIn analogy with the [[1.2 Limits of scalar functions#^f50bb0|scalar case]], limits are often computed exploiting previously known limits and the following algebraic manipulations of limits (theorem 3 in chapter 2 of M-T).\n\n[!important] Proposition: Algebraic manipulations of limits of multivariable scalar functions Let \\(\\mathbf{x}_{0}\\in\\mathbb{R}^{n}\\), \\((\\alpha_{1},\\cdots\\alpha_{m})\\) be a finite sequence of real numbers, and \\((f_{1},\\cdots,f_{m})\\) a finite sequence of multivariable scalar functions. Suppose that\n\\[\n\\lim_{\\mathbf{x}\\rightarrow \\mathbf{x}_{0}}f_{j}(\\mathbf{x})=\\mathbf{L}_{j}\\in \\mathbb{R}\n\\]\nfor every \\(j=1,...,n\\). Then, it holds\n\\[\n\\begin{split}\n1)\\quad & \\lim_{\\mathbf{x}\\rightarrow \\mathbf{x}_{0}} \\sum_{j=1}^{m}\\,\\alpha_{j}\\,f_{j}(\\mathbf{x}) =\\sum_{j=1}^{m}\\,\\lim_{\\mathbf{x}\\rightarrow \\mathbf{x}_{0}} \\left(\\alpha_{j}\\,f_{j}(\\mathbf{x})\\right) = \\sum_{j=1}^{m}\\,\\alpha_{j}\\,\\mathbf{L}_{j} \\\\ & \\\\\n2) \\quad &\\lim_{\\mathbf{x}\\rightarrow \\mathbf{x}_{0}}\\prod_{j=1}^{m} \\,\\alpha_{j}\\,f_{j}(\\mathbf{x}) =\\prod_{j=1}^{m}\\,\\lim_{\\mathbf{x}\\rightarrow \\mathbf{x}_{0}} \\left(\\alpha_{j}\\,f_{j}(\\mathbf{x})\\right) = \\prod_{j=1}^{m}\\,\\alpha_{j}\\,\\mathbf{L}_{j} .\n\\end{split}\n\\]\nMoreover, if \\(m=2\\) and \\(\\alpha_{2},\\mathbf{L}_{2}\\neq 0\\), it holds\n\\[\n\\lim_{\\mathbf{x}\\rightarrow \\mathbf{x}_{0}}\\,\\frac{\\alpha_{1}f_{1}(\\mathbf{x})}{\\alpha_{2}f_{2}(\\mathbf{x})}=\\frac{\\alpha_{1}\\mathbf{L}_{1}}{\\alpha_{2} \\mathbf{L}_{2}}.\n\\]\n\n\n[!rem] Remark: multivariable polynomials Using the algebraic manipulations of limits mentioned above, we can prove that \\[ \\lim_{\\mathbf{x}\\rightarrow\\mathbf{x}_{0}} P_{n}(\\mathbf{x})= P_{n}(\\mathbf{x}_{0})\\] for every \\(\\mathbf{x}_{0}\\in\\mathbb{R}^{n}\\), where \\(P_{n}\\) is a polynomial of order \\(n\\) in the variables \\(x_{1},\\cdots x_{n}\\) making up \\(\\mathbf{x}=(x_{1},\\cdots,x_{n})\\in\\mathbb{R}^{n}\\). In particular, every polynomial of this type is [[2.2 Limits of multivariable functions#^eef27f|continuous]].\n\nOften, the multivariable vector case can be reduced to the multivariable scalar case according to the following result (theorem 3 in chapter 2 of M-T).\n\n[!important] Proposition If \\(f\\colon D\\subseteq\\mathbb{R}^{n}\\rightarrow \\mathbb{R}^{m}\\) can be written as\n\\[\nf(\\mathbf{x})=\\left(f_{1}(\\mathbf{x}),\\cdots,f_{m}(\\mathbf{x})\\right)\n\\]\nwith \\(f_{j}\\colon D\\subseteq \\mathbb{R}^{n}\\rightarrow \\mathbb{R}\\) for every \\(j=1,\\cdots, m\\), then\n\\[\n\\lim_{\\mathbf{x}\\rightarrow\\mathbf{x}_{0}}\\,f(\\mathbf{x})=\\mathbf{b}=(b_{1},\\cdots,b_{m})\n\\]\nif and only if\n\\[\n\\lim_{\\mathbf{x}\\rightarrow\\mathbf{x}_{0}}\\,f_{j}(\\mathbf{x})=b_{j}\n\\]\nfor every \\(j=1,...,m\\).\n\nFor a function \\((D,\\mathbb{R},f,\\mathbb{R})\\), there are only two ways we can approach a point \\(x_{0}\\) in the input space when evaluating the limit \\(x\\rightarrow x_{0}\\). Specifically, we can approach \\(x_{0}\\) either from the left, or from the right.\nThe same is no longer true for multivariable vector functions. For instance, for \\((D,\\mathbb{R}^{2},f,\\mathbb{R})\\), when we evaluate the limit for \\((x,y)\\rightarrow (0,0)\\), we can approach the origin along infinitely many different paths (e.g., all lines passing through the origin, all parabolas passing through the origin, and, more generally, all curves in the plane passing through the origin).\nThis instance can be helpful to prove that a given limit does not exist by showing that we get different values when evaluating the limit along different trajectories.\n\n[!exmp] Example Let us consider the function \\(f\\colon\\mathbb{R}^{2}-\\{(0,0)\\}\\rightarrow \\mathbb{R}\\) given by\n\\[\nf(x,y)=\\frac{x^{2} - y^{2}}{x^{2} + y^{2}} ,\n\\]\nand let us investigat the limit for \\((x,y)\\rightarrow (0,0)\\). If we approach the origin along the \\(x\\)-axis \\(y=0\\), we obtain\n\\[\nf(x,0)=\\frac{x^{2}}{x^{2}}\\stackrel{x\\rightarrow 0}{\\longrightarrow} 1 .\n\\]\nHowever, if we approach the origin along the \\(y\\)-axis \\(x=0\\), we obtain\n\\[\nf(0,y)=\\frac{-y^{2}}{y^{2}}\\stackrel{y\\rightarrow 0}{\\longrightarrow} -1 .\n\\]\nMoreover, if we approach the limit along the line \\(x=y\\), we obtain\n\\[\nf(x,x)=\\frac{0}{2x^{2}}\\stackrel{x\\rightarrow 0}{\\longrightarrow} 0 .\n\\]\nWe thus see that the result depends on the path we take to “compute the limit”, and thus we conclude that the limit of \\(f\\) for \\((x,y)\\rightarrow (0,0)\\) does not exist.\n\nLet us now consider another instructive example that should prevent you to make a very common mistake, namely, to think that switching to polar coordinates is enough to compute a limit.\n\n[!exmp] Example (Limits and polar coordinates) Let \\(f\\colon \\mathbb{R}^{2} - \\{(0,0)\\}\\rightarrow \\mathbb{R}\\) be given by \\[\nf(x,y)=\\frac{x^{2}\\,y}{x^{4} + y^{2}},\n\\]\nand let us investigate the limit\n\\[\n\\lim_{(x,y)\\rightarrow(0,0)}\\,f(x,y) .\n\\]\nIf we pass to polar coordinates, we have that \\((x,y)\\rightarrow (0,0)\\) implies \\(r\\rightarrow 0\\), and we immediately compute\n\\[\n\\lim_{r\\rightarrow 0}\\,f(r\\cos(\\theta),r\\sin(\\theta))=\\lim_{r\\rightarrow 0}\\,\\frac{r^{3}\\cos^{2}(\\theta)\\sin(\\theta)}{r^{2}(r^{2}\\cos^{4}(\\theta) + \\sin^{2}(\\theta))}=0 .\n\\]\nIt may happen that you think that the previous limit forces the result\n\\[\n\\lim_{(x,y)\\rightarrow(0,0)}\\,f(x,y) =0 ,\n\\]\nbut this is definitely wrong. Indeed, if we evaluate the limit along the parabola \\(y=x^{2}\\) we obtain\n\\[\n\\lim_{(x,y)\\rightarrow(0,0)}\\,f(x,x^{2}) = \\,\\lim_{(x,y)\\rightarrow(0,0)}\\, \\frac{x^{4}}{2x^{4}}=\\frac{1}{2} ,\n\\]\nthus showing that the original limit actually does not exist.\n\nJust as in the [[1.2 Limits of scalar functions#^ec717d|scalar case]], the squeeze theorem holds for multivariable scalar functions and makes our lives better.\n\n[!important] Theorem: Squeeze (pinching) theorem Let \\(f,g,h\\) be multivariable scalar functions with the same domain \\(D\\subseteq \\mathbb{R}^{n}\\), and let \\(x_{0}\\) be an interior point for \\(D\\). If \\(h(\\mathbf{x})\\leq f(\\mathbf{x})\\leq g(\\mathbf{x})\\) for all \\(\\mathbf{x}\\in D\\), and \\[\n\\lim_{\\mathbf{x}\\rightarrow \\mathbf{x}_{0} }h(\\mathbf{x})=\\lim_{\\mathbf{x}\\rightarrow \\mathbf{x}_{0} }g(\\mathbf{x})=L\\in\\mathbb{R},\n\\] then it follows that \\[\n\\lim_{\\mathbf{x}\\rightarrow \\mathbf{x}_{0} }f(\\mathbf{x})=L.\n\\]\n\n\n\nContinuity of multivariable vector functions\nJust as in the [[1.2 Limits of scalar functions#^91cff8|scalar case]], from an intuitive point of view, a scalar function is continuous if its graph can be drawn without lifting the pen/pencil from the paper, and the following definition provides a rigorous mathematical formulation of this idea.\n\n[!defn] Definition: The function \\(f\\colon D\\subseteq \\mathbb{R}^{n}\\rightarrow \\mathbb{R}^{m}\\) is said to be continuous at \\(\\mathbf{x}_{0}\\in D\\) if\n\\[\n\\lim_{\\mathbf{x}\\rightarrow \\mathbf{x}_{0}}f(\\mathbf{x})=f(\\mathbf{x}_{0})\n\\]\nThe function \\(f\\) is said to be continuous if it is continuous at \\(\\mathbf{x}_{0}\\) for every \\(\\mathbf{x}_{0}\\in D\\).\n\n^eef27f\nSome important results about continuous functions are collected below (and their proofs can be found, for instance, in chapter 2 of M-T).\n\n[!important] Proposition: algebraic manipulations of continuous functions Let \\(f,g\\colon D\\subseteq \\mathbb{R}^{n}\\rightarrow \\mathbb{R}^{m}\\). Then: 1) if \\(f,g\\) are continuous at \\(\\mathbf{x}_{0}\\) and \\(\\alpha,\\beta\\in\\mathbb{R}\\) then \\(\\alpha f + \\beta g\\) is continuous at \\(\\mathbf{x}_{0}\\); 2) if \\(m=1\\), \\(f\\) and \\(g\\) are continuous at \\(\\mathbf{x}_{0}\\), and \\(\\alpha,\\beta\\in\\mathbb{R}\\), then \\((\\alpha f)(\\beta g)\\) is continuous at \\(\\mathbf{x}_{0}\\); 3) if \\(m=1\\), \\(f\\) and \\(g\\) are continuous at \\(\\mathbf{x}_{0}\\), and \\(\\alpha,\\beta\\in\\mathbb{R}\\) then \\(\\frac{\\alpha f}{\\beta g}\\) is continuous at \\(\\mathbf{x}_{0}\\) provided that \\(\\beta,g(\\mathbf{x}_{0})\\neq 0\\); 4) if \\(f(\\mathbf{x})=(f_{1}(\\mathbf{x}),\\cdots,f_{m}(\\mathbf{x}))\\) with \\(f_{j}\\colon\\mathbb{R}^{n}\\rightarrow \\mathbb{R}\\) for every \\(j=1,...,m\\), then \\(f\\) is continuous at \\(\\mathbf{x}_{0}\\) if and only if each \\(f_{j}\\) is continuous at \\(\\mathbf{x}_{0}\\); 5) if \\(m=n=1\\), and \\(f\\) is invertible and continuous on an open interval \\(I\\) then its inverse \\(f^{-1}\\) is also continuous.\n\n\n[!important] Proposition: composition of continuous functions Let \\(g\\colon D\\subseteq \\mathbb{R}^{n}\\rightarrow \\mathbb{R}^{m}\\) and \\(f\\colon B\\subseteq \\mathbb{R}^{m}\\rightarrow \\mathbb{R}^{k}\\). Suppose that \\(g(D)\\subseteq B\\) so that \\(f\\circ g\\) is defined on \\(D\\). If \\(g\\) is continuous at \\(\\mathbf{x}_{0}\\) and \\(f\\) is continuous at \\(\\mathbf{y}_{0}=g(\\mathbf{x}_{0})\\), then \\(f\\circ g\\) is continuous at \\(\\mathbf{x}_{0}\\).\n\n^7370dd\n\n[!exmp] Example The limit of composite functions may be used to prove that \\[ \\lim_{\\mathbf{x}\\rightarrow\\mathbf{0}} \\frac{\\sin(\\Vert\\mathbf{x}\\Vert^{2})}{\\Vert\\mathbf{x}\\Vert^{2}} = 1.\\] Indeed, we know that \\(\\frac{\\sin(t)}{t}\\) can be extended to be continuous for every \\(t\\in \\mathbb{R}\\) setting it equal to \\(1\\) when \\(t=0\\), and we also know that \\(\\Vert \\mathbf{x}\\Vert^{2}=x_{1}^{2}+x_{2}^{2}+\\cdots + x_{n}^{2}\\) is continuous for every \\(\\mathbf{x}=(x_{1},\\cdots,x_{n})\\in\\mathbb{R}^{n}\\). Therefore, a direct application of the [[2.2 Limits of multivariable functions#^7370dd|previous proposition]] gives the result.",
    "crumbs": [
      "Home",
      "Teaching",
      "Calculus - Robotics Engineering - UC3M",
      "Limits multivariable vector functions"
    ]
  },
  {
    "objectID": "teaching/calculus-uc3m-robotics/3.3 Double integrals.html",
    "href": "teaching/calculus-uc3m-robotics/3.3 Double integrals.html",
    "title": "Double integrals",
    "section": "",
    "text": "Double integrals\nThe double integral of \\(f\\colon D\\subseteq \\mathbb{R}^{2}\\rightarrow\\mathbb{R}\\) over a closed and bounded region \\(\\Omega\\subseteq D\\) is the two-dimensional analogue of the notion of [[3.2 Integrals|integral of a scalar function]] over \\([a,b]\\subset \\mathbb{R}\\). Specifically, we consider a function \\(f\\colon D\\subseteq \\mathbb{R}^{2}\\rightarrow \\mathbb{R}\\) and a set \\(\\Omega\\subseteq D\\) on which \\(f&gt;0\\), and introduce the set \\[\nS_{\\Omega}^{f}=\\left\\{(x,y,z)\\in\\mathbb{R}^{3}\\;|\\quad (x,y)\\in\\Omega,\\;z=f(x,y)\\right\\} .\n\\] The double integral of \\(f\\) over \\(\\Omega\\) is built in order to give a rigorous definition of the volume of the set \\(S_{\\Omega}^{f}\\) in such a way that, among other things, it coincides with the area of \\(\\Omega\\) when \\(f(x,y)=1\\) on \\(\\Omega\\).\nOf course, the existence of the integral depends on both \\(\\Omega\\) and \\(f\\). In particular, unlike the one-dimensional case, we must be careful in choosing the region \\(\\Omega\\) of integration.\nWhen it exists, the integral of \\(f\\) over \\(\\Omega\\) is denoted as \\[\n\\mathrm{V}(S_{\\Omega}^{f})=\\iint_{\\Omega} f\\;\\mathrm{d}A.\n\\] It is then possible to extend the notion of double integral also to functions admitting negative values at the price of breaking the physical interpretation in terms of volume, or allowing for a different understanding of volumes admitting also negative values.\nBecause of the little time at our disposal, and because of the difficulty of the argument, we do not discuss how to explicitly define the double integral of \\(f\\) on \\(\\Omega\\), and simply jump to how to compute it in some quite useful particular cases. More details can be found in Paul’s Online Notes, or in chapter 15 of the Mathematics LibreText, or in chapter 5 of M-T.\nIt turns out we can evaluate \\(\\mathrm{V}(S_{\\Omega}^{f})\\) for subsets that are known as elementary regions.\n\n[!defn] Definition: elementary region of the plane Let \\(\\phi_{1},\\phi_{2}\\colon[a,b]\\rightarrow\\mathbb{R}\\) be continuous and such that \\(\\phi_{1}(x)\\leq\\phi_{2}(x)\\) on \\([a,b]\\). The set \\[\nD:= \\left\\{ (x,y)\\in\\mathbb{R}^{2}\\;|\\quad a\\leq x\\leq b,\\;\\phi_{1}(x)\\leq y\\leq \\phi_{2}(x)\\right\\}\n\\] is called a \\(y\\)-simple region of the plane \\(\\mathbb{R}^{2}\\). Note that \\(D\\) is necessarily closed and bounded (being all the functions involved continuous on a closed and bounded interval) and its boundary \\(\\partial D\\) is the union of the graphs of at most 4 continuous functions. Specifically, we have \\[\n\\partial D= \\partial D_{1}^{x} \\cup \\partial D_{2}^{x} \\cup \\partial D_{1}^{y}\\cup \\partial D_{2}^{y}\n\\] with \\[\n\\begin{split}\n\\partial D_{1}^{x}&=\\left\\{(x,y)\\in\\mathbb{R}^{2}\\;|\\quad a\\leq x\\leq b\\;, y=\\phi_{1}(x)\\right\\}=\\mathrm{graph}(\\phi_{1}) \\\\\n\\partial D_{2}^{x}&=\\left\\{(x,y)\\in\\mathbb{R}^{2}\\;|\\quad a\\leq x\\leq b\\;, y=\\phi_{2}(x)\\right\\} =\\mathrm{graph}(\\phi_{2}) \\\\\n\\partial D_{1}^{y}&=\\left\\{(x,y)\\in\\mathbb{R}^{2}\\;|\\quad x=a,\\;c\\leq y \\leq d\\right\\}  \\\\\n\\partial D_{2}^{y}&=\\left\\{(x,y)\\in\\mathbb{R}^{2}\\;|\\quad x=b,\\; c\\leq y\\leq d\\right\\}  .\n\\end{split}\n\\] Three typical examples are pictured below (source):\n![[y_simple_region.jpg]]\nLet \\(\\psi_{1},\\psi_{2}\\colon[c,d]\\rightarrow\\mathbb{R}\\) be continuous and such that \\(\\psi_{1}(y)\\leq\\psi_{2}(y)\\) on \\([c,d]\\). The set \\[\nD:=\\left\\{(x,y)\\in\\mathbb{R}^{2}\\;|\\quad c\\leq y\\leq d,\\;\\psi_{1}(y)\\leq x\\leq \\psi_{2}(y)\\right\\}\n\\] is called an \\(x\\)-simple region of the plane \\(\\mathbb{R}^{2}\\). Note that \\(D\\) is necessarily closed and bounded (being all the functions involved continuous on a closed and bounded interval) and its boundary \\(\\partial D\\) is the union of the graphs of at most 4 continuous functions. Specifically, we have \\[\n\\partial D= \\partial D_{1}^{x} \\cup \\partial D_{2}^{x} \\cup \\partial D_{1}^{y}\\cup \\partial D_{2}^{y}\n\\]\nwith \\[\n\\begin{split}\n\\partial D_{1}^{y}&=\\left\\{(x,y)\\in\\mathbb{R}^{2}\\;|\\quad c\\leq y\\leq d\\;, x=\\psi_{1}(y)\\right\\}=\\mathrm{graph}(\\psi_{1}) \\\\\n\\partial D_{2}^{y}&=\\left\\{(x,y)\\in\\mathbb{R}^{2}\\;|\\quad c\\leq y\\leq d\\;, x=\\psi_{2}(y)\\right\\} =\\mathrm{graph}(\\psi_{2}) \\\\\n\\partial D_{1}^{x}&=\\left\\{(x,y)\\in\\mathbb{R}^{2}\\;|\\quad y=c\\;, a\\leq x\\leq b\\right\\}  \\\\\n\\partial D_{2}^{x}&=\\left\\{(x,y)\\in\\mathbb{R}^{2}\\;|\\quad   y = d\\;, a\\leq x\\leq b\\right\\}  .\n\\end{split}\n\\] Three typical examples are pictured below (source):\n![[x_simple_region.jpg]]\nA set \\(D\\) which is either \\(x\\)-simple or \\(y\\)-simple is referred to as a elementary region of the plane \\(\\mathbb{R}^{2}\\).\n\n^c48dd6 # Double integrals over rectangles\nThe first case we consider is the case in which \\(\\Omega\\) is a rectangle of the form \\([a,b]\\times[c,d]\\subset\\mathbb{R}^{2}\\) and \\(f\\) is continuous and positive on \\(\\Omega\\). In this case, we also write \\(R\\) instead of \\(\\Omega\\).\nIntuitively speaking, \\(\\mathrm{V}(S_{R}^{f})\\) is computed as the limit of its approximation from below and from above in analogy with the [[3.2 Integrals#^964215|one-dimensional case]]. ^050619\nThe following animation allows for a visualization of the approximation from below (source):\n![[volume_as_double_integral.gif]]\nOf course, the integral exists when both the approximation from below and from above converge to the same limit. In this case, we write \\[\n\\mathrm{V}(S_{R}^{f})=\\iint_{R} f \\,\\mathrm{d}A\n\\] as anticipated above.\nWithout discussing technical details, we write down a magnificent theorem that helps us in computing \\(\\mathrm{V}(S_{R}^{f})\\) for an interesting class of functions.\n\n[!important] Theorem: Fubini’s theorem If \\(f\\colon R\\subset\\mathbb{R}^{2}\\rightarrow \\mathbb{R}\\) is bounded and continuous except possibly on the boundary of an elementary region \\(D\\subseteq R\\), then \\(\\mathrm{V}(S_{R}^{f})\\) exists. Moreover, if the integral \\[\nF_{[c,d]}(x)= \\int_{c}^{d}f(x,y)\\;\\mathrm{d}y\n\\] exists for all \\(x\\in[a,b]\\), \\(\\mathrm{V}(S_{R}^{f})\\) is equal to the following iterated integral: \\[\n\\mathrm{V}(S_{R}^{f})=\\iint_{R}f\\;\\mathrm{d}A=\\int_{a}^{b}F_{[c,d]}(x)\\;\\mathrm{d}x.\n\\] Similarly, if the integral \\[\nF_{[a,b]}(y)= \\int_{a}^{b}f(x,y)\\;\\mathrm{d}x\n\\] exists for all \\(y\\in [c,d]\\), \\(\\mathrm{V}(S_{R}^{f})\\) is equal to the following iterated integral: \\[\n\\mathrm{V}(S_{R}^{f})=\\int_{c}^{d}F_{[a,b]}(y)\\;\\mathrm{d}y.\n\\] If both previous assumptions hold simultaneously (for instance when \\(f\\) is continuous on the whole \\(R\\)), then \\[\n\\mathrm{V}(S_{R}^{f})=\\iint_{R}f\\;\\mathrm{d}A=\\int_{a}^{b}F_{[c,d]}(x)\\;\\mathrm{d}x=\\int_{c}^{d}F_{[a,b]}(y)\\;\\mathrm{d}y\n\\] and we are free to choose the one we prefer for the sake of computations. ^1d3dd7\n\nThe following animation allows for a visualization of the computations associated with Fubini’s theorem (source):\n![[double_integration_process_general.gif]]\n\n[!exmp] Example Let \\(R=[-2,1]\\times [0,1]\\) and \\(f\\colon R\\rightarrow\\mathbb{R}\\) be given by \\(f(x,y)=y(x^{3} - 12x)\\). Since the hypothesis of Fubini’s theorem are satisfied, we can write\n\\[\n\\begin{split}\n\\iint_{R} f(x,y)&=\\int_{-2}^{1}\\left(\\int_{0}^{1} y(x^{3} - 12x)\\;\\mathrm{d}y\\right)\\;\\mathrm{d}x= \\\\\n&= \\int_{-2}^{1}\\left[\\frac{y^{2}}{2}\\right]_{0}^{1}\\,(x^{3} - 12x)\\;\\mathrm{d}x= \\\\\n&= \\frac{1}{2}\\int_{-2}^{1}\\,(x^{3} - 12x)\\;\\mathrm{d}x =\\\\\n&= \\frac{1}{2}\\,\\left[\\frac{x^{4}}{4} - 6x^{2}\\right]_{-2}^{1}=\\frac{57}{8} .\n\\end{split}\n\\]\nObviously, if we invert the order of the integrals and perform the integration with respect to \\(x\\) first, the result does not change.\n\n\n\nDouble integrals over elementary regions\nThe definition of double integrals over elementary regions can be easily accommodated exploiting Fubini’s theorem recalled above.\nAssume \\(D\\) is a elementary region strictly contained in the rectangle \\(R=[a,b]\\times [c,d]\\), and assume \\(f\\colon D\\subset\\mathbb{R}^{2}\\rightarrow\\mathbb{R}\\) is continuous. We extend \\(f\\) from \\(D\\) to \\(R\\) setting \\(f(x,y)=0\\) for every \\((x,y)\\in R\\) but not in \\(D\\).\nThis almost trivial extension of \\(f\\) (denoted again by \\(f\\) with an evident abuse of notation) turns out to be continuous on \\(R\\) except possibly on the boundary of the elementary region \\(D\\). Therefore, the hypothesis of Fubini’s theorem recalled above apply to the (extension of) \\(f\\) and we set \\[\n\\mathrm{V}(S_{D}^{f})\\equiv\\iint_{D} f\\;\\mathrm{d}A := \\iint_{R}f\\;\\mathrm{d}A=\\mathrm{V}(S_{R}^{f}).\n\\]\n\n[!rem] Remark When \\(f(x,y)=1\\) for every \\((x,y)\\in D\\), the double integral of \\(f\\) over \\(D\\), when it exists, coincides with the area of \\(D\\).\n\nThe explicit computation of \\(\\mathrm{V}(S_{D}^{f})\\) can be performed as follows. First of all, suppose \\(D\\) is \\(y\\)-simple. The integral \\[\nF_{[c,d]}(x)=\\int_{c}^{d}f(x,y)\\;\\mathrm{d}y\n\\] exists because \\(f(x,y)\\), seen as a function of \\(y\\), is continuous in \\([c,d]\\) except at a finite number of points (recall the proposition on the [[3.2 Integrals#^54f5d8|interval additivity]] of the one-dimensional integral). Moreover, \\(f(x,y)=0\\) for all \\(c\\leq y&lt;\\phi_{1}(x)\\) and for all \\(\\phi_{2}(x)&lt;y\\leq d\\), so that it holds \\[\nF_{[c,d]}(x)=\\underbrace{\\int_{c}^{\\phi_{1}(x)}f(x,y)\\;\\mathrm{d}y}_{=0} + \\int_{\\phi_{1}(x)}^{\\phi_{2}(x)}f(x,y)\\;\\mathrm{d}y + \\overbrace{\\int_{\\phi_{2}(x)}^{x}f(x,y)\\;\\mathrm{d}y}^{=0}= \\int_{\\phi_{1}(x)}^{\\phi_{2}(x)}f(x,y)\\;\\mathrm{d}y .\n\\] Consequently, we have \\[\n\\mathrm{V}(S_{D}^{f})\\equiv\\iint_{D} f\\;\\mathrm{d}A := \\iint_{R}f\\;\\mathrm{d}A=\\int_{a}^{b}F_{[c,d]}(x)\\,\\mathrm{d}x=\\int_{a}^{b}\\left(\\int_{\\phi_{1}(x)}^{\\phi_{2}(x)}f(x,y)\\;\\mathrm{d}y\\right)\\;\\mathrm{dx}\n\\] because of Fubini’s theorem above.\n\n[!exmp] Example Let us compute the double integral \\[\n\\iint_{T}(x^{3}y + \\cos(x))\\,\\mathrm{d}A\n\\] where \\(T\\) is the triangular region with vertices \\((0,0)\\), \\((\\pi/2,\\pi/2)\\), and \\((\\pi/2,0)\\). First of all, following the sketch (source):\n![[triangle.jpg]]\nWe write \\(T\\) as a \\(y\\)-simple region according to \\[\nT:=\\left\\{(x,y)\\in\\mathbb{R}^{2}\\;|\\quad 0\\leq x\\leq \\frac{\\pi}{2},\\quad 0\\leq y\\leq x \\right\\}.\n\\] Consequently, we have \\[\n\\begin{split}\n\\iint_{T}(x^{3}y + \\cos(x))\\,\\mathrm{d}A&=\\int_{0}^{\\frac{\\pi}{2}}\\int_{0}^{x}(x^{3}y + \\cos(x))\\;\\mathrm{d}y\\,\\mathrm{d}x=\\int_{0}^{\\frac{\\pi}{2}} \\left[\\frac{x^{3}y^{2}}{2} + y\\cos(x)\\right]_{0}^{x}\\;\\mathrm{d}x = \\\\ & \\\\\n&= \\int_{0}^{\\frac{\\pi}{2}}\\left(\\frac{x^{5}}{2} + x\\cos(x)\\right)\\,\\mathrm{d}x =\\left[\\frac{x^{6}}{2} + x\\sin(x) + \\cos(x)\\right]^{\\frac{\\pi}{2}}_{0} = \\\\ & \\\\ &=\\frac{\\pi^{6}}{2^{7}} + \\frac{\\pi}{2} - 1\n\\end{split}\n\\]\n\nOf course, if \\(D\\) is an \\(x\\)-simple region, a similar chain of reasoning leads us to \\[\n\\mathrm{V}(S_{D}^{f})\\equiv\\iint_{D} f\\;\\mathrm{d}A := \\iint_{R}f\\;\\mathrm{d}A=\\int_{c}^{d}F_{[a,b]}(y)\\,\\mathrm{d}y=\\int_{c}^{d}\\left(\\int_{\\psi_{1}(y)}^{\\psi_{2}(y)}f(x,y)\\;\\mathrm{d}x\\right)\\;\\mathrm{dy} ,\n\\] while if \\(D\\) is both \\(x\\)-simple and \\(y\\)-simple it holds \\[\n\\mathrm{V}(S_{D}^{f})\\equiv\\iint_{D} f\\;\\mathrm{d}A =\\int_{a}^{b}\\left(\\int_{\\phi_{1}(x)}^{\\phi_{2}(x)}f(x,y)\\;\\mathrm{d}y\\right)\\;\\mathrm{dx}=\\int_{c}^{d}\\left(\\int_{\\psi_{1}(y)}^{\\psi_{2}(y)}f(x,y)\\;\\mathrm{d}x\\right)\\;\\mathrm{dy} .\n\\] It turns out that being able to exchange the role of integrals could be quite effective in reducing computational difficulties.\n\n[!important] Proposition: Linearity of the double integral Let \\(D\\) be an elementary region in \\(\\mathbb{R}^{2}\\) and let \\(f,g\\) be continuous on \\(D\\), except at most on its boundary. Then it holds \\[\n\\iint_{D}(f+g)\\;\\mathrm{d}A=\\iint_{D}f\\;\\mathrm{d}A + \\iint_{D}g\\;\\mathrm{d}A,\n\\] that is, the double integral is linear.\n\n\n[!important] Proposition: additivity of the double integral Let \\(D\\) be an elementary region in \\(\\mathbb{R}^{2}\\) that can be decomposed as the union \\(D=D_{1} \\cup D_{2}\\) of two elementary regions \\(D_{1},D_{2}\\) such that \\(D_{1}\\cap D_{2}\\) is the image of a continuous function \\(\\gamma\\colon I\\subset \\mathbb{R}\\rightarrow \\mathbb{R}\\), i.e., a curve. Then, it holds \\[\n\\iint_{D}f\\;\\mathrm{d}A=\\iint_{D_{1}} f\\;\\mathrm{d}A + \\iint_{D_{2}}f\\;\\mathrm{d}A\n\\] for every \\(f\\) continuous on \\(D\\), except at most on its boundary.\n\n\n[!exmp] Example Let us consider the double integral \\[\n\\begin{split}\n\\int_{0}^{2}\\int_{\\frac{y}{4}}^{5}(x^2 + y^2 )\\;\\mathrm{d}x\\,\\mathrm{d}y &= \\int_{0}^{2}\\left[\\frac{x^3}{3} + xy^2\\right]_{\\frac{y}{4}}^{5}\\;\\mathrm{d}y = \\int_{0}^{2}\\left(\\frac{5^3}{3} + 5y^2 - \\frac{y^{3}}{4^{3}\\,3} - \\frac{y^3}{4}\\right) \\;\\mathrm{d}y  = \\\\ & \\\\ & = \\frac{5^3 \\, 2}{3} + \\frac{20}{3} - \\frac{1}{ 48} - 1 =\\frac{4591}{48}  ,\n\\end{split}\n\\] and let us perform an exchange of the order of integration. At this purpose, we have to write the \\(x\\)-simple region \\[\nD=\\left\\{(x,y)\\in\\mathbb{R}^{2}\\,|\\qquad 0\\leq y \\leq 2,\\quad \\frac{y}{4}\\leq x \\leq 5\\right\\}\n\\] as a \\(y\\)-simple region. A direct look at the sketch of \\(D\\) reveals that it is better to write it as the union \\(D=T \\cup R\\) of the triangle \\[\nT=\\left\\{(x,y)\\in\\mathbb{R}^{2}\\,|\\qquad 0\\leq x \\leq \\frac{1}{2},\\quad 0\\leq y\\leq 4x \\right\\}\n\\] with the rectangle \\(R=[\\frac{1}{2},5]\\times [0,2]\\). Therefore, we obtain \\[\n\\begin{split}\n\\int_{0}^{2}\\int_{\\frac{y}{4}}^{5}(x^2 + y^2 )\\;\\mathrm{d}x\\,\\mathrm{d}y &= \\int_{0}^{\\frac{1}{2}}\\int_{0}^{4x}(x^2 + y^2 )\\;\\mathrm{d}y\\,\\mathrm{d}x +  \\int_{2}^{5}\\int_{0}^{2}(x^2 + y^2 )\\;\\mathrm{d}y\\,\\mathrm{d}x = \\\\ & \\\\\n& = \\int_{0}^{\\frac{1}{2}}\\left[yx^2 + \\frac{y^3}{3}\\right]_{0}^{4x}\\;\\mathrm{d}x +  \\int_{\\frac{1}{2}}^{5}\\left[yx^2 + \\frac{y^3}{3}\\right]_{0}^{2}\\;\\mathrm{d}x = \\\\ & \\\\\n& = \\int_{0}^{\\frac{1}{2}}4x^3 + \\frac{(4x)^3}{3} \\;\\mathrm{d}x +  \\int_{\\frac{1}{2}}^{5} 2x^2 + \\frac{2^3}{3} \\;\\mathrm{d}x = \\\\ & \\\\\n& = \\left[x^4 + \\frac{4^{2} x^4}{3}\\right]_{0}^{\\frac{1}{2}} +  \\left[\\frac{2}{3}x^3 + \\frac{2^3}{3}x \\right]_{\\frac{1}{2}}^{5} = \\\\ & \\\\\n& = \\frac{19}{48} + \\left(\\frac{2\\cdot 5^{3}}{3}+\\frac{2^{3}\\cdot 5}{3} - \\frac{1}{12} - \\frac{4}{3}\\right)= \\frac{19}{48} + \\frac{1143}{12}= \\frac{4591}{48}\n\\end{split}\n\\] as it should be.\n\n\n\nDouble integrals and change of variables\nSometimes, it could be useful to transform a double integral through an appropriate change of variables.\nIn this case, it turns out that the appropriate way to transform the double integral is through the use of the determinant of the matrix of the partial derivatives of the function implementing the change of variables.\n\n[!important] Proposition Let \\(\\overline{D^{*}},\\overline{D}\\subseteq\\mathbb{R}^{2}\\) be elementary regions, and let \\(D^{*},D\\) denote their open interiors. Let \\(T\\colon \\overline{D^{*}}\\rightarrow\\mathbb{R}^{2}\\) be a one-to-one continuous map which is \\(C^{1}\\) on \\(D^{*}\\), and is such that \\(T(D^{*})=D\\). Let \\(f\\colon \\overline{D}\\rightarrow \\mathbb{R}\\) be continuous so that its double integral over \\(D\\) exists. Then it holds \\[\n\\iint_{\\overline{D}}f(x,y)\\,\\mathrm{d}A = \\iint_{\\overline{D^{*}}}f\\circ T(u,v)\\,|DT(u,v)|\\;\\mathrm{d}A,\n\\] where \\(|DT(u,v)|\\) is the determinant of the matrix of partial derivatives of \\(T\\) at the point \\((u,v)\\).\n\n\n[!exmp] Example Consider the double integral \\[\n\\iint_{\\overline{D}}\\ln(x^2 + y^2)\\;\\mathrm{d}A\n\\] where the elementary region \\(\\overline{D}\\) is the region in the first quadrant lying between the arcs of the circles \\(x^2 + y^2 = a^2\\) and \\(x^2 + y^2 = b^2\\), where \\(0 &lt; a &lt; b\\). To evaluate the integral we would have to write \\(\\overline{D}\\) as a \\(y\\)-simple or \\(x\\)-simple region. However, we also note that the symmetry of the problem is actually suggesting to use polar coordinates. Indeed, the equation of the two arcs of circles would then be \\(r^2=a^2\\) and \\(r^2=b^2\\), and the fact that \\(\\overline{D}\\) lies in the first quadrant can be implemented constraining \\(\\theta\\in[0,\\pi/2]\\). Consequently, the region \\(\\overline{D}\\) becomes the rectangle \\(R=[a,b]\\times[0,\\pi/2]\\). To be more precise, we define the map \\(T\\colon  R\\times \\mathbb{R}^{2}\\) given by \\[\nT(r,\\theta):=\\left(\\begin{matrix}r\\cos(\\theta) \\\\ r\\sin(\\theta)\\end{matrix}\\right).\n\\] It is then a matter of direct computation to check that \\(T(R)=\\overline{D}\\) and that \\[\nDT=\\left(\\begin{matrix} \\cos(\\theta) & -r\\sin(\\theta) \\\\ \\sin(\\theta)  & r\\cos(\\theta)\\end{matrix}\\right)\\;\\Rightarrow \\; |DT|=r ,\n\\] so that the change of variable formula above implies \\[\n\\begin{split}\n\\iint_{\\overline{D}}\\ln(x^2 + y^2)\\;\\mathrm{d}A &=\\iint_{R}r\\ln(r)\\;\\mathrm{d}A=\\int_{a}^{b}\\int_{0}^{\\frac{\\pi}{2}}r\\ln(r^2)\\;\\mathrm{d}\\theta \\,\\mathrm{d}r= \\\\\n& =\\frac{\\pi}{2}\\int_{a}^{b} r\\ln(r^2)\\; \\mathrm{d}r =\\pi\\int_{a}^{b} r\\ln(r)\\; \\mathrm{d}r .\n\\end{split}\n\\] The last integral can be computed using [[3.1 Antiderivatives#Integration by parts|integration by parts]] to get \\[\n\\iint_{\\overline{D}}\\ln(x^2 + y^2)\\;\\mathrm{d}A =\\frac{\\pi}{2}\\left(b^2\\,\\ln(b) -a^2\\,\\ln(a) - \\frac{1}{2}(b^2 - a^2)\\right).\n\\]",
    "crumbs": [
      "Home",
      "Teaching",
      "Calculus - Robotics Engineering - UC3M",
      "Double integrals"
    ]
  },
  {
    "objectID": "teaching/calculus-uc3m-robotics/3.5 Exercises.html",
    "href": "teaching/calculus-uc3m-robotics/3.5 Exercises.html",
    "title": "Anti-derivatives",
    "section": "",
    "text": "Anti-derivatives\n\nUse [[3.1 Antiderivatives#^b5075f|integration by parts]] to compute the anti-derivatives of the following functions \\[ \\begin{split} 1) \\qquad & f(x) = x^{2}\\,(x +1)^{9} \\\\ & \\\\ 2) \\qquad & f(x) = x^{2}(2x - 1)^{-7} \\\\ & \\\\ 3) \\qquad & f(x) = \\ln(x) \\\\ & \\\\ 4)  \\qquad & f(x) = x\\ln(x) \\\\ & \\\\ 5)  \\qquad & f(x) = (\\ln(x))^{2} \\\\ & \\\\ 6)  \\qquad & f(x) = \\arcsin(x) \\\\ & \\\\ 7)  \\qquad & f(x) = \\arccos(x) \\\\ & \\\\ 8)  \\qquad & f(x) = \\arctan(x) \\\\ & \\\\ 9)  \\qquad & f(x) = \\mathrm{e}^{x}\\,\\sin(x)\\\\ & \\\\ 10)  \\qquad & f(x) = x^{3} \\,\\sin(x^{2})\\\\ & \\\\ 11)  \\qquad & f(x) = x\\,\\ln(x+1)\\\\ & \\\\ 12)  \\qquad & f(x) = x\\,\\sqrt{x+1}\\\\ & \\\\ 13) \\qquad & f(x) = \\cos(\\ln(x))\\qquad \\mbox{Hint: integrate by parts twice} \\\\ & \\\\ 14) \\qquad & f(x) = \\frac{1}{x}\\arcsin(\\ln(x)) \\\\ & \\\\ 15) \\qquad & f(x) = x\\,\\arctan(x^{2}) \\\\ & \\\\ \\end{split} \\]\nIf \\(f\\colon[a,b]\\rightarrow \\mathbb{R}\\) is continuous and invertible in \\((a,b)\\) with inverse \\(f^{-1}\\) and primitive \\(F\\), use [[3.1 Antiderivatives#^b5075f|integration by parts]] to prove that \\[ \\int f^{-1}(x)\\,\\mathrm{d}x =x f^{-1}(x) - F(f^{-1}(x)) + c .\\]\nUse the [[3.1 Antiderivatives#^93db8a|a suitable change of variable]] to compute the anti-derivatives of the following functions \\[ \\begin{split} 1) \\qquad & f(x) = \\frac{1 + \\sqrt{1-\\sqrt{x}}}{\\sqrt{x}}  \\\\ & \\\\ 2) \\qquad & f(x) =  x^{2}\\,\\sin(\\sqrt{x^{3}}) \\\\ & \\\\ 3) \\qquad & f(x) =   \\cos(\\ln(x)) \\\\ & \\\\ 4) \\qquad & f(x) =   \\frac{\\sqrt{x}+1}{x+3} \\\\ & \\\\ 5) \\qquad & f(x) =  \\frac{x^{3}}{(1+x^{2})^{3}} \\\\ & \\\\ 6) \\qquad & f(x) =   \\frac{1}{(x+2)\\sqrt{1+x}} \\\\ & \\\\ 7) \\qquad & f(x) =   \\frac{\\mathrm{e}^{4x}}{\\mathrm{e}^{2x} + 2\\mathrm{e}^{x} + 2} \\\\ & \\\\ 8) \\qquad & f(x) =   \\frac{1}{\\sqrt{\\mathrm{e}^{2x} - 1}}\\\\ & \\\\ 9) \\qquad & f(x) =   \\frac{\\sin^{2}(x)\\,\\cos^{5}(x)}{\\tan^{3}(x)}\\\\ & \\\\ 10) \\qquad & f(x) =  \\frac{\\sin(x) + 3\\cos(x)}{\\sin(x) + 2\\cos(x)} \\\\ & \\\\ 11) \\qquad & f(x) =   \\frac{\\sin(x) + 3\\cos(x)}{\\sin(x)\\cos(x) + \\sin(x)}\\\\ & \\\\ 12) \\qquad & f(x) = \\sqrt{\\sqrt{x} + 1}  \\\\ & \\\\ 13) \\qquad & f(x) = \\frac{1}{\\mathrm{e}^{x} - 4\\mathrm{e}^{-x}} \\\\ & \\\\ 14) \\qquad & f(x) = \\frac{1}{x^{2}\\sqrt{9-x^{2}}}  \\\\ & \\\\ \\end{split} \\] &gt;[!note]- Solution: &gt;1) Use \\(t=\\sqrt{1-\\sqrt{x}}\\). &gt;2) Use \\(t=x^{\\frac{3}{2}}\\). &gt;3) Use \\(t=\\ln(x)\\) and integrate by part twice. &gt;4) Use \\(t=\\sqrt{x} +1\\) and long division of polynomials. &gt;5) Use \\(t=1+x^2\\). &gt;6) Use \\(t=\\sqrt{1+x}\\). &gt;7) Use \\(t=\\mathrm{e}^x\\) and long division of polynomials. &gt;8) Use \\(t=\\sqrt{\\mathrm{e}^{2x}-1}\\). &gt;9) Use \\(t=\\cos(x)\\). &gt;10) Use &gt;11) Use \\(t=\\tan(x/2)\\), so that \\(\\sin(x)=2t/(1+t^2)\\) and \\(\\cos(x)=(1-t)/(1+t^2)\\). &gt;12) Use \\(t=\\sqrt{\\sqrt{x}+1}\\). &gt;13) Use \\(t=\\mathrm{e}^x\\). &gt;14) Use first \\(y=x/3\\) and then \\(y=\\sin(t)\\).\n\n\n\nDouble integrals\n\nCompute the double integral \\[ \\iint_{D}y^{2}\\,\\mathrm{d}A\\] where \\(D\\) is the triangular region with vertices \\((0,2)\\), \\((2,4)\\), and \\((4,2)\\).\nCompute the double integral \\[ \\iint_{D}\\mathrm{e}^{x-y}\\,\\mathrm{d}A \\] where \\(D\\) is the triangular region with vertices \\((0,0)\\), \\((1,3)\\), and \\((2,2)\\).\nCompute the following double integrals and sketch the regions of integration: \\[ \\begin{split} 1) \\qquad & \\int_{0}^{2}\\int_{\\frac{x}{8}}^{\\sqrt[3]{x}}\\, \\;\\mathrm{d}y\\,\\mathrm{d}x & \\\\ 2) \\qquad & \\int_{1}^{2}\\int_{\\ln(x)}^{\\mathrm{e}^{x}}\\, \\;\\mathrm{d}y\\,\\mathrm{d}x & \\\\ 3) \\qquad & \\int_{0}^{2}\\int_{2}^{3}\\, (x + \\mathrm{e}^{-y})\\;\\mathrm{d}x\\,\\mathrm{d}y & \\\\ 4)  \\qquad & \\int_{1}^{3}\\int_{\\frac{\\pi}{2}}^{\\pi}\\,y\\sin(x) \\;\\mathrm{d}x\\,\\mathrm{d}y & \\\\ 5)  \\qquad & \\int_{2}^{3}\\int_{1}^{4}\\,\\left(\\frac{x}{y} + \\frac{y}{x}\\right) \\;\\mathrm{d}y\\,\\mathrm{d}x & \\\\ 6)  \\qquad & \\int_{0}^{\\frac{\\pi}{2}}\\int_{0}^{y}\\, y\\sin(x)\\;\\mathrm{d}x\\,\\mathrm{d}y & \\\\ 7)  \\qquad & \\int_{0}^{1}\\int_{0}^{x}\\,\\sqrt{1-x^{2}} \\;\\mathrm{d}y\\,\\mathrm{d}x & \\\\ 8)  \\qquad & \\int_{0}^{3}\\int_{0}^{x^{2}}\\,y^{2}x \\;\\mathrm{d}y\\,\\mathrm{d}x & \\\\ 9)  \\qquad & \\int_{0}^{1}\\int_{x}^{1}\\,xy \\;\\mathrm{d}y\\,\\mathrm{d}x & \\\\ 10)  \\qquad & \\int_{0}^{\\frac{\\pi}{2}}\\int_{0}^{\\cos(x)}\\,\\cos(x) \\;\\mathrm{d}y\\,\\mathrm{d}x & \\\\ 11)  \\qquad & \\int_{-1}^{1}\\int_{|y|}^{1}\\,(x+y)^{2} \\;\\mathrm{d}x\\,\\mathrm{d}y & \\\\ 12)  \\qquad & \\int_{-3}^{1}\\int_{-\\sqrt{9-y^{2}}}^{\\sqrt{9-y^{2}}}\\, x^{2}\\;\\mathrm{d}x\\,\\mathrm{d}y & \\\\ 13) \\qquad & \\int_{-1}^{1}\\int_{-2|x|}^{|x|}\\,\\mathrm{e}^{x+y} \\;\\mathrm{d}y\\,\\mathrm{d}x & \\\\ 14) \\qquad & \\int_{0}^{1}\\int_{2-y}^{1}\\,(x+y)^{2} \\;\\mathrm{d}x\\,\\mathrm{d}y & \\\\ 15) \\qquad & \\int_{0}^{1}\\int_{1}^{\\mathrm{e}^{x}}\\,x + y \\;\\mathrm{d}y\\,\\mathrm{d}x \\end{split} \\]\nCompute the following integrals by changing the order of integration: \\[ \\begin{split} 1) \\qquad & \\int_{0}^{a }\\int_{0}^{\\sqrt{a^{2} - x^{2}}}\\sqrt{a^{2} - y^{2}}\\, \\;\\mathrm{d}y\\,\\mathrm{d}x & \\\\ 2)  \\qquad & \\int_{1}^{2}\\int_{0}^{\\ln(x)}\\,(x-1)\\,\\sqrt{1+ \\mathrm{e}^{2y}} \\;\\mathrm{d}y\\,\\mathrm{d}x & \\\\ 3)  \\qquad & \\int_{0}^{8}\\int_{\\frac{y}{2}}^{4}\\, \\;\\mathrm{d}x\\,\\mathrm{d}y & \\\\ 4)  \\qquad & \\int_{0}^{1}\\int_{x}^{1}\\,xy \\;\\mathrm{d}y\\,\\mathrm{d}x & \\\\ 5)  \\qquad & \\int_{-3}^{1}\\int_{-\\sqrt{9-y^{2}}}^{\\sqrt{9-y^{2}}}x^{2}\\, \\;\\mathrm{d}x\\,\\mathrm{d}y & \\\\ 6)  \\qquad & \\int_{0}^{4}\\int_{\\frac{y}{2}}^{2}\\,\\mathrm{e}^{x^{2}} \\;\\mathrm{d}x\\,\\mathrm{d}y & \\\\ 7)  \\qquad & \\int_{0}^{3}\\int_{\\sqrt[3]{x}}^{27}\\,xy \\;\\mathrm{d}y\\,\\mathrm{d}x \\end{split} \\] &gt;[!note]- Solution: &gt;1) &gt;2) The region of integration reads \\(D=\\{(x,y)\\in\\mathbb{R}^{2}|1\\leq x\\leq 2, 0\\leq y\\leq \\ln(x)\\}\\). As such, it is written as a \\(y\\)-simple region, but we can turn it into an \\(x\\)-simple region writing \\(D=\\{(x,y)\\in\\mathbb{R}^{2}|0\\leq y\\leq \\ln(2),\\mathrm{e}^{y}\\leq x\\leq 2\\}\\). Next, we perform the change of variable \\(\\mathrm{e}^{y}=t\\), which means \\(\\mathrm{e}^{y}\\mathrm{d}y=\\mathrm{d}t\\) and thus &gt; \\[\\begin{split} \\int_{1}^{2}\\int_{0}^{\\ln(x)}(x-1)\\sqrt{1+\\mathrm{e}^{2y}}\\,\\mathrm{d}y\\mathrm{d}x&=\\int_{0}^{\\ln(2)}\\int_{\\mathrm{e}^{y}}^{2}(x-1)\\sqrt{1+\\mathrm{e}^{2y}}\\,\\mathrm{d}x\\mathrm{d}y=\\\\ &= \\int_{0}^{\\ln(2)}\\mathrm{e}^{y}\\left(1-\\frac{\\mathrm{e}^{y}}{2}\\right)\\sqrt{1+\\mathrm{e}^{2y}}\\,\\mathrm{d}y= \\\\ &=\\int_{1}^{2}\\left(1-\\frac{t}{2}\\right)\\sqrt{1+t^{2}}\\,\\mathrm{d}t=\\\\ & =\\int_{1}^{2} \\sqrt{1+t^{2}}\\,\\mathrm{d}t-\\int_{1}^{2}\\frac{t}{2}\\sqrt{1+t^{2}}\\,\\mathrm{d}t.\\end{split}\\] &gt; The second integral is immediate because \\(2t\\) is the derivative of \\(1+t^{2}\\), and the result is thus \\(\\frac{1}{6}(2^{\\frac{3}{2}}-5^{\\frac{3}{2}})\\). To solve the first integral in the RHS of the previous long chain of equalities, we introduce the change of variable \\(t=\\sinh(u)\\) (look here for the definition and all the properties of hyperbolic functions we need), which leads to \\(\\mathrm{d}t=\\cosh(x)\\mathrm{d}u\\) and \\(1+\\sinh^{2}(u)=\\cosh^{2}(u)\\). Since \\(\\cosh(u)\\) is always positive, we have &gt; \\[\\begin{split}\\int_{1}^{2} \\sqrt{1+t^{2}}\\,\\mathrm{d}t&=\\int_{0}^{\\mathrm{arcsinh}(2)} \\cosh^{2}(u)\\,\\mathrm{d}u= \\\\ & =\\int_{0}^{\\mathrm{arcsinh}(2)}\\cosh(u) \\cosh(u)\\,\\mathrm{d}u= \\\\ & = \\sinh(x)\\cosh(x) - \\int_{0}^{\\mathrm{arcsinh}(2)} \\sinh^{2}(u)\\,\\mathrm{d}u=\\\\ & =\\sinh(x)\\cosh(x) - \\int_{0}^{\\mathrm{arcsinh}(2)} \\cosh^{2} (u)-1\\,\\mathrm{d}u,\\end{split}\\] &gt; which leads to &gt; \\[\\begin{split}\\int_{0}^{\\mathrm{arcsinh}(2)}\\cosh^{2}(u)\\,\\mathrm{d}u&=\\left[\\frac{sinh(u)\\cosh(u) +u}{2}\\right]_{0}^{\\mathrm{arcsinh}(2)}=\\\\ &= \\cosh(\\mathrm{arcsinh}(2))+\\frac{\\mathrm{arcsinh(2)}}{2}\\end{split}.\\] &gt; In conclusion, the original integral is \\(cosh(\\mathrm{arcsinh}(2))+\\frac{\\mathrm{arcsinh(2)}}{2}+\\frac{1}{6}(2^{\\frac{3}{2}}-5^{\\frac{3}{2}})\\). &gt;3) &gt;4) &gt;5) &gt;6) &gt;7)\n\n\n\nDouble integrals and change of variables\n\nLet \\(D\\) be the closed unit disk in \\(\\mathbb{R}^{2}\\). Compute the double integral \\[ \\iint_{D}\\mathrm{e}^{x^2 + y^{2}}\\;\\mathrm{d}A\\] using a suitable change of variables.\nLet \\(D\\) be the region in \\(\\mathbb{R}^{2}\\) delimited by the lines \\[ x+y=0,\\quad x+y=1,\\quad 2x-y=0,\\quad 2x-y=3 . \\] Compute the double integral \\[ \\iint_{D}(x^{2} + y^{2})\\;\\mathrm{d}A \\] using a suitable change of variables.\nLet \\(D\\) be the region in \\(\\mathbb{R}^2\\) delimited by \\[ x^2 + y^2 = 4,\\quad  x^2 + y^2 = 9, \\quad x^2 - y^2 = 1, \\quad x^{2 }- y^{2} = 4.\\] Compute the double integral \\[ \\iint_{D}xy\\;\\mathrm{d}A \\] using a suitable change of variable.\nConsider the region \\[ D=\\left\\lbrace (x,y)\\in \\mathbb{R}^2 \\mid x\\geq 2,\\: (x-2)^2+y^{2}\\leq 1 \\right\\rbrace . \\] Using the polar coordinates transformation with center at the point \\((1,0)\\) shows that \\(D\\) is the image of a simple region in the plane \\((r,\\theta)\\). Compute the double integral \\[ \\iint_D \\dfrac{1}{x-1}\\,dA\\,, \\] exploiting the change of variable just discussed. &gt;[!note]- Solution: &gt;The transformation reads &gt;\\[(r,\\theta)\\mapsto (x=1+r\\cos\\theta , y= r\\sin\\theta),\\] &gt; and the determinant of its jacobian is \\(r\\) like in the case of ordinary polar coordinates. In this coordinates, the inequality \\(x\\geq 2\\) becomes \\(r\\cos\\theta\\geq 1\\), while the inequality \\((x-2)^2+y^{2}\\leq 1\\) becomes \\(r^{2}\\cos^{2}(\\theta) - 2r\\cos\\theta + 1+ r^{2}\\sin^{2}(\\theta)\\leq 1\\), which means \\(r\\leq 2\\cos\\theta\\). Consequently, the region \\(D\\) is the image of the region \\(D^{*}\\) determined by \\(\\frac{1}{\\cos\\theta}\\leq r\\leq 2\\cos\\theta\\), and \\(0\\leq \\theta \\leq \\frac{\\pi}{4}\\). The inequality in \\(\\theta\\) follows from drawing the region \\(D\\) in the \\(xy\\)-plane, and recalling the geometrical interpretation of the angle theta for the generalized polar coordinates introduced above. Clearly, the region \\(D^{*}\\) is \\(r\\)-simple, and so it is an elementary region in the \\(r\\theta\\)-plane. Applying the change of variable formula, the integral becomes &gt; \\[\\iint_{D}\\frac{1}{x-1}\\mathrm{d}A=\\iint_{D^{*}}\\frac{r}{r\\cos\\theta}\\mathrm{d}A'=\\int_{0}^{\\frac{\\pi}{4}}\\int_{\\frac{1}{\\cos\\theta}}^{2\\cos\\theta}\\frac{1}{\\cos\\theta}\\mathrm{d}r\\,\\mathrm{d}\\theta=[2\\theta - \\tan\\theta]_{0}^{\\frac{\\pi}{4}}=\\frac{\\pi}{2} -1\\]\nEvaluate the double integral: \\[ I = \\int_{0}^{\\pi/2}\\int_{0}^{2\\sin(\\theta)}\\frac{r\\sqrt{1+\\sin^{2}(\\theta)}}{\\sin(\\theta)}\\,\\mathrm{d}r\\,\\mathrm{d}\\theta\\] &gt;[!note]- Solution: &gt;We first evaluate the inner integral with respect to \\(r\\) because we can treat \\(\\theta\\) as a constant for this step: &gt;\\[ \\int_{0}^{2\\sin(\\theta)}\\frac{r\\sqrt{1+\\sin^{2}(\\theta)}}{\\sin(\\theta)}\\,\\mathrm{d}r = \\frac{\\sqrt{1+\\sin^{2}(\\theta)}}{\\sin(\\theta)} \\left[ \\frac{r^{2}}{2} \\right]_{r=0}^{r=2\\sin(\\theta)}= 2\\sin(\\theta)\\,\\sqrt{1+\\sin^{2}(\\theta)},\\] &gt;where we used that \\(\\sin(\\theta)\\neq0\\) for \\(\\theta\\in(0,\\frac{\\pi}{2}\\)], and that the potential issue at \\(\\theta=0\\) where \\(\\sin(0)=0\\) is handled because the result of the inner definite integral correctly evaluates to \\(0\\) at \\(\\theta=0\\). To evaluate the outer integral &gt;\\[I = \\int_{0}^{\\frac{\\pi}{2}} 2\\sin(\\theta)\\sqrt{1+\\sin^{2}(\\theta)}\\,\\mathrm{d}\\theta,\\] &gt;we use the substitution \\(u=\\cos(\\theta)\\), from which we obtain \\(\\sin^{2}(\\theta)=1−\\cos^{2}(\\theta)=1−u^{2}\\), and \\(\\mathrm{d}u=−\\sin(\\theta)\\mathrm{d}\\theta\\), so that the change of the limits of variable leads to &gt;\\[ I = \\int_{1}^{0} 2 \\sqrt{1+(1-u^2)} (-\\mathrm{d}u)  = \\int_{1}^{0} -2 \\sqrt{2-u^{2},\\mathrm{d}u}= \\int_{0}^{1} 2 \\sqrt{2-u^2},\\mathrm{d}u.\\] &gt;This integral requires a trigonometric substitution. Let \\(u=\\sqrt{2}\\sin(\\phi)\\). Then \\(\\mathrm{d}u=\\sqrt{2}\\cos(\\phi)\\mathrm{d}\\phi\\), \\(\\phi\\in[0,\\pi/4]\\), and &gt;\\[ I = \\int_{0}^{\\pi/4} 2 \\sqrt{2(1 - \\sin^2(\\phi))} (\\sqrt{2}\\cos(\\phi))\\,\\mathrm{d}\\phi = \\int_{0}^{\\pi/4} 2 \\sqrt{2\\cos^2(\\phi)} (\\sqrt{2}\\cos(\\phi))\\,\\mathrm{d}\\phi   = \\int_{0}^{\\pi/4} 4\\cos^2(\\phi)\\,\\mathrm{d}\\phi,\\] &gt;where we used \\(1−\\sin^{2}(\\phi)=\\cos^{2}(\\phi)\\), and the fact that \\(0\\leq\\phi\\leq\\frac{\\pi}{4}\\) implies \\(\\cos(\\phi)\\geq 0\\), and \\(\\sqrt{\\cos^{2}(\\phi)}=\\cos(\\phi)\\). Using the identity \\(cos^{2}(\\phi)=\\frac{1+\\cos(\\phi)}{2}\\)​, we obtain &gt;\\[ I = \\int_{0}^{\\pi/4} 4 \\left( \\frac{1+\\cos(2\\phi)}{2} \\right)\\,\\mathrm{d}\\phi = 2 \\int_{0}^{\\pi/4} (1+\\cos(2\\phi))\\,\\mathrm{d}\\phi= 2 \\left[ \\phi + \\frac{\\sin(2\\phi)}{2} \\right]_{0}^{\\pi/4} =  \\boxed{1 + \\frac{\\pi}{2}}.\\]\n\n\n\nTriple integrals\n\nEvaluate the following triple integral \\[ \\iiint_{W} \\sin(x)\\,\\mathrm{d}V \\] where \\(W\\) is the solid given by \\(0\\leq x\\leq \\pi\\), \\(0,\\leq y\\leq 1\\), \\(0, z\\leq x\\).\nLet \\(W\\) be the region bounded by the planes \\(x=0\\), \\(y = 0\\), and \\(z=2\\), and the surface \\(z = x^{2} + y^{2}\\), and lying in the quadrant \\(x \\geq 0\\), \\(y\\geq  0\\). Compute \\[ \\iiint_{W}x\\,\\mathrm{d}V \\] in two different ways, and sketch the region \\(W\\).\nEvaluate the triple integral \\[ \\iiint_{W}\\,\\mathrm{d}V=\\int_{0}^{1}\\int_{0}^{x}\\int_{x^{2} + y^{2}}^{2}\\,\\mathrm{d}z\\,\\mathrm{d}y \\,\\mathrm{d}x, \\] sketch the region \\(W\\) and interpret the result geometrically.",
    "crumbs": [
      "Home",
      "Teaching",
      "Calculus - Robotics Engineering - UC3M",
      "Anti-derivatives"
    ]
  },
  {
    "objectID": "teaching/calculus-uc3m-robotics/1.5 Power series, trascendental functions, and Taylor series.html",
    "href": "teaching/calculus-uc3m-robotics/1.5 Power series, trascendental functions, and Taylor series.html",
    "title": "Power series",
    "section": "",
    "text": "Power series\nOnce we understand that it is possible to make sense of the sum of infinite numbers using numerical series, nothing prevents us in trying to develop similar ideas for the sum of an infinite number of polynomials, thus arriving at the notion of power series.\n\n[!defn] Definition: power series A power series is a series whose elements are polynomial functions: \\[\nP(x,x_{0})=\\sum_{n=0}^{+\\infty} \\;a_{n}\\,(x-x_{0})^{n}\n\\] where \\(x,x_{0}\\in\\mathbb{R}\\) and \\(\\{a_{n}\\}_{n\\in\\mathbb{N}}\\) is a numerical sequence.\n\nEssentially, for any fixed choice of \\(x,x_{0}\\in \\mathbb{R}\\), we obtain a numerical series. However, since \\(x\\) may vary, it makes sense to look at \\(P(x,x_{0})\\) as a kind of function (depending on the parameter \\(x_{0}\\)). This point of view turns out to be particularly useful because it leads to the introduction of new interesting functions.\nWe use the [[1.4 Sequences and series#^3a9a60|root test]] to obtain that the power series \\(P(x,x_{0})\\) is [[1.4 Sequences and series#^32251b|absolutely convergent]] if\n\\[\n\\lim_{n\\rightarrow \\infty}\\sqrt[n]{|a_{n}|\\,|(x-x_{0})^{n}|}=|x-x_{0}|\\,\\left(\\lim_{n\\rightarrow + \\infty} \\sqrt[n]{|a_{n}|}\\right) &lt; 1 .\n\\]\nThe case where the limit is precisely \\(1\\) must be handled with a case-by-case analysis.\nThe extended real number \\(\\rho\\geq 0\\) given by\n\\[\n\\frac{1}{\\rho}:=\\lim_{n\\rightarrow +\\infty} \\sqrt[n]{|a_{n}|}\n\\]\nis then called the convergence radius of the power series because, when the power series is absolutely convergent according to the root test above, then \\(|x-x_{0}|&lt; \\rho\\).\nIf a power series converges in \\(I=(x_{0}-\\rho, x_{0}+\\rho)\\) then we can define the scalar function \\((I,\\mathbb{R},f,\\mathbb{R})\\) with\n\\[\nf(x)=\\sum_{n=0}^{+\\infty}a_{n}\\,(x-x_{0})^{n} .\n\\]\nUnless \\(a_{n}=0\\) for every \\(n&gt;N\\) with \\(N\\) fixed (the polynomial case), the scalar function defined above is a trascendental function (we can not compute its value at \\(x\\) using only algebraic operations).\nFunctions defined through a power series enjoy a lot of interesting properties.\n\n[!important] Proposition Let \\(x_{0},\\rho\\in\\mathbb{R}\\), with \\(\\rho&gt;0\\), and consider the function \\((I,\\mathbb{R},f,\\mathbb{R})\\), where \\(I=(x_{0}-\\rho,x_{0}+\\rho)\\), and \\(f(x)\\) is defined through the power series \\[\nf(x)=\\sum_{n=0}^{+\\infty}a_{n}\\,(x-x_{0})^{n}\n\\] with radius of convergence \\(\\rho\\). This function is [[1.3 Derivatives of scalar functions#^a6b2f2|smooth]], that is, it has infinitely many continuous derivatives in \\(I\\) given by \\[\n\\begin{split}\nf^{(k)}(x)&=\\sum_{n=0}^{+\\infty}\\frac{\\mathrm{d}^{k}}{\\mathrm{d}x^{k}}\\,\\left(a_{n}\\,(x-a)^{n}\\right)= \\\\ & = \\sum_{n=k}^{+\\infty}n(n-1)\\cdots(n-k+1)\\,\\left(a_{n}\\,(x-a)^{n-k}\\right).\n\\end{split}\n\\] In particular, any such function \\(f\\) is continuous.\n\n\n\nThe exponential and logarithm functions\nThe discussion in this section follows chapter 8 of “Rudin - Principle of Mathematical Analysis (third edition)”, which contains all the proofs of the results recalled here.\nThe convergence radius of the power series \\[\nE(x,x_{0})=\\sum\\limits_{k=0}^{+\\infty} \\frac{(x-x_{0})^{k}}{k!}\n\\] [[1.7 Exercises#^98a973|satisfies]] \\[\n\\frac{1}{\\rho}=\\lim_{k\\rightarrow+\\infty}\\frac{|x-a|}{\\sqrt[k]{k!}} =0,\n\\] which means that the power series converges for all \\(x\\in\\mathbb{R}\\). The smooth function defined by \\(E(x,0)\\) is called the exponential function and is denoted by \\[\n\\mathrm{exp}(x):=E(x,0)=\\sum\\limits_{k=0}^{+\\infty} \\frac{x^{k}}{k!}.\n\\] The value \\(\\mathrm{exp}(1)\\) is called the Euler number, and it is denoted by \\(\\mathrm{e}\\). It is also customary to write \\(\\mathrm{e}^{x}\\equiv\\mathrm{exp}(x)\\).\n\n[!important] Proposition: the exponential function The exponential function \\(\\mathrm{exp}\\colon \\mathbb{R}\\rightarrow \\mathbb{R}\\) satisfies the following properties: 1) \\(\\mathrm{exp}(x)'=\\mathrm{exp}(x)\\) for all \\(x\\in\\mathbb{R}\\); 2) \\(\\mathrm{exp}(a+b)=\\mathrm{exp}(a)\\,\\mathrm{exp}(b)\\) for all \\(a,b\\in\\mathbb{R}\\) (in particular, \\(\\mathrm{exp}(0)=1\\), and \\(\\mathrm{exp}(-x)=(\\mathrm{exp}(x))^{-1}\\)); 3) \\(\\mathrm{exp}(x)&gt;0\\) for all \\(x\\in\\mathbb{R}\\); 4) it is bijective (injective and surjective); 5) \\(\\lim_{x\\rightarrow+\\infty}\\,\\mathrm{exp}(x)=+\\infty\\); 6) \\(\\lim_{x\\rightarrow-\\infty}\\,\\mathrm{exp}(x)=0\\).\n\nBecause of property 3 of the exponential function, we have \\[\nE(x,x_{0})=\\mathrm{exp}(x-x_{0})=\\mathrm{exp}(x)\\,\\mathrm{exp}(-x_{0}),\n\\] which means that \\(\\mathrm{exp}(x)\\) is all we really need to discuss all the functions of the form \\(E(x,x_{0})\\).\n\n[!important] Proposition: the logarithm function The inverse function of \\(\\mathrm{exp}(x)\\equiv\\mathrm{e}^{x}\\) is called the logarithm function and it is denoted by \\(\\ln(x)\\). Since \\(\\mathrm{exp}(x)&gt;0\\), the domain of the logarithm function is \\((0,\\infty)\\). The logarithm function satisfies the following properties: 1) \\(\\ln(x)&lt;0\\) for \\(x\\in(0,1)\\), \\(\\ln(1)=0\\), and \\(\\ln(x)&gt;0\\) for \\(x&gt;1\\); 2) \\(\\ln(ab)=\\ln(a) + \\ln(b)\\) for all \\(a,b&gt;0\\) (in particular, \\(\\ln(x^{n})=n\\ln(x)\\) for all \\(n\\in\\mathbb{N}\\));\n3) \\(\\ln(\\frac{a}{b})=\\ln(a)-\\ln(b)\\) for all \\(a,b&gt;0\\); 4) \\(\\ln(x)'=\\frac{1}{x}\\) for all \\(x&gt;0\\) (using the [[1.3 Derivatives of scalar functions#^05d051|the rule for the derivative of the inverse]]), which means that \\(\\ln(x)\\) is infinitely differentiable; 5) it is (obviously) invertible, and its inverse is (obviously) the exponential function; 6) \\(\\lim_{x\\rightarrow 0^{+}}\\,\\ln(x)=-\\infty\\), and \\(\\lim_{x\\rightarrow+\\infty}\\,f(x)=x\\).\n\nUsing the exponential and logarithm functions, given \\(a&gt;0\\), we define the function \\[\na^{x}:=\\mathrm{e}^{x\\ln(a)}.\n\\] Clearly, for \\(a=1\\), we obtain a constant function. Moreover, \\(a^{x}\\) is well defined for all \\(x\\in\\mathbb{R}\\), and \\(a^{x}&gt;0\\) for every \\(x\\in\\mathbb{R}\\). Therefore, we obtain \\[\n\\ln(a^{b})=\\ln(\\mathrm{e}^{b\\ln(a)})=b\\ln(a),\n\\] which is a sort of generalization of the particular case discussed in property 2 of the logarithm function.\n\n\nThe trigonometric functions\nThe power series \\[\n\\begin{split}\nS(x,x_{0})& =\\sum\\limits_{k=0}^{+\\infty} \\frac{(-1)^{k}\\, (x-x_{0})^{2k+1}}{(2k+1)!} \\\\ & \\\\\nC(x,x_{0})& =\\sum\\limits_{k=0}^{+\\infty} \\frac{(-1)^{k} (x-x_{0})^{2k}}{(2k)!}\n\\end{split}\n\\] (absolutely) converge for every \\(x,x_{0}\\in\\mathbb{R}\\) because of the [[1.4 Sequences and series#^b44184|ratio test]]. Consequently, the radius of convergence is infinite. Setting \\(x_{0}=0\\), we obtain two smooth functions denoted by \\[\n\\begin{split}\n\\sin(x) & := \\sum\\limits_{k=0}^{+\\infty} \\frac{(-1)^{k}\\,x^{2k+1}}{(2k+1)!} \\\\ & \\\\\n\\cos(x) & := \\sum\\limits_{k=0}^{+\\infty} \\frac{(-1)^{k}\\,x^{2k}}{(2k)!},\n\\end{split}\n\\] and referred to as sine and cosine function, respectively. These functions are the building block of the so-called trigonometric functions.\n\n[!important] Proposition: sine and cosine functions Let \\(x_{0}=0\\). The sine and cosine function defined above satisfy the following properties: 1) \\(\\sin(x+2k\\pi)= \\sin(x)\\), \\(\\cos(x)=\\cos(x + 2k\\pi)\\) for all \\(x\\in\\mathbb{R}\\), and all \\(k\\in\\mathbb{Z}\\) (the functions are periodic with period \\(2\\pi\\)); 2) \\(\\sin(-x)=-\\sin(x)\\), and \\(\\cos(-x)=\\cos(x)\\) for all \\(x\\in\\mathbb{R}\\); 3) \\(\\cos(x)=\\sin(x + \\pi/2)\\) for all \\(x\\in\\mathbb{R}\\); 4) \\(\\sin(0)=\\sin(\\pi)=0\\), and \\(\\sin(\\pi/2)=1\\) (using property 3 we obtain the values of \\(\\cos\\) for the same input numbers); 5) \\(\\sin(x)'=\\cos(x)\\) and \\(\\cos(x)'=-\\sin(x)\\) for all \\(x\\in\\mathbb{R}\\); 6) \\(\\sin^{2}(x) + \\cos^{2}(x)=1\\) for all \\(x\\in\\mathbb{R}\\); 7) the inverse function of \\(\\sin(x)\\) is defined for \\(\\frac{-\\pi}{2}\\leq x\\leq \\frac{\\pi}{2}\\) and is denoted as \\(\\arcsin(x)\\); 8) the inverse function of \\(\\cos(x)\\) is defined for \\(0\\leq x\\leq \\pi\\) and is denoted as \\(\\arccos(x)\\); 9) both \\(\\sin(x)\\) and \\(\\cos(x)\\) do not admit limits at \\(\\pm\\infty\\) because they oscillate.\n\n\n[!exmp] Example Now that we know how to compute the derivative of \\(\\sin(x)\\) in a way that does not make direct use of the definition of derivative, we can apply L’Hôpital’s to solve the limit we could not solve like this [[1.3 Derivatives of scalar functions#^77ad27|before]].\n\n^ed7011\nSince they are periodic, both \\(\\sin(x)\\) and \\(\\cos(x)\\) are not invertible in their maximal domain. However, if we fix \\(-\\frac{\\pi}{2}&lt;x&lt;\\frac{\\pi}{2}\\), then both functions are invertible (because bijective), and their inverse functions are denoted by \\(\\arcsin(x)\\) and \\(\\arccos(x)\\), respectively. Note that the maximal domain of these inverse trigonometric functions is \\((-1,1)\\). Moreover, again because of periodicity, the choice \\(-\\frac{\\pi}{2}&lt;x&lt;\\frac{\\pi}{2}\\) is not unique, and we could select different intervals in which \\(\\sin(x)\\) and \\(\\cos(x)\\) are bijective.\nStarting from \\(\\sin(x)\\) and \\(\\cos(x)\\), we define the tangent and cotangent function as \\[\n\\tan(x):=\\frac{\\sin(x)}{\\cos(x)}\\,\\qquad \\cot(x):=\\frac{\\cos(x)}{\\sin(x)}=\\frac{1}{\\tan(x)},\n\\] respectively. Note that both \\(\\tan(x)\\) and \\(\\cot(x)\\) are not defined on the whole real linea because there are infinitely many points at which \\(\\sin(x)\\) or \\(\\cos(x)\\) are \\(0\\). Moreover, both \\(\\tan(x)\\) and \\(\\cot(x)\\) are periodic functions of period \\(\\pi\\) (half of \\(\\sin(x)\\) and \\(\\cos(x)\\)).\nIt turns out that \\(\\tan(x)\\) is invertible for \\(\\frac{\\pi}{2}\\leq x\\leq \\frac{\\pi}{2}\\), and its inverse function is denoted by \\(\\arctan(x)\\). Similarly, \\(\\cot(x)\\) is invertible in \\(0\\leq x\\leq \\pi\\), and its inverse function is denoted by \\(\\mathrm{arccot}(x)\\).\nAdditional information on trigonometric functions, especially in relation to their geometrical interpretation, can be found in this Wikipedia page, while additional (and often useful) trigonometric identities can be found in this Wikipedia page.\n\n\nThe Taylor series of a scalar function\nA particular type of power series is the so-called Taylor series associated with a particular scalar function \\(f\\).\n\n[!note] Definition: Taylor series Let \\(f\\) be a scalar function having continuous derivatives of any order in an interval centered at the point \\(x_{0}\\). The power series \\[\n\\sum_{n=0}^{+\\infty}\\,a_{n}\\;(x-x_{0})^{n}=\\sum_{n=0}^{+\\infty}\\,\\frac{f^{(n)}(x_{0})}{n!}\\;(x-x_{0})^{n}\n\\] is called the Taylor series of \\(f\\) centered at \\(x_{0}\\).\n\n\n[!rem] Remark Computing a closed-form expression for the Taylor series of a given function is not an easy task!\n\nThe Taylor series of a function provides a suitable way to approximate the function itself.\n\n[!important] Theorem: Taylor’s theorem Let \\(f\\) be a scalar function which is \\((k+1)\\)-times differentiable in an open interval \\(I\\) containing \\(x_{0}\\). Then, for every \\(x\\in I\\), it holds \\[\nf(x)=\\sum_{n=0}^{k}\\frac{f^{(n)}(x_{0})}{n!}\\,(x-x_{0})^{n} + o((x-x_{0})^{k}),\n\\] where the function \\(o((x-x_{0})^{k+1})\\) is defined in \\(I\\), it is called the remainder, and satisfies \\[ \\lim_{x\\rightarrow x_{0}} \\frac{o((x-x_{0})^{k})}{(x-x_{0})^{m}}=0\\] for all \\(m\\leq k\\). An explicit form for the remainder, called the Lagrange form, is \\[o((x-x_{0})^{k})= \\frac{f^{(k+1)}((1-\\theta)x_{0} + \\theta x)}{(k+1)!}\\,(x-x_{0})^{k+1}, \\] where \\(\\theta \\in (0,1)\\).\n\n^113a5b A visualization of how Taylor’s approximation works (source)  \nNow, a question. If the Taylor series of a function converges in \\(I=(x_{0}-\\rho,x_{0}+\\rho)\\), can we say it must converge to the original function?\nLooking at Taylor’s theorem above, we may be tempted to say the answer to the previous question is positive, but we would be wrong!\n\n[!exmp] Example The prototypical example of smooth (infinitely differentiable) function which is non-analytic and has a convergent Taylor series is the function \\((\\mathbb{R},\\mathbb{R},f,\\mathbb{R})\\) given by \\[ f(x)=\\left\\{\\begin{matrix} \\mathrm{e}^{-\\frac{1}{x}} &\\quad x&gt;0 \\\\ 0 & \\quad x\\leq 0 . \\end{matrix}\\right. \\]\nA non-trivial computation shows that the Taylor series of \\(f\\) around \\(x=0\\) is made of terms that are all equal to \\(0\\) so that it always converges to the zero function which is clearly different from \\(f\\).\n\n\n[!defn] Definition: analytic function If the Taylor series of a function converges to the function itself in an open interval \\(I\\), then the original function is called analytic in \\(I\\).\n\n\n\nTaylor series and limits\nOne useful exploitation of Taylor series is connected with the indeterminate form \\(\\frac{0}{0}\\) in evaluating limits.\nLet us consider the limit \\[ \\lim_{x\\rightarrow 0}\\; \\frac{f(x)}{g(x)} \\,=\\,\\lim_{x\\rightarrow 0} \\;\\frac{1-\\cos^{2}(x)}{x^{2}} .\\] The second order Taylor expansion for \\(\\cos^{2}(x)\\) reads \\[ \\cos^{2}(x)= 1 -x^{2} + o(x^{2}) \\] so that \\[\\lim_{x\\rightarrow 0} \\;\\frac{1-\\cos^{2}(x)}{x^{2}} =\\lim_{x\\rightarrow 0}\\frac{1-1+x^{2} - o(x^{3})}{x^{2}} = \\lim_{x\\rightarrow 0}\\left(\\frac{x^{2} }{x^{2}} - \\frac{o(x^{3})}{x^{2}} \\right) = 1 \\] because of the property of the remainder.",
    "crumbs": [
      "Home",
      "Teaching",
      "Calculus - Robotics Engineering - UC3M",
      "Power series"
    ]
  },
  {
    "objectID": "misc/geodesics_dual_connections_qubit.html",
    "href": "misc/geodesics_dual_connections_qubit.html",
    "title": "Geodesics of dual connections",
    "section": "",
    "text": "Show/Hide code\nimport sympy as sp\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom sympy import *\nfrom ipywidgets import interact, FloatSlider, interact_manual\n\n\n\n\nShow/Hide code\n# ------------------------------------------------------------------\n# 0.  Pauli-basis helper functions\n# ------------------------------------------------------------------\nI2 = sp.eye(2)\nsx = sp.Matrix([[0, 1], [1, 0]])\nsy = sp.Matrix([[0, -sp.I], [sp.I, 0]])\nsz = sp.Matrix([[1, 0], [0, -1]])\n\n\n\n\n\nShow/Hide code\ndef coord_to_matrix(w_0, x_0, y_0, z_0):\n    \"\"\"Return x0*I + x1*σx + x2*σy + x3*σz.\"\"\"\n    return w_0*I2 + x_0*sx + y_0*sy + z_0*sz\n\ndef f_of_pauli(w_0, x_0, y_0, z_0, func):\n    \"\"\"\n    Spectral functional calculus for an Hermitian operator.\n    Returns f(A) using eigenprojector formula.\n    \"\"\"\n    r  = sp.sqrt(x_0**2 + y_0**2 + z_0**2)\n    lam_p, lam_m = w_0 + r, w_0 - r\n    a = sp.Rational(1, 2) * (func(lam_p) + func(lam_m))\n    b = sp.Rational(1, 2) * (func(lam_p) - func(lam_m))\n    # Unit vector  n̂ = x⃗ / r  (SymPy handles r→0 with Piecewise)\n    nx, ny, nz = x_0/r, y_0/r, z_0/r\n    expr = a*I2 + b*(nx*sx + ny*sy + nz*sz)\n    return sp.simplify(expr)          # explicit call\n\n\n\n\n\nShow/Hide code\n# from density operator in Pauli basis to coordinates\n\ndef bloch_coords(rho):\n    #rx = sp.simplify(sp.trace(rho * sx))\n    #ry = sp.simplify(sp.trace(rho * sy))\n    #rz = sp.simplify(sp.trace(rho * sz))\n    rx = sp.trace(rho * sx)\n    ry = sp.trace(rho * sy)\n    rz = sp.trace(rho * sz)\n    return [rx, ry, rz]\n\n\n\n\nShow/Hide code\n# ------------------------------------------------------------------\n# 1.  Declare symbolic coordinates\n# ------------------------------------------------------------------\nx_0, y_0, z_0 = sp.symbols('x_0 y_0 z_0', real=True)\na_1, a_2, a_3 = sp.symbols('a_1 a_2 a_3', real=True)\nt             = sp.symbols('t', real=True)\n\nrho = coord_to_matrix(1/2, x_0/2, y_0/2, z_0/2)\n\nprint(rho)\n\n\nMatrix([[z_0/2 + 0.5, x_0/2 - I*y_0/2], [x_0/2 + I*y_0/2, 0.5 - z_0/2]])\n\n\n\n\nShow/Hide code\nprint(\"Loading symbolic expressions of geodesic of BKM\")\n\nprint(\"Computing ln(rho)\")\n\nlog_rho   = f_of_pauli(1/2, x_0/2, y_0/2, z_0/2, sp.log)    # log(p)\n\nprint(\"Computing ta\")\n\nta      = coord_to_matrix(0, t*a_1, t*a_2, t*a_3)  # t a\n\nprint(\"Computing ln(rho) +ta\")\n\nA       = (log_rho + ta).expand()\n\nprint(\"Computing coordinates of ln(rho) +ta\")\n\n# Extract Pauli components of A\nA0 = (A.trace() / 2).expand()\nA1 = (A[0,1] + A[1,0]) / 2\nA2 = (A[1,0] - A[0,1]) / (2*sp.I)\nA3 = (A[0,0] - A[1,1]) / 2\n\nprint(\"Computing exp(ln(rho) +ta)\")\n\nnum1  = f_of_pauli(A0, A1, A2, A3, sp.exp)\n\nprint(\"Computing Tr(exp(ln(rho) +ta))\")\n\nden1  = sp.trace(num1)\n\nprint(\"Final expression\")\n\n#rho_BKM = (num1 / den1).simplify()\n\nrho_BKM = num1 / den1\n\n\nprint(\"Extracting coordinates in the Pauli basis\")\n\nbloch_BKM = bloch_coords(rho_BKM)\n\n\nLoading symbolic expressions of geodesic of BKM\nComputing ln(rho)\nComputing ta\nComputing ln(rho) +ta\nComputing coordinates of ln(rho) +ta\nComputing exp(ln(rho) +ta)\nComputing Tr(exp(ln(rho) +ta))\nFinal expression\nExtracting coordinates in the Pauli basis\n\n\n\n\nShow/Hide code\nprint(\"Loading symbolic expressions of geodesic of BH\")\n\nprint(\"exponential of ta\")\n\nexp_ta = f_of_pauli(0, t*a_1, t*a_2, t*a_3, sp.exp)\n\nprint(\"exp(ta)rho exp(ta)\")\n\nnumer  = (exp_ta * rho * exp_ta).expand()\n\nprint(\"trace of exp(ta)rho exp(ta)\")\ndenom  = sp.trace(numer)\n\nprint(\"final expression\")\n#rho_BH  = (numer / denom).simplify()\n\nrho_BH  = numer / denom\n\nprint(\"Extracting coordinates in the Pauli basis\")\n\nbloch_BH = bloch_coords(rho_BH)\n\n\nLoading symbolic expressions of geodesic of BH\nexponential of ta\nexp(ta)rho exp(ta)\ntrace of exp(ta)rho exp(ta)\nfinal expression\nExtracting coordinates in the Pauli basis\n\n\n\n\nShow/Hide code\nprint(\"Loading symbolic expressions of geodesic of WY\")\n\nprint(\"sqrt(rho)\")\n\nsqrt_rho   = f_of_pauli(1/2, x_0/2, y_0/2, z_0/2, sp.sqrt)    \n\n \n\nprint(\"exponential of ta\")\n\nexp_ta = f_of_pauli(0, t*a_1, t*a_2, t*a_3, sp.exp)\nexp_2ta = f_of_pauli(0, 2*t*a_1, 2*t*a_2, 2*t*a_3, sp.exp)\n\nprint(\"exp(ta) sqrt(rho) exp(2ta) sqrt(rho)\")\n\nnumer  = (exp_ta * sqrt_rho * exp_2ta * sqrt_rho * exp_ta).expand()\n\nprint(\"trace of exp(ta) sqrt(rho) exp(2ta) sqrt(rho)\")\ndenom  = sp.trace(numer)\n\nprint(\"final expression\")\n\n\nrho_WY  = numer / denom\n\nprint(\"Extracting coordinates in the Pauli basis\")\n\nbloch_WY = bloch_coords(rho_WY)\n\n\nLoading symbolic expressions of geodesic of WY\nsqrt(rho)\nexponential of ta\nexp(ta) sqrt(rho) exp(2ta) sqrt(rho)\ntrace of exp(ta) sqrt(rho) exp(2ta) sqrt(rho)\nfinal expression\nExtracting coordinates in the Pauli basis\n\n\n\n\nShow/Hide code\nprint(\"Loading symbolic expressions of geodesic of CDN\")\n\nprint(\"cbrt(rho)\")\n\ncbrt_rho   = f_of_pauli(1/2, x_0/2, y_0/2, z_0/2, sp.cbrt)    \n\n \n\nprint(\"exponential of ta\")\n\nexp_ta = f_of_pauli(0, t*a_1, t*a_2, t*a_3, sp.exp)\nexp_2ta = f_of_pauli(0, 2*t*a_1, 2*t*a_2, 2*t*a_3, sp.exp)\n\n\nprint(\"exp(ta) cbrt(rho) exp(2ta) cbrt(rho) exp(2ta) cbrt(rho) exp(ta)\")\n\nnumer  = (exp_ta * cbrt_rho * exp_2ta * cbrt_rho * exp_2ta * cbrt_rho * exp_ta).expand()\n\nprint(\"trace of exp(ta) cbrt(rho) exp(2ta) cbrt(rho) exp(2ta) cbrt(rho) exp(ta)\")\ndenom  = sp.trace(numer)\n\nprint(\"final expression\")\n\n\nrho_CDN  = numer / denom\n\nprint(\"Extracting coordinates in the Pauli basis\")\n\nbloch_CDN = bloch_coords(rho_CDN)\n\n\nLoading symbolic expressions of geodesic of CDN\ncbrt(rho)\nexponential of ta\nexp(ta) cbrt(rho) exp(2ta) cbrt(rho) exp(2ta) cbrt(rho) exp(ta)\ntrace of exp(ta) cbrt(rho) exp(2ta) cbrt(rho) exp(2ta) cbrt(rho) exp(ta)\nfinal expression\nExtracting coordinates in the Pauli basis\n\n\n\n\nShow/Hide code\n\ndef draw_bloch_sphere(ax, alpha=0.1):\n    u, v = np.mgrid[0:2*np.pi:100j, 0:np.pi:50j]\n    x = np.cos(u) * np.sin(v)\n    y = np.sin(u) * np.sin(v)\n    z = np.cos(v)\n    ax.plot_surface(x, y, z, color='lightblue', alpha=alpha, linewidth=0)\n    \n    # Draw axis lines\n    ax.plot([-1, 1], [0, 0], [0, 0], color='k', lw=0.5)\n    ax.plot([0, 0], [-1, 1], [0, 0], color='k', lw=0.5)\n    ax.plot([0, 0], [0, 0], [-1, 1], color='k', lw=0.5)\n    \n    ax.set_xlim([-1, 1])\n    ax.set_ylim([-1, 1])\n    ax.set_zlim([-1, 1])\n    ax.set_box_aspect([1, 1, 1])\n    ax.set_xlabel(\"X\")\n    ax.set_ylabel(\"Y\")\n    ax.set_zlabel(\"Z\")\n    ax.set_title(\"Bloch Sphere + Geodesic\")\n\n\n \n\n\n\ndef interactive_plot(x0=0, y0=0, z0=0.4, ax=0, ay=0, az=2):\n    # Turn off interactive plotting temporarily\n    plt.ioff()\n\n\n    # Pre-create the figure and axes\n    fig = plt.figure(figsize=(12, 8))\n    ax1 = fig.add_subplot(111, projection='3d')\n    draw_bloch_sphere(ax1, alpha=0.15)\n    ax1.set_title(\"Quantum State Evolution on Bloch Sphere\")\n\n    fig2 = plt.figure(figsize=(12, 8))\n    ax2 = fig2.add_subplot(111)\n    ax2.set_xlabel('Time t')\n    ax2.set_ylabel('Bloch Coordinates')\n    ax2.set_title('Bloch x-Components vs Time')\n    ax2.grid(True, alpha=0.3)\n    ax2.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n    \n    fig3 = plt.figure(figsize=(12, 8))\n    ax3 = fig3.add_subplot(111)\n    ax3.set_xlabel('Time t')\n    ax3.set_ylabel('Bloch y-coordinates')\n    ax3.set_title('Bloch y-components vs Time')\n    ax3.grid(True, alpha=0.3)\n    ax3.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n\n    fig4 = plt.figure(figsize=(12, 8))\n    ax4 = fig4.add_subplot(111)\n    ax4.set_xlabel('Time t')\n    ax4.set_ylabel('Bloch z-coordinates')\n    ax4.set_title('Bloch z-components vs Time')\n    ax4.grid(True, alpha=0.3)\n    ax4.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n\n  \n \n\n \n    \n    #plt.tight_layout()\n\n\n\n    \n    if x0**2 + y0**2 + z0**2 &gt; 1:\n        print(\"Invalid Bloch vector: must be inside unit sphere\")\n        return\n    # Create substitution dictionary mapping symbols to values\n    subs = {x_0: x0, y_0: y0, z_0: z0, a_1: ax, a_2: ay, a_3: az}\n \n \n    \n\n \n\n    bloch_BH_funcs = [lambdify(t, comp.subs(subs), 'numpy') for comp in bloch_BH]    \n\n    bloch_BKM_funcs = [lambdify(t, comp.subs(subs), 'numpy') for comp in bloch_BKM]    \n\n    bloch_WY_funcs = [lambdify(t, comp.subs(subs), 'numpy') for comp in bloch_WY] \n    \n    bloch_CDN_funcs = [lambdify(t, comp.subs(subs), 'numpy') for comp in bloch_CDN] \n        \n    \n    \n    # Compute trajectory\n    ts = np.linspace(-3, 3, 1000)\n    # choices like x0=y0=a1=a2=0 return an integer for xs and ys that is later incompatible with scatter2, and this is why the if statement below is used\n    results_BH = []\n    for f in bloch_BH_funcs:\n        result_BH = f(ts)\n        if np.isscalar(result_BH):\n            # It's a constant, so create an array of that constant\n            results_BH.append(np.full(len(ts), float(result_BH)))\n        else:\n            results_BH.append(np.real(result_BH).astype(np.float64))\n        \n    xsBH, ysBH, zsBH = results_BH\n    \n    purity_final_state_BH = xsBH[-1]**2 + ysBH[-1]**2 + zsBH[-1]**2\n    # the :.numf below determines the precision of the approximation  \n    print(f\"Final state coordinates BH: X_BH~{xsBH[-1]:}, Y_BH~{ysBH[-1]:}, Z_BH~{zsBH[-1]:} \\nFinal state purity BH: {1/2*(1+ xsBH[-1]**2 + ysBH[-1]**2 + zsBH[-1]**2):}\") \n\n    results_BKM = []\n    for f in bloch_BKM_funcs:\n        result_BKM = f(ts)\n        if np.isscalar(result_BKM):\n            # It's a constant, so create an array of that constant\n            results_BKM.append(np.full(len(ts), float(result_BKM)))\n        else:\n            results_BKM.append(np.real(result_BKM).astype(np.float64))\n        \n    xsBKM, ysBKM, zsBKM = results_BKM\n    \n    purity_final_state_BKM = xsBKM[-1]**2 + ysBKM[-1]**2 + zsBKM[-1]**2\n    # the :.numf below determines the precision of the approximation  \n    print(f\"Final state coordinates BKM: X_BKM~{xsBKM[-1]:}, Y_BKM~{ysBKM[-1]:}, Z_BKM~{zsBH[-1]:} \\nFinal state purity BKM: {1/2*(1+ xsBKM[-1]**2 + ysBKM[-1]**2 + zsBKM[-1]**2):}\") \n    \n    results_WY = []\n    for f in bloch_WY_funcs:\n        result_WY = f(ts)\n        if np.isscalar(result_WY):\n            # It's a constant, so create an array of that constant\n            results_WY.append(np.full(len(ts), float(result_WY)))\n        else:\n            results_WY.append(np.real(result_WY).astype(np.float64))\n        \n    xsWY, ysWY, zsWY = results_WY\n    \n    purity_final_state_WY = xsWY[-1]**2 + ysWY[-1]**2 + zsWY[-1]**2\n    # the :.numf below determines the precision of the approximation  \n    print(f\"Final state coordinates WY: X_WY~{xsWY[-1]:}, Y_WY~{ysBKM[-1]:}, Z_WY~{zsBH[-1]:} \\nFinal state purity WY: {1/2*(1+ xsWY[-1]**2 + ysWY[-1]**2 + zsWY[-1]**2):}\") \n    \n    results_CDN = []\n    for f in bloch_CDN_funcs:\n        result_CDN = f(ts)\n        if np.isscalar(result_CDN):\n            # It's a constant, so create an array of that constant\n            results_CDN.append(np.full(len(ts), float(result_CDN)))\n        else:\n            results_CDN.append(np.real(result_CDN).astype(np.float64))\n        \n    xsCDN, ysCDN, zsCDN = results_CDN\n    \n    purity_final_state_CDN = xsCDN[-1]**2 + ysCDN[-1]**2 + zsCDN[-1]**2\n    # the :.numf below determines the precision of the approximation  \n    print(f\"Final state coordinates CDN: X_CDN~{xsCDN[-1]:}, Y_CDN~{ysCDN[-1]:}, Z_CDN~{zsCDN[-1]:} \\nFinal state purity CDN: {1/2*(1+ xsCDN[-1]**2 + ysCDN[-1]**2 + zsCDN[-1]**2):}\") \n\n    \n    # Plot trajectory BH\n    line = ax1.plot(xsBH, ysBH, zsBH, color='red', linewidth=2, label='Geodesic BH', alpha=0.8)\n    line = ax1.plot(xsBKM, ysBKM, zsBKM, color='blue', linewidth=2, label='Geodesic BKM', alpha=0.8)\n    line = ax1.plot(xsWY, ysWY, zsWY, color='green', linewidth=2, label='Geodesic WY', alpha=0.8)\n    line = ax1.plot(xsCDN, ysCDN, zsCDN, color='yellow', linewidth=2, label='Geodesic CDN', alpha=0.8)\n \n    \n    # Mark initial and final states\n    scatter1 = ax1.scatter([x0], [y0], [z0], color='black', s=100, label='Initial state', alpha=0.9)\n    scatter2 = ax1.scatter([xsBH[-1]], [ysBH[-1]], [zsBH[-1]], color='red', s=80, label='Final state BH', alpha=0.9)\n    scatter2 = ax1.scatter([xsBKM[-1]], [ysBKM[-1]], [zsBKM[-1]], color='blue', s=80, label='Final state BKM', alpha=0.9)\n    scatter2 = ax1.scatter([xsWY[-1]], [ysWY[-1]], [zsWY[-1]], color='green', s=80, label='Final state WY', alpha=0.9)\n    scatter2 = ax1.scatter([xsCDN[-1]], [ysCDN[-1]], [zsCDN[-1]], color='yellow', s=80, label='Final state CDN', alpha=0.9)\n\n    \n    # Add legend for 3D plot\n    ax1.legend(loc='upper right')\n    \n    # 2D trajectory components plot\n    line2d_x = ax2.plot(ts, xsBH, 'r-', label='X_BH(t)', alpha=0.8)\n    line2d_x = ax2.plot(ts, xsBKM, 'b-', label='X_BKM(t)', alpha=0.8)\n    line2d_x = ax2.plot(ts, xsWY, 'g-', label='X_WY(t)', alpha=0.8)\n    line2d_x = ax2.plot(ts, xsCDN, 'y-', label='X_CDN(t)', alpha=0.8)\n\n    line2d_y = ax3.plot(ts, ysBH, 'r-', label='Y_BH(t)', alpha=0.8)\n    line2d_y = ax3.plot(ts, ysBKM, 'b-', label='Y_BKM(t)', alpha=0.8)\n    line2d_y = ax3.plot(ts, ysWY, 'g-', label='Y_WY(t)', alpha=0.8)\n    line2d_y = ax3.plot(ts, ysCDN, 'y-', label='Y_CDN(t)', alpha=0.8)\n    \n    line2d_z = ax4.plot(ts, zsBH, 'r-', label='Z_BH(t)', alpha=0.8)\n    line2d_z = ax4.plot(ts, zsBKM, 'b-', label='Z_BKM(t)', alpha=0.8)\n    line2d_z = ax4.plot(ts, zsWY, 'g-', label='Z_WY(t)', alpha=0.8)\n    line2d_z = ax4.plot(ts, zsCDN, 'y-', label='Z_CDN(t)', alpha=0.8)\n    \n    \n    \n    # Add legend for 2D plot\n    ax2.legend(loc='upper right')\n    ax3.legend(loc='upper right')\n    ax4.legend(loc='upper right')\n \n    \n\n    \n    plt.tight_layout()\n    plt.ion()  # Turn interactive plotting back on\n    plt.show()\n \n\n \n\ninteract_manual(\n    interactive_plot,\n    x0=FloatSlider(min=-1.0, max=1.0, step=0.05, value=0),\n    y0=FloatSlider(min=-1.0, max=1.0, step=0.05, value=0),\n    z0=FloatSlider(min=-1.0, max=1.0, step=0.05, value=0.4),\n    ax=FloatSlider(min=-10, max=10, step=0.5, value=0),\n    ay=FloatSlider(min=-10, max=10, step=0.5, value=0),\n    az=FloatSlider(min=-10, max=10, step=0.5, value=2)\n); #semicolon is to suppress &lt;function __main__.interactive_plot(x0=0, y0=0, z0=0.4, a_1=0, a_2=0, a_3=2)&gt; to appear at the end of the cell",
    "crumbs": [
      "Home",
      "Miscellanea",
      "Geodesics of dual connections"
    ]
  },
  {
    "objectID": "misc/Fundamentals of statistics.html",
    "href": "misc/Fundamentals of statistics.html",
    "title": "Fundamentals of statistics",
    "section": "",
    "text": "The starting point will be operational, but really limited. We start with situations in which we have a finite and discrete outcome space \\(\\Omega\\) (dices, coins, alphabets, and so on).\nEach element in \\(\\Omega\\) is referred to as an event, and it should be intuitively clear that what we mean by that is that an operational inquiry among those we are considering will determine uniquely the selection of one element in \\(\\Omega\\). In this case, given the finite and discrete nature of \\(\\Omega\\), everything is clear and “directly verifiable”.\n\nRandom variables\nOften, the whole probability space \\((\\Omega,\\tau)\\) is “too much” for the practical purposes of statistical instances.\nFor instance, if we are interested in the number of heads after \\(n=3\\) throws of a fair coin, the outcome space \\(\\Omega\\) would be \\[\n\\Omega=\\{HHH,HHT,HTH,HTT,THH,THT,TTH,TTT\\},\n\\] and the probability \\(\\tau\\) would be a probability distribution on \\(\\Omega\\). However, if we introduce the function \\(X\\colon \\Omega\\rightarrow\\mathbb{R}\\) whose output is precisely the number of heads of the given configuration in \\(\\Omega\\), we obtain a smaller probability space \\((\\Omega_{X},\\tau_{X})\\) with \\(\\Omega_{X}=X(\\Omega)=\\{0,1,2,3\\}\\), and \\(\\tau_{X}=X_{* }\\tau\\) (the pushforward measure). For all practical purposes, \\((\\Omega_{X},\\tau_{X})\\) is the only thing we really need.\nSince \\(\\mid\\Omega\\mid= 2^{n}\\) while \\(\\mid\\Omega_{X}\\mid=n+1\\), where \\(n\\) is the number of throws, we see that \\(\\Omega_{X}\\) becomes quite smaller than \\(\\Omega\\) when \\(n\\) grows.\nThis example, although silly, describes an interesting fact: sometimes, we are not interested in the whole probability space modelling a random phenomena, but only in a specific “random quantity” that we can model with a real-valued (measurable) function.\nWe thus arrive to the notion of random variable as a real-valued, measurable function \\(X\\colon\\Omega\\rightarrow\\mathbb{R}\\).\nOnce we have random variables, we can even dare to essentially forget the probability space \\((\\Omega,\\tau)\\), and focus on \\(\\Omega_{X}=X(\\Omega)\\subseteq\\mathbb{R}\\). This approach can be particularly useful because there are practical situations in which we only have a quite uncertain knowledge of \\(\\Omega\\).\nEXAMPLES NEEDED!!!\nREFERENCES TO THE VARIOUS DISCUSSIONS ON DISPOSE OF PROBABILITY SPACES/RANDOM VARIABLES (FREMLIN VOL. 2 AND STACKEXCHANGE)\nIn practice, we focus on the induced distribution of a random variable because we can only really measure real numbers. This fact is coherent with the operator algebraic setting in which experimental contexts are realized in terms of Abelian algebras.\n\n\nCovariance\nConsider two random variables \\(X,Y\\colon \\Omega\\rightarrow\\mathbb{R}\\), and the associated vector random variable \\(Z\\equiv(X,Y)\\colon\\Omega\\rightarrow\\mathbb{R}^{2}\\). The pushforward measure \\(Z_{*}\\tau\\) is in general not a product measure, and covariance helps us understanding how far from a product measure it is. Specifically, if \\(\\mathrm{Cov}(X,Y)\\neq 0\\) then \\(Z_{*}\\tau\\) is not a product measure (however, if \\(\\mathrm{Cov}(X,Y)=0\\), we can not say anything (see example 2.5.3 in “Introduction to mathematical statistics” by Hogg, McKean, Craig)).\n\n\nFrom Cramér’s “Mathematical methods of statistics” Chapter 13\n13.1. On random experiments. In the most varied fields of practical and scientific activity, cases occur where certain experiments or observations may be repeated a large number of times under similar circumstances. On each occasion, our attention is then directed to a result of the observation, which is expressed by a certain number of characteristic features. In many cases these characteristics directly take a quantitative form: at each observation something is counted or measured. In other cases, the characteristics are qualitative: we observe e. g. the colour of a certain object, the occurrence or non-occurrence of some specified event in connection with each experiment, etc. In the latter case, it is always possible to express the characteristics in numerical form according to some conventional system of notation. Whenever it is found convenient, we may thus always suppose that the result of each observation is expressed by a certain number of quantities.\n13.5. On the frequency interpretation. If E is an impossible event, i. e. an event that can never occur at a performance of the experiment, any frequency of Ε must be zero; and consequently we take the probability P to be 0. On the other hand, if we know that for some event Ε we have Ρ=0, then Ε is not necessarily an impossible event. In fact, the frequency interpretation of Ρ only implies that the frequency v/n of Ε will for large n be approximately equal to zero, so that in the long run Ε will at most occur in a very small percentage of all cases. The same conclusion holds not only when \\(Ρ = 0\\), but even under the more general assumption that \\(0&lt;P&lt;\\epsilon\\), where \\(\\epsilon\\) is some very small number. If Ε is an event of this type, and if the experiment is performed one single time, it can thus be considered as practically certain that Ε will not occur. Analogous considerations hold for certain events and \\(P=1\\). This particular case of the frequency interpretation of a probability will often be applied in the sequel.",
    "crumbs": [
      "Home",
      "Miscellanea",
      "Fundamentals of statistics"
    ]
  },
  {
    "objectID": "index_teaching.html",
    "href": "index_teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\nDescription\n\n\n\n\n\n\n\n\nCalculus - Robotics Engineering - UC3M\n\n\nCalculus course for UC3M\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Home",
      "Teaching"
    ]
  },
  {
    "objectID": "index_misc.html",
    "href": "index_misc.html",
    "title": "Miscellanea",
    "section": "",
    "text": "Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\nDate\n\n\n\ntags\n\n\n\n\n\n\n\n\nGeodesics of dual connections\n\n\nSep 21, 2025\n\n\ntest\n\n\n\n\n\n\nFundamentals of statistics\n\n\nSep 18, 2025\n\n\n \n\n\n\n\n\n\nTimeline on fields of covariances\n\n\nMay 18, 2025\n\n\nidee, memento, accademia/NCP, accademia/fields-of-covariances, accademia/statistical-categories\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Home",
      "Miscellanea"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "conferences.html",
    "href": "conferences.html",
    "title": "Conferences & Seminars",
    "section": "",
    "text": "Geometric Science of Information GSI’25, Palais du Grand Large, Saint Malo, France, 2025, Scientific Committee, Co-chairman of the session “Geometry of Classical and Quantum States”.\n\n\nIX International Workshop on Information Geometry, Quantum Mechanics and Applications 2025, UC3M, Madrid, Spain, 2025, Local Organizer.\n\n\nThe XXXIX RSEF Physics Biennial, Donostia/San Sebastián, Spain, 2024, Symposia Organizer.\n\n\nVI International Workshop on Information Geometry, Quantum Mechanics and Applications 2024, UC3M, Madrid, Spain, 2024, Local Organizer.\n\n\nGeometry of Information Theory, UC3M, Madrid, Spain, 2023, Local Organizer.\n\n\nGeometric Science of Information GSI’23, Palais du Grand Large, Saint Malo, France, 2023, Scientific Committee, Co-chairman of the session “Geometry of Quantum States”.\n\n\nV International Workshop on Information Geometry, Quantum Mechanics and Applications 2023, UC3M, Madrid, Spain, 2023, Local Organizer.\n\n\nGeometric Science of Information GSI’21, Sorbonne University, Paris, France, 2021, Scientific Committee, Co-chairman of the session “Geometry of Quantum States”.\n\n\nGeometric Science of Information GSI’19, ENAC, Toulouse, France, 2019, Co-chairman of the session “Geometry of Quantum States”.\n\n\nII International Workshop on Information Geometry, Quantum Mechanics and Applications, Agriturismo Erbanito, San Rufo (Salerno), Italy, 2018, Organizing committee.\n\n\nCurrent Problems in Theoretical Physics XXIV, Lloyd’s Baia Hotel, Vietri sul Mare, Italy, 2018, Local organizer.\n\n\nInformation Geometry, Quantum Mechanics and Applications, Agriturismo Erbanito, San Rufo (Salerno), Italy, 2017, Organizing committee."
  },
  {
    "objectID": "conferences.html#organization-of-conferences-and-workshops",
    "href": "conferences.html#organization-of-conferences-and-workshops",
    "title": "Conferences & Seminars",
    "section": "",
    "text": "Geometric Science of Information GSI’25, Palais du Grand Large, Saint Malo, France, 2025, Scientific Committee, Co-chairman of the session “Geometry of Classical and Quantum States”.\n\n\nIX International Workshop on Information Geometry, Quantum Mechanics and Applications 2025, UC3M, Madrid, Spain, 2025, Local Organizer.\n\n\nThe XXXIX RSEF Physics Biennial, Donostia/San Sebastián, Spain, 2024, Symposia Organizer.\n\n\nVI International Workshop on Information Geometry, Quantum Mechanics and Applications 2024, UC3M, Madrid, Spain, 2024, Local Organizer.\n\n\nGeometry of Information Theory, UC3M, Madrid, Spain, 2023, Local Organizer.\n\n\nGeometric Science of Information GSI’23, Palais du Grand Large, Saint Malo, France, 2023, Scientific Committee, Co-chairman of the session “Geometry of Quantum States”.\n\n\nV International Workshop on Information Geometry, Quantum Mechanics and Applications 2023, UC3M, Madrid, Spain, 2023, Local Organizer.\n\n\nGeometric Science of Information GSI’21, Sorbonne University, Paris, France, 2021, Scientific Committee, Co-chairman of the session “Geometry of Quantum States”.\n\n\nGeometric Science of Information GSI’19, ENAC, Toulouse, France, 2019, Co-chairman of the session “Geometry of Quantum States”.\n\n\nII International Workshop on Information Geometry, Quantum Mechanics and Applications, Agriturismo Erbanito, San Rufo (Salerno), Italy, 2018, Organizing committee.\n\n\nCurrent Problems in Theoretical Physics XXIV, Lloyd’s Baia Hotel, Vietri sul Mare, Italy, 2018, Local organizer.\n\n\nInformation Geometry, Quantum Mechanics and Applications, Agriturismo Erbanito, San Rufo (Salerno), Italy, 2017, Organizing committee."
  },
  {
    "objectID": "conferences.html#presentations-and-talks",
    "href": "conferences.html#presentations-and-talks",
    "title": "Conferences & Seminars",
    "section": "Presentations and talks:",
    "text": "Presentations and talks:\n\n\nThe geometry of classical and quantum states – Conference Talk at “The XXXIX RSEF Physics Biennial”, Donostia/San Sebastián, Spain, 2024.\n\n\nCan Čencov meet Petz? – Conference Talk at the “V International Workshop of Information Geometry, Quantum Mechanics and Applications 2023”, UC3M, Madrid, Spain, 2023.\n\n\nCan Cencov meet Petz? – Seminar at the “Q-Math Seminars”, UC3M, Madrid, Spain, 2023.\n\n\nJordan algebras, coadjoint orbits, and information geometry – Online Seminar at the “Prague-Hradec Králové seminar on Cohomology in algebra, geometry, physics and statistics”, 2022.\n\n\nThe Information Geometry of Jordan algebras – Online Conference Talk at “Virtual Meeting on Information Geometry, 2021”, Università di Bologna, 2021.\n\n\nA groupoid-based perspective on Quantum Mechanics – Online Seminar at “Topological Quantum Field Theory Seminar”, Técnico Lisboa, 2021.\n\n\nGently wandering through the space of quantum states, relating metric tensors with group actions – Online Seminar at the Max Planck Institute for the Mathematics in the Sciences, Leipzig, Germany, 2020.\n\n\nOn the notion of composite systems – Conference Talk at “Geometric Science of Information GSI’19”, Toulouse, France, 2019.\n\n\nThe Jordan product, the Fisher-Rao metric tensor, and the Bures-Uhlmann metric tensor – Conference Talk at “Information Geometry, Quantum Mechanics and Applications III”, Grajera (Segovia), Spain, 2019.\n\n\nA gentle introduction to Schwinger’s picture and groupoids in Quantum Mechanics – Conference Talk at “Information Geometry, Quantum Mechanics and Applications II”, San Rufo (Salerno), Italy, 2018 (link Wayback Machine).\n\n\nThe space of quantum states relative entropies and metric tensors – Conference Talk at “Current Problems in Theoretical Physics XXIV”, Vietri sul Mare, Italy, 2018.\n\n\nThe space of quantum states, relative entropies and metric tensors – Conference Talk at “60 Years Alberto Ibort Fest - Classical and Quantum Physics: Geometry, Dynamics and Control”, Madrid, Spain, 2018.\n\n\nThe space of quantum states, relative entropies and metric tensors – Seminar at the Max Planck Institute for Mathematics in Science, Leipzig, Germany, 2018,.\n\n\nHamilton-Jacobi theory and Information Geometry – Conference Talk at “Geometric Science of Information GSI’17”, Paris, France, 2017.\n\n\nMetric tensors on the space of invertible quantum states – Conference Talk at “XXVI International fall workshop on Geometry and Physics”, Braga, Portugal, 2017.\n\n\nA differential geometric approach to the geometry of quantum states – Conference Talk at “Information Geometry, Quantum Mechanics and Applications”, San Rufo (Salerno), Italy, 2017.\n\n\nTime, classical and quantum – Conference Talk at “XXV International fall workshop on Geometry and Physics”, Madrid, Spain, 2016.\n\n\nTime, classical and quantum – Conference Talk at “INFN Quantum Meeting”, Trieste, Italy, 2016.\n\n\nTime, classical and quantum – Conference Talk at “I.Q.I.S. 2015”, Monopoli, Italy, 2015."
  },
  {
    "objectID": "misc/2025-05-27 timeline on fields of covariances.html",
    "href": "misc/2025-05-27 timeline on fields of covariances.html",
    "title": "Timeline on fields of covariances",
    "section": "",
    "text": "2025-05-27 states and fields of Hilbert spaces\nDiscussing with Gemini 2.5 pro (see my zotero), I realized the idea of a “GNS bundle” over the space of states \\(\\mathcal{S}(\\mathscr{A})\\) on \\(\\mathscr{A}\\) must be replaced with the idea of a field of Hilbert spaces because of varying dimensions.\n\n\n2025-05-23 estimators as sections of the GNS bundle\nWhile driving back home, I realized that statistical estimators are section of the “GNS bundle” over the space of states \\(\\mathcal{S}(\\mathscr{A})\\) on \\(\\mathscr{A}\\). Then, when \\(i:M\\rightarrow \\mathcal{S}(\\mathscr{A})\\), I can pull-back the estimator to the model.\n\n\n2025-03-01 fields of covariances and metric tensor as functors\nThis note has been written on 2025-05-18, and the date 2025-03-01 is the best approximation of the original date I came up with.\n\nRecently, together with Laura, we developed the idea that an interesting way of looking at Riemannian metric tensors is as functors from \\(\\mathsf{C}(M)\\) (a manifold seen as a category) to \\(\\mathsf{Hilb}_{\\mathbb{R}}\\). If a Lie group acts on \\(M\\), invariant Riemannian metric tensors are functor from the action groupoid to \\(\\mathsf{Hilb}_{\\mathbb{R}}\\). Contravariant Riemannian metric tensors are contravariant functors.\nThis point of view is particularly useful for \\(\\mathsf{NCP}\\) because it makes fields of covariances become functors from \\(\\mathsf{NCP}\\) to \\(\\mathsf{Hilb}\\), thus making them elegant. Moreover, the GNS construction is clearly a functor from \\(\\mathsf{NCP}\\) to \\(\\mathsf{Hilb}\\) (a fact so trivial yet nowhere to be found in the literature).\n\n\n2023-10-26 fields of covariances on non-faithful states\nReferring to the “Čencov meets Petz” problem, perhaps, the way to address the non-faithful states is by suitably choosing the notion of continuity of the covariance field. Roughly speaking, since \\(\\mathcal{H}_{qp}\\) is the kernel of \\(\\Delta_{\\rho}\\), a suitable continuity condition can allow us to say that, given a net \\(\\rho_{\\lambda}\\) of faithful states that converges to \\(\\rho\\), the covariance operator \\(T_{\\rho}\\) on \\(\\mathcal{H}_{qp}\\) is the limit of \\(T_{\\rho_{\\lambda}}=F(\\Delta_{\\rho_{\\lambda}})\\) which will then be determined by \\(F(0)\\). I have no clue of which topology one should use, nor of which concrete object should be continuous. However, I believe in this idea.\n\n\n2023-10-23 the tracial states on type I factors determine the fields of covariances on rational tracial states\nAfter the seminar “Čencov meets Petz” for the Groupoid’s Group, I spoke with Fabio DC, and we realized that the way to get the covariance on any rational trace on a finite-dimensional algebra \\(\\mathscr{A}\\cong\\bigoplus_{j=1}^{N}\\mathcal{B}(\\mathcal{H}_{j})\\) is to immerse such an algebra into \\(\\mathcal{B}(\\mathcal{H})\\) with \\(\\mathcal{H}=\\bigoplus_{k=1}^{NL}\\mathcal{K}_{k}\\) with \\(\\mathcal{K}_{k}\\cong\\bigotimes_{j=1}^{N}\\mathcal{H}_{j}\\). When \\(L=1\\), \\(\\mathcal{B}(\\mathcal{H}_{1})\\) is sent into \\(\\mathcal{B}(\\mathcal{K}_{1})\\) setting the others to the identity, and proceeding in analogy. When \\(L&gt;1\\), I choose which \\(\\mathcal{B}(\\mathcal{H}_{j})\\) gets represented multiple times.\n\n\n2023-10-17 is normality necessary in NCP?\nWhile giving the seminar on “Čencov meets Petz” for the Groupoid’s Group, I realized that maybe the normality condition in \\(\\mathsf{NCP}\\) is not really necessary. At the level of the GNS Hilbert space, even if \\(\\rho\\) is not faithful, we can characterize what would be \\(\\mathcal{H}_{\\rho}^{qp}\\) as the kernel of the Tomita operator \\(S_{\\rho}\\). If everything works, dropping normality may be useful because we recover a lot of conditional expectations into \\(\\mathcal{B}(\\mathcal{H})\\) with \\(\\mathcal{H}\\) separable.\n\n\n2023-10-06 covariance at a faithful state is determined by the covariance on the GNS Hilbert space\nI believe the covariance at a normal, faithful state \\(\\rho\\) with separable GNS space \\(\\mathcal{H}_{\\rho}\\) is determined by the covariance at the normal, faithful state \\(\\tilde{\\rho}\\) on \\(\\mathcal{B}(\\mathcal{H}_{\\rho})\\) extending \\(\\rho\\). The idea is to show that the representation of \\(\\mathcal{A}\\) in \\(\\mathcal{B}(\\mathcal{H}_{\\rho}\\)) gives a subalgebra which is invariant w.r.t. the modular flow of \\(\\tilde{\\rho}\\) and then use invariance w.r.t. congruent embeddings/conditional expectations.\nOn 2023-11-04, I wrote somewhere that, together with Fabio DC, we realized this idea is wrong, but I did not write why. It would be nice to know.\n\n\n2023-09-25 GNS-bundle and pullback of fields of covariances\nMotivated by the chat with Laura Elena González Bravo, I thought that a good way to start reasoning about how to “pull back” covariance fields from NCP to a statistical groupoid is that of building a kind of pull-back bundle whose fiber is generated by the GNS Hilbert space. I believe taking the base of the statistical groupoid as the base of this fiber bundle is the easier thing to do, but then I have no idea about the role played by morphisms. Perhaps, the bundle gives the “metric” and the morphisms give symmetries? Would it be possible to build a bundle with the GNS and the morphism space?\n\n\n2023-09-21 centralizers, countably-decomposable algebras, and infinite-dimensional Čencov-Petz\nMentre guidavo mi sono reso conto che il centralizzatore \\(\\mathscr{M}_{\\rho}\\) di uno stato normale \\(\\rho\\) su una W*-algebra \\(\\mathscr{A}\\) deve necessariamente essere un’algebra “countably-decomposable” perché lo stato \\(\\rho\\) è fedele su \\(\\mathscr{M}_{\\rho}\\). Inoltre, \\(\\rho\\) è una traccia su \\(\\mathscr{M}_{\\rho}\\), e quindi il centralizzatore è sempre un’algebra molto particolare. La congettura che abbiamo formulato con Fabio DC è che i centralizzatori con le loro tracce sono le controparti “corrette” dei sistemi classici di Cencov. Cosa comporta il fatto che tutte queste algebre sono “countably decomposable”? Tenendo in conto che per queste algebre esistono “conditional expectations” normali in \\(\\mathcal{B}(\\mathcal{H})\\), e tenendo in conto il teorema di Takesaki sulle “conditional expectations”, forse c’è speranza di fare qualcosa anche a dimensione infinita.\n\n\n2023-09-12 gli stati tracciali mantengono l’unicità a la Čencov\nAnche oggi ho fatto un po’ di ricerca! Tralasciando una mezz’ora passata a cercare di cavare un ragno dal buco delle superalgebre di Jordan, insieme a Fabio DC abbiamo pensato una cosa interessante: la parte classica della categoria NCP non è data dalle algebre commutative, ma dagli stati tracciali. È per gli stati tracciali che il teorema di Čencov deve essere vero. Speriamo di riuscire a finire questo lavoro.",
    "crumbs": [
      "Home",
      "Miscellanea",
      "Timeline on fields of covariances"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "I was born and raised in Napoli where I grew up eating pizza, reading books and manga, playing guitar, and, most importantly, wondering about abstractness during my BSc, MSc, and PhD at the Università degli Studi di Napoli Federico II under the supervision of G. Marmo."
  },
  {
    "objectID": "index.html#academic-trajectory",
    "href": "index.html#academic-trajectory",
    "title": "Home",
    "section": "Academic trajectory",
    "text": "Academic trajectory\n\nJuly 2021 - Present: Investigador distinguido in the Department of Mathematics at the Universidad Carlos III de Madrid (UC3M), in the Q-Math research group.\nJuly 2018 - June 2021: Postdoctoral researcher in J. Jost’s group at the Max Planck Institute for the Mathematics in the Sciences (MIS-MPG)."
  },
  {
    "objectID": "index.html#funding-projects-et-similia",
    "href": "index.html#funding-projects-et-similia",
    "title": "Home",
    "section": "Funding, projects et similia",
    "text": "Funding, projects et similia\n\nJanuary 2025 - Young principal researcher of the project “QUITEMAD”, 01/2025 -12/2027.\nMay 2022 - Secondary proposer of the COST project “CaLISTA”, 10/2022 - 10/2026.\nNovember 2021 - Principal researcher of the project “Classical and Quantum Information Theory and Functional Analysis: Foundations and applications“, 11/2021 - 12/2024.\nSeptember 2021 - Member of the research team of the project “GRUPOIDES, ALGEBRAS DE VON NEUMANN Y LOS FUNDAMENTOS MATEMATICOS DE LA MECANICA CUANTICA: TEORIA Y APLICACIONES“, 09/2021 - 08/2025.\nApril 2021 - Italian national scientific habilitation (ASN) in Theoretical Physics and Phyiscs of Fundamental Interactions, settore concorsuale 02/A2.\nSeptember 2020 - Ayuda Beatriz Galindo for a (4+1)-year position at the UC3M."
  },
  {
    "objectID": "index.html#editorial-work",
    "href": "index.html#editorial-work",
    "title": "Home",
    "section": "Editorial work",
    "text": "Editorial work\n\nFebruary 2021 - Present: Editor for the European Physical Journal Plus.\nNovember 2020 - Present: Review Editor for Frontiers in Physics and Frontiers in Applied Mathematics and Statistics."
  },
  {
    "objectID": "index.html#digital-places-where-i-appear",
    "href": "index.html#digital-places-where-i-appear",
    "title": "Home",
    "section": "Digital places where I appear",
    "text": "Digital places where I appear\n\nAcademia        \narXiv         \nGoogle Scholar         \nINSPIRE         \nLinkedIn\nLoop         \nMathSciNet        \nORCID         \nWeb of Science          \nResearchGate\nSciProfiles         \nSciRate         \nScopus         \nSemantic Scholar"
  },
  {
    "objectID": "index.html#music",
    "href": "index.html#music",
    "title": "Home",
    "section": "Music",
    "text": "Music\nI like music and I used to play guitar.\nI recorded the EP Via Soffritto  neapolitan rock-blues band ArtRePaz. I was the lead guitar and we had a lot of fun before the bitterness of life chewed to death our rock ’n roll dreams.\nI also have a Bandcamp page I seldomly update. There is no logic, nor beauty, nor any kind of sense in it, but I like it nonetheless."
  },
  {
    "objectID": "teaching/calculus-uc3m-robotics/2.1 From 1 to many dimensions.html",
    "href": "teaching/calculus-uc3m-robotics/2.1 From 1 to many dimensions.html",
    "title": "From 1 to many dimensions",
    "section": "",
    "text": "From 1 to many dimensions\nLet’s start recalling that the set \\(\\mathbb{R}^{n}\\) of \\(n\\)-tuple of real numbers is also a vector space. Roughly speaking, this means that we can “sum” points (or better, vectors) with each other according to \\[\n\\mathbf{v} + \\mathbf{w}=(v_{1},\\cdots,v_{n}) + (w_{1},\\cdots,w_{n})= (v_{1}+w_{1},\\cdots,v_{n}+w_{n}).\n\\] We can also multiply every vector \\(\\mathbf{v}\\in\\mathbb{R}^{n}\\) with a real number \\(\\alpha\\in\\mathbb{R}\\) obtaining \\(\\alpha\\,\\mathbf{v}=(\\alpha,v_{1},\\cdots,\\alpha \\, v_{n})\\). Moreover, there is a vector in \\(\\mathbb{R}^{n}\\) that is peculiar: the zero vector \\(\\mathbf{0}=(0,\\cdots, 0)\\). Indeed, a direct check shows that \\(\\mathbf{v}+\\mathbf{0}=\\mathbf{0}+\\mathbf{v}=\\mathbf{v}\\) for every \\(\\mathbf{v}\\in\\mathbb{R}^{n}\\).\nWhen \\(n=3\\), we can think of \\(\\mathbb{R}^{n=3}\\) as a (local) model of the physical space we live in. Establishing a correspondence between the components of the vectors and physical reference axis, and declaring the origin of this physical reference system (i.e. declaring where the zero vector \\(\\mathbf{0}=(0,0,0)\\) is located) we can think of a vector \\((x,y,z)=\\mathbf{v}\\in\\mathbb{R}^{3}\\) as a line joining the point in space with coordinates \\((x,y,z)\\) with the origin \\((0,0,0)\\) of the reference physical system (more on this below).\nIt follows from Pitagora’s theorem that the length of that line is just \\(\\sqrt{x^{2} + y^{2} + z^{2}}\\), and we can also look at this lenght as the distance between \\((x,y,z)\\) and \\((0,0,0)\\). We can generalize this picture to obtain the distance between \\(\\mathbf{v}=(x,y,z)\\) and \\(\\mathbf{v}_{0}=(x_{0},y_{0},z_{0})\\) according to\n\\[\nd(\\mathbf{v},\\mathbf{v}_{0})=\\sqrt{(x-x_{0})^{2} + (y-y_{0})^{2} + (z-z_{0})^{2}}.\n\\]\nWhen \\(n=1\\) we [[1.2 Limits of scalar functions|already found]] a similar expression for the distance between two points on the line, and we used it in the definition of limits. Something analogous happens for multivariable functions.\nFor arbitrary \\(n\\), the distance between two points is defined as\n\\[\nd(\\mathbf{v},\\mathbf{v}_{0})=\\sqrt{\\sum_{j=1}^{n}\\,(x^{j} - x_{0}^{j})^{2}},\n\\]\nwhere \\(\\mathbf{v}=(x^{1},\\cdots x^{n})\\) and \\(\\mathbf{v}_{0}=(x^{1}_{0},\\cdots x^{n}_{0})\\). The distance between \\(\\mathbf{v}\\) and \\(\\mathbf{0}\\) is also called the modulus (or norm) of \\(\\mathbf{v}\\) and it is also denoted as \\(||\\mathbf{v}||\\). It follows that\n\\[\n||\\mathbf{v}-\\mathbf{w}||=d(\\mathbf{v},\\mathbf{w})\n\\]\nThe couple \\((\\mathbb{R}^{n},||\\cdot ||)\\) will be referred to as the \\(n\\)-dimensional Euclidean space. An important property of the norm \\(\\Vert\\cdot\\Vert\\) is the so-called triangle inequality \\[\n||\\mathbf{v} + \\mathbf{w}||\\leq ||\\mathbf{v}|| + ||\\mathbf{w}||,\n\\] holding for all \\(\\mathbf{v},\\mathbf{w}\\in\\mathbb{R}^{n}\\).\nThe multidimensional analogue of an [[1.2 Limits of scalar functions#^19990d|open/closed interval]] is the open/closed ball.\n\n[!defn] Definition: open/closed ball We define the open ball centered at \\(\\mathbf{v}_{0}\\in\\mathbb{R}^{n}\\) with radius \\(r&gt;0\\) to be the set \\[\nB_{\\mathbf{v}_{0},r}:=\\left\\{\\mathbf{v}\\in \\mathbb{R}^{n}\\,|\\quad ||\\mathbf{v}-\\mathbf{v}_{0}||=d(\\mathbf{v},\\mathbf{v}_{0}) &lt; r \\right\\}.\n\\] If instead of \\(&lt;\\) we use \\(\\leq\\), we obtain the closed ball denoted by \\(\\overline{B_{\\mathbf{v}_{0},r}}\\).\n\n\n[!defn] Definition: interior and boundary points of a set Let \\(A\\) be a subset of \\(\\mathbb{R}^{n}\\). A point \\(\\mathbf{x}_{0}\\in A\\) is called an interior point of \\(A\\) if there is \\(r&gt;0\\) such that the open ball \\(B_{\\mathbf{x}_{0},r}\\) is all contained in \\(A\\). A point \\(\\mathbf{x}_{0}\\in \\mathbb{R}^{n}\\) is called a boundary point of \\(A\\) if the open ball \\(B_{\\mathbf{x}_{0},r}\\) contains at least one point which is not ín \\(A\\) for every \\(r&gt;0\\).\n\n^ceea9b\n\n[!rem] Remark Note that a boundary point of \\(A\\) does not necessarily belong to \\(A\\) itself. For instance, every point \\(\\mathbf{x}_{0}\\) such that \\(||\\mathbf{x} - \\mathbf{x}_{0}||=3\\) is a boundary point of \\(A=B_{\\mathbf{x},3}\\) which does not belong to \\(A\\).\n\n\n[!note] Definition (Closed and open sets) A closed set \\(C\\subseteq\\mathbb{R}^{n}\\) is a set that contains all its boundary points, while an open set \\(O\\subseteq \\mathbb{R}^{n}\\) is a set which is the complement of a closed set. The intersection of closed sets is still a closed set, while the union of open sets is still an open set.\n\nIt turns out that the Euclidean distance can be written in terms the so-called scalar (dot) product: \\[\n\\mathbf{v}\\cdot\\mathbf{w}:=\\sum_{j=1}^{n}v^{j}w^{j}.\n\\] The scalar product allows to introduce a notion of orthogonality among vectors by declaring two vector orthogonal, say \\(\\mathbf{v}\\perp\\mathbf{w}\\), whenever their dot product is vanishes.\nThere is an intimate connection between the scalar product and the Euclidean distance introduced above given by the formula \\[\nd(\\mathbf{v},\\mathbf{w})=||\\mathbf{v} - \\mathbf{w}||=\\sqrt{(\\mathbf{v} - \\mathbf{w})\\cdot (\\mathbf{v} - \\mathbf{w})} ,\n\\] which can be directly checked.\nHaving familiarized with the basics of scalar functions, we are now interested in understanding the behavior of functions whose input space is \\(\\mathbb{R}^{n}\\) with \\(n\\geq 1\\), and whose output space is \\(\\mathbb{R}^{m}\\) with \\(m\\geq 1\\).\nWhen \\(m=1\\), we obtain the so-called multivariable scalar functions, or simply multivariable functions (note that scalar functions are a subfamily of multivariable functions). A typical example of multivariable function is the temperature function \\(T\\colon \\mathbb{R}^{3}\\rightarrow \\mathbb{R}\\) that assigns to every point in our local model of the physical space a number representing the temperature at that point.\nWhen \\(n=1\\) but \\(m\\geq 1\\), we obtain the so-called vector functions. The typical example is the position function \\((D,\\mathbb{R},\\mathbf{r},\\mathbb{R}^{3})\\) that assigns the position of a particle in our local model of physical space \\(\\mathbb{R}^{3}\\) to a point in our local model of time \\(\\mathbb{R}\\).\nWhen both \\(n,m\\geq 1\\), we obtain the so-called multivariable vector functions (note that all previously mentioned classes of functions are particular cases of multivariable vector functions). The typical example is the electric field \\((D,\\mathbb{R}^{3},\\mathbf{E},\\mathbb{R}^{3})\\) that assigns to each point in our local model of physical space \\(\\mathbb{R}^{3}\\) the electric field vector \\(\\mathbf{E}=(E_{x},E_{y},E_{z})\\) at that point.\n\n\nPolar coordinates\nThe main idea behind the change of variables/coordinates is that of changing how we represent “something”.\nFor concreteness, let us consider how can we change the representation of points in the plane \\(\\mathbb{R}^{2}\\). It is well-known that we can represent/describe a point \\(\\mathbf{p}\\) in the plane using two numbers \\((p_{x},p_{y})\\equiv(x,y)\\) by introducing two mutually orthogonal lines and considering the orthogonal projections of the point on said lines. By doing this, we introduced the so-called Cartesian coordinates on the plane. With a little bit of immagination, this procedure can be naturally extended from the plane \\(\\mathbb{R}^{2}\\) to an arbitrary \\(\\mathbb{R}^{n}\\).\n\n[!rem] Remark The way we “built” Cartesian coordinates exploits the notion of orthogonality which is connected with the notion of dot product discussed above. However, orthogonality is not needed, and we could develop a Cartesian coordinate system which is not orthogonal.\n\nThe representation through Cartesian coordinates is very intuitive but it is not the only one possible. Indeed, if we can take a look at the graphical representation of a point \\(\\mathbf{p}\\) in a plane with Cartesian coordinates \\((p_{x},p_{y})\\equiv(x,y)\\) , we may realize that we can also represent \\(\\mathbf{p}\\) by means of two different numbers: the distance \\(r=d(\\mathbf{p},\\mathbf{0})\\) with respect to the origin, and the angle \\(\\theta\\) that the vector determined by \\(\\mathbf{p}\\) makes with the horizontal line. The explicit expressions of \\(r\\) and \\(\\theta\\) in terms of Cartesian coordinates are \\[\nr(x,y)=\\sqrt{x^{2}+y^{2}}\n\\] and \\[\n\\theta(x,y) = \\mathrm{atan}2(x,y)= \\left\\{\\begin{matrix}\\arctan\\left(\\frac{y}{x}\\right) & \\quad x&gt;0 \\\\\n\\arctan\\left(\\frac{y}{x}\\right) + \\pi & \\quad x&lt;0,\\quad x\\geq 0 \\\\\n\\arctan\\left(\\frac{y}{x}\\right) - \\pi & \\quad x&lt;0,\\quad x&lt; 0 \\\\\n\\frac{\\pi}{2} & x=0,\\quad y&gt; 0 \\\\\n-\\frac{\\pi}{2} & x=0,\\quad y&lt;0 ,\\end{matrix} \\right.\n\\] and the function \\(\\mathrm{atan}2\\) is thoroughly discussed here on Wikipedia.\nBy introducing \\(r\\) and \\(\\theta\\), we introduced the so-called polar coordinates in the plane. There are two important things we must comment:\n\nthe origin \\(\\mathbf{0} =(0,0)\\) can not be represented using polar coordinates, and we conclude that we must specify a domain of application when changing coordinates (in particular, Cartesian coordinates can be used for every point in the plane, while polar coordinates can not be used to represent the origin);\nthe representation in polar coordinates is expressed through the representation in Cartesian coordinates, and we conclude that changes of coordinates are always intended with respect to some other coordinates system.\n\nTherefore, we can look at the change of coordinates from the Cartesian to the polar system as a function \\(\\psi\\colon \\mathbb{R}^{2}-\\{(0,0)\\}\\rightarrow \\mathbb{R}^{2}\\) given by \\[\n\\psi(x,y)=(r(x,y),\\theta(x,y))\n\\] where the expressions for \\(r\\) and \\(\\theta\\) are given above. Note that the range of \\(\\psi\\) is not the whole \\(\\mathbb{R}^{2}\\). We can define an inverse of \\(\\psi\\) if we consider the subset \\(A=(0,+\\infty)\\times[0,2\\pi)\\subset \\mathbb{R}^{2}\\) and set \\(\\psi^{-1}\\colon A\\rightarrow \\mathbb{R}^{2}\\) by \\[\n\\psi^{-1}(r,\\theta)=(x(r,\\theta)=r\\cos(\\theta), y(r,\\theta)=r\\sin(\\theta)) .\n\\] Of course, the origin \\((0,0)\\) is not in the range of \\(\\psi^{-1}\\) because it is not in the domain space of \\(\\psi\\).\n\n\nCylindrical coordinates\n^1f484e\nThe idea behind polar coordinates can be extended from \\(\\mathbb{R}^{2}\\) to \\(\\mathbb{R}^{3}\\) in at least two ways. Specifically, we can decide to use a polar coordinate system for the \\(xy\\)-plane (\\(z=0\\)) and a Cartesian coordinate system for the \\(z\\)-axis obtaining the so-called cylindrical coordinates determined by the function \\(\\psi\\colon \\mathbb{R}^{3}-\\{(0,0,0)\\} \\rightarrow \\mathbb{R}^{3}\\) given by\n\\[\n\\psi(x,y,z)=\\left\\{\\begin{matrix} \\rho(x,y,z)=\\sqrt{x^{2} + y^{2} } \\\\ \\\\ \\theta(x,y,z)= \\mathrm{atan}2(x,y) \\\\ \\\\ z(x,y,z)=z ,\\end{matrix} \\right.\n\\]\n^432b85\nand with inverse \\(\\psi^{-1}\\colon A\\times \\mathbb{R}\\rightarrow \\mathbb{R}^{3}\\) given by\n\\[\n\\psi^{-1}(\\rho,\\theta,z)=\\left\\{ \\begin{matrix} x(\\rho,\\theta,z)=\\rho\\cos(\\theta) \\\\ \\\\ y(\\rho,\\theta,z)=\\rho\\sin(\\theta) \\\\ \\\\ z(\\rho,\\theta,z)=z , \\end{matrix}\\right.\n\\]\n^cbf7ee\nwith \\(A=(0,+\\infty)\\times[0,2\\pi)\\subset \\mathbb{R}^{2}\\).\n\n\nSpherical coordinates\n^b5d3f4\nThe second way to generalize polar coordinates is given by the so-called spherical coordinates (note that the role of \\(\\theta\\) and \\(\\varphi\\) are exchanged with respect to the Wikipedia article) determined by the function \\(\\psi\\colon \\mathbb{R}^{3}-\\{(0,0,0)\\} \\rightarrow \\mathbb{R}^{3}\\) given by\n\\[\n\\psi(x,y,z)=\\left\\{\\begin{matrix} \\rho(x,y,z)=\\sqrt{x^{2} + y^{2} + z^{2}} \\\\ \\\\\n\\theta(x,y,z)= \\mathrm{atan}2(x,y) \\\\ \\\\\n\\varphi(x,y,z)=\\arccos\\left(\\frac{z}{\\sqrt{x^{2} + y^{2} + z^{2}}}\\right)\\end{matrix}\\right.,\n\\]\nand with inverse \\(\\psi^{-1}\\colon B\\rightarrow \\mathbb{R}^{3}\\) given by\n\\[\n\\psi^{-1}(\\rho,\\theta,\\varphi)=\\left\\{\\begin{matrix} x(\\rho,\\theta,\\varphi)=\\rho\\cos(\\theta)\\sin(\\varphi) \\\\ \\\\\ny(\\rho,\\theta,\\varphi)=\\rho\\sin(\\theta)\\sin(\\varphi) \\\\ \\\\\nz(\\rho,\\theta,\\varphi)=\\rho \\cos(\\varphi) , \\end{matrix} \\right.\n\\]\n^1ce5e7\nwith \\(B=(0,+\\infty)\\times[0,2\\pi)\\times[0,\\pi]\\subset \\mathbb{R}^{2}\\).\nOnce we are here, we can let our creativity run wild and invent as many change of coordinates we want. However, not all of them can be directly interpreted geometrically like Cartesian, polar, cylindrical, and spherical. We will discuss some more examples when investigating integrals.\n\n\nChange of coordinates and functions\nSuppose we know the expression of the function \\((A,\\mathbb{R},f,\\mathbb{R}\\) in some coordinate system (e.g., Cartesian coordinates). What happens to \\(f\\) if we change coordinates?\nConsider the function \\((\\mathbb{R}^{2},\\mathbb{R}^{2},f,\\mathbb{R})\\) that in Cartesian coordinates reads \\[\nf(x,y)=y .\n\\] If we consider polar coordinates \\((r,\\theta)\\), then we know that we can express \\(x\\) and \\(y\\) as functions of \\(r\\) and \\(\\theta\\) (as long as we exclude the origin \\((0,0)\\)) by means of the function \\((A,\\mathbb{R}^{2},\\psi^{-1},\\mathbb{R}^{2}\\) given by \\[\n\\psi^{-1}(r,\\theta)=(x(r,\\theta)=r\\cos(\\theta), y(r,\\theta)=r\\sin(\\theta)),\n\\] with \\(A=(0,+\\infty)\\times[0,2\\pi)\\subset \\mathbb{R}^{2}\\). Therefore, we can express \\(y\\) as \\(r\\sin(\\theta)\\), obtaining the expression of \\(f\\) in polar coordinates, which is written \\[\nf\\circ\\psi^{-1}(r,\\theta)=r\\sin(\\theta) .\n\\] The generalization to other coordinates system follows the same idea.\nWe thus conclude that changing the coordinate representation of a function is nothing but composing the original function with the multivariable vector function implementing the change of coordinates.",
    "crumbs": [
      "Home",
      "Teaching",
      "Calculus - Robotics Engineering - UC3M",
      "From 1 to many dimensions"
    ]
  },
  {
    "objectID": "teaching/calculus-uc3m-robotics/2.4 Extrema of multivariable functions.html",
    "href": "teaching/calculus-uc3m-robotics/2.4 Extrema of multivariable functions.html",
    "title": "Critical points and local extrema for multivariable scalar functions",
    "section": "",
    "text": "Critical points and local extrema for multivariable scalar functions\nWe now turn our attention to the problem of finding local and global extrema for multivariable scalar functions.\n\n[!defn] Definition: local extrema Consider the function \\(f\\colon D\\subseteq\\mathbb{R}^{n}\\rightarrow\\mathbb{R}\\) and the [[2.1 From 1 to many dimensions#^ceea9b|interior point]] \\(\\mathbf{x}_{0}\\in D\\): 1) it is said that \\(f\\) has a local maximum at \\(\\mathbf{x}_{0}\\) if there is \\(\\epsilon&gt;0\\) such that \\(f(\\mathbf{x}_{0})&gt;f(\\mathbf{x})\\) for all \\(\\mathbf{x}\\in B_{\\mathbf{x}_{0},\\epsilon}\\); 2) it is said that \\(f\\) has a local minimum at \\(\\mathbf{x}_{0}\\) if there is \\(\\epsilon&gt;0\\) such that \\(f(\\mathbf{x}_{0})&lt;f(\\mathbf{x})\\) for all \\(\\mathbf{x}\\in B_{\\mathbf{x}_{0},\\epsilon}\\) 3) it is said that \\(f\\) has a local extrema at \\(\\mathbf{x}_{0}\\) if \\(\\mathbf{x}_{0}\\) is either a local maximum or a local minimum of \\(f\\).\n\nFor a multivariable scalar function \\(f\\colon D\\subseteq\\mathbb{R}^{n}\\rightarrow\\mathbb{R}\\), it is possible to introduce a notion of derivative that takes into account how \\(f\\) changes along an arbitrary direction in \\(\\mathbb{R}^{n}\\) determined by the vector \\(\\mathbf{v}\\in\\mathbb{R}^{n}\\). This notion of derivative is called directional derivative and is defined below.\n\n[!defn] Definition: directional derivative Consider the function \\(f\\colon D\\subseteq\\mathbb{R}^{n}\\rightarrow\\mathbb{R}\\), an interior point \\(\\mathbf{x}_{0}\\in D\\), and the vector \\(\\mathbf{v}\\in\\mathbb{R}^{n}\\). The directional derivative \\(\\mathbf{D}_{\\mathbf{v}}f(\\mathbf{x}_{0})\\) of \\(f\\) at \\(\\mathbf{x}_{0}\\) along the direction \\(\\mathbf{v}\\) is defined as \\[\n\\mathbf{D}_{\\mathbf{v}}f(\\mathbf{x}_{0}):=\\lim_{t\\rightarrow 0}\\,\\frac{f(\\mathbf{x}_{0} + t\\mathbf{v}) - f(\\mathbf{x}_{0})}{t}=\\frac{\\mathrm{d} f(\\mathbf{x}_{0} + t\\mathbf{v})}{\\mathrm{d}t}(0) ,\n\\] where the right-hand side is thought of as a vector function of one variable \\(t\\in\\mathbb{R}\\). A moment of thought reveals that the directional derivative of \\(f\\) at \\(\\mathbf{x}_{0}\\) along the direction \\(\\mathbf{e}_{j}\\) of the standard basis coincides with the \\(j\\)-th partial derivative of \\(f\\) at \\(\\mathbf{x}_{0}\\) as defined [[2.3 Derivatives and Taylor’s theorem for multivariable functions#^c8b7dc|here]].\n\nFocusing on the vector function \\(\\gamma\\colon D'\\subseteq \\mathbb{R} \\rightarrow \\mathbb{R}^{n}\\) given by \\[\n\\gamma(t)=\\mathbf{x}_{0} + t\\mathbf{v},\n\\] where \\(D'\\) is determined by the condition \\(\\gamma(D')\\subseteq D\\), a direct application of the [[2.3 Derivatives and Taylor’s theorem for multivariable functions#^9ceaa7|chain rule]] for the function \\(f\\circ \\gamma\\) allows to prove the following proposition that gives a direct link between directional derivatives and the gradient vector.\n\n[!important] Proposition: directional derivatives and the gradient Let \\(f\\colon D\\subseteq\\mathbb{R}^{n}\\rightarrow \\mathbb{R}\\) be differentiable in \\(D\\), and \\(\\mathbf{x}_{0}\\in D\\) be an interior point. Then, it holds \\[\n\\mathbf{D}_{\\mathbf{v}}f(\\mathbf{x}_{0})= \\mathbf{v}\\,\\cdot\\,\\nabla f(\\mathbf{x}_{0}) ,\n\\] where \\(\\cdot\\) is the scalar product in \\(\\mathbb{R}^{n}\\).\n\n\n[!rem] Remark: gradient vector as direction of fastest increase For a differentiable function \\((D,  \\mathbb{R}^{n},f, \\mathbb{R})\\), since the scalar product is maximum when \\(\\mathbf{v}\\) is parallel to \\(\\nabla f(\\mathbf{x}_{0})\\), we conclude that the gradient vector points in the direction along which \\(f\\) increases the fastest.\n\nMotivated by the previous remark, we introduce the notion of critical point in analogy with the [[1.6 Derivatives and extrema of scalar functions#^d2fcd0|one-variable scalar case]], and we state how it is connected to local extrema.\n\n[!defn] Definition: critical point Given the function \\((D,\\mathbb{R}^{n},f,\\mathbb{R})\\), an interior point \\(\\mathbf{x}_{0}\\in D\\) is called a critical point if either \\(\\mathbf{D}f(\\mathbf{x}_{0})=\\nabla f(\\mathbf{x}_{0})=0\\) or it does not exist.\n\n\n[!important] Proposition: local extrema and critical points If \\((D,\\mathbb{R}^{n},f, \\mathbb{R})\\) has a local extremum at \\(\\mathbf{x}_{0}\\in D\\) then \\(\\mathbf{x}_{0}\\) is a critical point of \\(f\\).\n\n\n[!attention] Remark The converse of the previous proposition is not true, already for [[1.6 Derivatives and extrema of scalar functions#^f4bbab|scalar functions of one variable]].\n\nIn the genuinely multivariable case (i.e., when the output space is \\(\\mathbb{R}^{n}\\) with \\(n&gt;1\\)), it is difficult to establish if a critical point is a local extrema. Intuitively speaking, this is due to the fact that a local extrema \\(\\mathbf{x}_{0}\\) has to be an extrema along every direction pointing to it, and this is a very demanding request. For instance, let us consider the function \\(f\\colon \\mathbb{R}^{2}\\rightarrow\\mathbb{R}\\) given by \\[\nf(x,y)=x^{2} - y^{2} .\n\\] It is immediate to check that the partial derivatives are \\[\nf_{x}=2x,\\qquad f_{y}=-2y,\n\\] so that \\((0,0)\\) is a critical point because \\(\\nabla f(0,0)=(0,0)\\). However, if we investigate the behaviour of \\(f\\) along the \\(x\\)-axis determined by \\(y=0\\) we find \\(f(x,0)=x^{2}\\) which has a global minimum at \\(x=0\\), while the behaviour of \\(f\\) along the \\(y\\)-axis gives back \\(f(0,y)=-y^{2}\\) which has a global maximum at \\(y=0\\). Consequently, \\((0,0)\\) can not be a local extrema for \\(f\\).\nThis situation is very typical for multivariable scalar functions, and, as it is clear from the example above, it applies also to very simple functions.\nA possible way to investigate the nature of a critical point is associated with the notion of [[2.3 Derivatives and Taylor’s theorem for multivariable functions#^1b4383|second order partial derivative]]. In particular, multivariable scalar functions that are \\(C^{2}\\) are important because of the following proposition.\n\n[!important] Proposition Let \\(f\\colon D\\subseteq \\mathbb{R}^{n}\\rightarrow\\mathbb{R}\\) be \\(C^{2}\\) on \\(A\\subseteq D\\), and let \\(\\mathbf{x}_{0}\\in A\\) be a critical point (which can only mean \\(\\nabla f(\\mathbf{x}_{0})=\\mathbf{0}\\) because all the partial derivatives exist when \\(f\\) is \\(C^{2}\\)). If the multivariable scalar function \\(Hf_{\\mathbf{x}_{0}}\\colon\\mathbb{R}^{n}\\rightarrow\\mathbb{R}\\) given by \\[\nHf_{\\mathbf{x}_{0}}(\\mathbf{h}):=(h^{1}\\,\\cdots\\, h^{n})\\left(\\begin{matrix}f_{11}|_{\\mathbf{x}_{0}}& \\cdots & f_{1n}|_{\\mathbf{x}_{0}} \\\\ & & \\\\ \\vdots & &\\vdots \\\\ & & \\\\ f_{n1}|_{\\mathbf{x}_{0}} & \\cdots & f_{nn}|_{\\mathbf{x}_{0}}\\end{matrix}\\right)\\,\\left(\\begin{matrix}h^{1} \\\\ \\\\\\vdots\\\\ \\\\h^{n} \\end{matrix}\\right)\n\\] is strictly positive for all \\(\\mathbf{0}\\neq\\mathbf{h}\\in\\mathbb{R}^{n}\\) then \\(\\mathbf{x}_{0}\\) is a local minimum; if it is strictly negative for all \\(\\mathbf{0}\\neq\\mathbf{h}\\in\\mathbb{R}^{n}\\) then \\(\\mathbf{x}_{0}\\) is a local maximum; if it takes both positive and negative values, and the Hessian matrix \\(Hf_{\\mathbf{x}_{0}}\\) is invertible, then \\(\\mathbf{x}_{0}\\) is a saddle point.\n\n^36f25b\n\n[!rem] Remark To check which type of local extrema is \\(\\mathbf{x}_{0}\\), we can also investigate the determinants of all the square sub-matrices of the Hessian matrix containing the first element. If they are all positive, then \\(\\mathbf{x}_{0}\\) is a local minimum. If they alternate starting with a negative one, then \\(\\mathbf{x}_{0}\\) is a local minimum. If they are all different then \\(0\\), but do not follow the previous conditions, then \\(\\mathbf{x}_{0}\\) is a saddle point.\n\n^99ac31\n\n\nGlobal extrema\nLet us start with the definition of global extrema.\n\n[!defn] Definition: global extrema Consider the function \\(f\\colon D\\subseteq\\mathbb{R}^{n}\\rightarrow\\mathbb{R}\\) and \\(\\mathbf{x}_{0}\\in D\\): 1) it is said that \\(f\\) has a global maximum at \\(\\mathbf{x}_{0}\\) if \\(f(\\mathbf{x}_{0})&gt;f(\\mathbf{x})\\) for all \\(\\mathbf{x}\\in D\\); 2) it is said that \\(f\\) has a global minimum at \\(\\mathbf{x}_{0}\\) if \\(f(\\mathbf{x}_{0})&lt;f(\\mathbf{x})\\) for all \\(\\mathbf{x}\\in D\\); 3) it is said that \\(f\\) has a local extrema at \\(\\mathbf{x}_{0}\\) if \\(\\mathbf{x}_{0}\\) is either a global maximum or a global minimum of \\(f\\).\n\nUnderstanding the behaviour of arbitrary multivariable scalar functions is incredibly difficult. We need to make regularity assumptions on the type of functions we consider in order to be able to attack the problem. At this purpose, multivariable scalar functions that are continuous are particularly important because they admit global extrema when they are defined on sets that are closed and bounded.\n\n[!defn] Definition: bounded and closed sets The set \\(A\\subseteq \\mathbb{R}^{n}\\) is called bounded if there is \\(M\\in\\mathbb{R}\\) such that \\(||\\mathbf{x}||\\leq M\\) for every \\(\\mathbf{x}\\in A\\). The set \\(A\\subseteq \\mathbb{R}^{n}\\) is called closed if it contains all its boundary points.\n\n^cf7a0c\n\n[!important] Theorem: Extreme value theorem Given a closed and bounded set \\(D\\subset\\mathbb{R}^{n}\\), a continuous function \\(f\\colon D\\subset\\mathbb{R}^{n}\\rightarrow\\mathbb{R}\\) admits global maximum and global minimum.\n\n^d586b6\n\n[!attention] Remark: the level sets of continuous functions are closed Fixing \\(c\\in\\mathbb{R}\\), it can be proved that the level set \\[\nS=\\left\\{\\mathbf{x}\\in\\mathbb{R}^{n}\\,|\\quad g(\\mathbf{x})=c \\right\\}\n\\] of the continuous function \\(g\\colon D\\subseteq\\mathbb{R}^{n}\\rightarrow\\mathbb{R}\\) is always closed in \\(\\mathbb{R}^{n}\\).\n\nIn the following, we discuss two possible ways to determine global extrema for continuous functions defined on a closed and bounded domain whose boundary is of a particular type. To be able to discuss more general cases would require a level of sophistication that is not compatible with the little time we have at our disposal for the course. However, if you are interested, a first approach is to read carefully the whole chapter 3 of M-T, the whole chapter 16 of S-E-H, and then consult the references and external links in the Wikipedia page for “Multivariable calculus”. # Global extrema in 2D: parametrizable boundary\nHere, we discuss how to find the global extrema of a continuous function \\((D,\\mathbb{R}^{2},f,\\mathbb{R})\\) when \\(D\\) is closed and bounded, and the boundary \\(\\partial D\\) is the union of finitely many smooth curves (e.g., polygons, ellipses, and similar things).\nInstead of trying to describe a general theory, we deal with a specific example that allows to understand the general structure. We consider the function \\((D,\\mathbb{R}^{2},f,\\mathbb{R})\\), with \\(f(x,y)=x^{2} + y^{2}-2x\\), and with \\(D\\) the triangle with vertices \\(A\\equiv(0,0)\\), \\(B\\equiv(2,0)\\), and \\(C\\equiv(0,2)\\).\nSince \\(f\\) is continuous and \\(D\\) is a closed and bounded set, there will be global extrema, and we have to first look for local extrema inside \\(D\\), and then investigate the behaviour at the boundary.\nThe local extrema inside \\(D\\) can be found and classified according to the theory we [[2.4 Extrema of multivariable functions#^36f25b|developed before]]. We obtain the critical point \\((1,0)\\) which lies on the boundary of the triangle. Since \\(f\\) is a polynomial function, it is \\(C^2\\), and the Hessian matrix at \\((1,0)\\) reads \\[\\mathbf{H}f_{(1,0)}=\\left(\\begin{matrix}2 & 0 \\\\ 0 & 2\\end{matrix}\\right).\\] According to the [[2.4 Extrema of multivariable functions#^36f25b|proposition]] and [[2.4 Extrema of multivariable functions#^99ac31|remark]] above, we conclude that \\((1,0)\\) is a local minimum.\nConcerning the behavior on the boundary, we consider the open segment \\(AB\\), the open segment \\(BC\\), the open segment \\(AC\\), and then the three vertices.\nThe segment \\(AB\\) can be parametrized by the function \\(t\\mapsto(t,0)\\) with \\(t\\in(0,2)\\), so that the original function on this segment looks like \\(f(t,0)=t^{2}-2t\\). Following what we learned for [[1.6 Derivatives and extrema of scalar functions|scalar functions of one variable]], we obtain a minimum at \\(t=1\\), that is, at the point \\((1,0)\\), which is compatible with what we found before.\nThe segment \\(AC\\) can be parametrized by the function \\(t\\mapsto (0,t)\\) with \\(t\\in(0,2)\\), so that the original function on this segment looks like \\(f(0,t)=t^{2}\\). This function has a minimum at \\(t=0\\), which is also a corner point.\nThe segment \\(BC\\) can be parametrized by \\(t\\mapsto(2t,2(1-t))\\) with \\(t\\in(0,1)\\), and the original function on the segment looks like \\(f(2t,2(1-t))= 4t^{2} +4 +4t^{2}-8t-4t=8t^{2}-12t+4\\). This function has a minimum at \\(t=\\frac{3}{4}\\), that is, at the point \\((\\frac{3}{2},\\frac{1}{2})\\).\nArrived here, to find the global extrema of \\(f\\) in \\(D\\), we have to evaluate the function at all the local extrema in the interior of \\(D\\) (in this case, there is no such point), and at the boundary \\(\\partial D\\) (including the vertices): \\[\n\\begin{split}\nf(0,0) & =0 \\\\ & \\\\\nf(2,0) & = 0 \\\\ & \\\\\nf(0,2) & = 4 \\\\ & \\\\\nf(1,0) & = -1 \\\\ & \\\\\nf\\left(\\frac{3}{2},\\frac{1}{2}\\right)& =-\\frac{1}{2},\n\\end{split}\n\\] and we conclude that \\((1,0)\\) is the global minima, while \\((0,2)\\) is the global maxima. # Global extrema: Lagrange multipliers\nHere, we discuss the method of Lagrange multipliers to find the global extrema on \\(D\\subset A\\) of a continuous function \\((A,\\mathbb{R}^{n},f,\\mathbb{R})\\) with \\(A\\) an open set, \\(D\\) closed, bounded, with open interior, and the boundary \\(\\partial D\\) regular enough in a sense specified below.\nThe first condition we impose on \\(\\partial D\\) is that it is the level set of a smooth function \\((A,\\mathbb{R}^{n},g,\\mathbb{R})\\) with \\(A\\) open, that is, we assume: \\[\n\\partial D=\\left\\{\\mathbf{x}\\in \\mathbb{R}^{n}\\;|\\quad g(\\mathbf{x})=c\\right\\},\n\\] with \\(c\\in\\mathbb{R}\\).\n\n[!important] Proposition: Lagrange multipliers and level sets Let \\(A\\subseteq \\mathbb{R}^{n}\\rightarrow \\mathbb{R}\\) be open, and let \\(f,g \\colon A\\rightarrow\\mathbb{R}\\) be \\(C^{1}\\) on \\(A\\). Let \\(c\\in\\mathbb{R}\\), let \\(S\\) be the level set of \\(c\\) through \\(g\\): \\[\nS=\\left\\{\\mathbf{x}\\in \\mathbb{R}^{n}\\,|\\quad g(\\mathbf{x})=c\\right\\},\n\\] and assume \\(\\nabla g(\\mathbf{x}_{0})\\neq\\mathbf{0}\\) at \\(\\mathbf{x}_{0}\\in S\\).\nIf the restriction of \\(f\\) to \\(S\\), usually denoted as \\(f|_{S}\\), has a local maximum or local minimum at \\(\\mathbf{x}_{0}\\) then \\[\n\\nabla f(\\mathbf{x}_{0})=\\lambda \\nabla g (\\mathbf{x}_{0})\n\\] for some \\(\\lambda\\in \\mathbb{R}\\) (which can also be \\(0\\)).\n\n^df7f57\n\n[!note] Definition: Lagrange multipliers and critical points on level sets The number \\(\\lambda\\) in the previous proposition is known as Lagrange multiplier, while a point \\(\\mathbf{x}_{0}\\in S\\) for which the equation \\[\n\\nabla f(\\mathbf{x}_{0})=\\lambda \\nabla g (\\mathbf{x}_{0})\n\\] holds is referred to as a critical point of \\(f\\) on \\(S\\).\n\nWhen the level set \\(S\\) in the previous proposition is the set of [[2.1 From 1 to many dimensions#^ceea9b|boundary points]] \\(\\partial D\\) of a [[2.4 Extrema of multivariable functions#^cf7a0c|closed, bounded]], and connected (meaning it can not be partitioned into the disjoint union of non-empty open sets) set \\(D\\), then we can find the global maxima and global minima of the \\(C^{1}\\) function \\(f\\colon A\\subseteq\\mathbb{R}^{n}\\rightarrow \\mathbb{R}\\) on \\(D\\subset A\\) as follows:\n\nwe look for critical points in the interior of \\(D\\) using the gradient vector \\(\\nabla f\\);\nwe look for critical points on \\(\\partial D\\) using Lagrange multipliers;\nwe compute \\(f\\) at all the points we found and select those giving maximum and minimum values.\n\n\n[!attention] Remark This procedure works because \\(D\\) is closed an bounded, and \\(f\\) is continuous on \\(D\\) (because it is \\(C^{1}\\) on \\(A\\supset D\\)). If even one of the previous assumptions falls, the procedure outlined above will fail!\n\n\n[!exmp] Example Let \\(f\\colon\\mathbb{R}^{2}\\rightarrow \\mathbb{R}\\) be given by\n\\[\nf(x,y)=x^{2} - y^{2},\n\\]\nand let \\(D\\) be the closed disk with unit radius centered in \\((0,0)\\). The only critical point in the interior of \\(D\\) is \\((0,0)\\). However, following the [[2.4 Extrema of multivariable functions#^36f25b|Hessian matrix criteria]], we conclude the origin is neither a local minima, nor a local maxima. Concerning the boundary \\(\\partial D\\), we can write it as the level set of the function \\(g(x,y)=x^{2} + y^{2}\\) corresponding to the value \\(1\\). Following the proposition on [[2.4 Extrema of multivariable functions#^df7f57|Lagrange mutipliers]], we have to solve the system \\[\n\\left\\{\\begin{matrix}\\nabla f(x,y)=\\lambda \\nabla g(x,y) \\\\ x^{2} + y^{2} = 1\\end{matrix}\\right.\n\\] with respect to \\((x,y)\\) and \\(\\lambda\\). A direct computation shows that the previous system is equivalent to \\[\n\\left\\{\\begin{matrix} x=\\lambda x \\\\ y=-\\lambda y \\\\ x^{2} + y^{2} = 1\\end{matrix}\\right. .\n\\] We solve this system by first considering \\(y=0\\) and then we solve it for \\(y\\neq 0\\). If \\(y=0\\), then the last equation forces \\(x=\\pm 1\\) so that the first one implies \\(\\lambda =1\\). Therefore, we obtain two solutions: \\((x=1,y=0;\\lambda=1)\\) and \\((x=-1,y=0;\\lambda=1)\\). When \\(y\\neq 0\\), the second equation implies \\(\\lambda=-1\\) so that the first equation forces \\(x=0\\), but then the third equation implies \\(y=\\pm 1\\). Therefore, we obtain other two solutions: \\((x=0, y=1;\\lambda=-1)\\) and \\((x=0,y=-1;\\lambda=-1)\\). These four points are the only candidates to be global extrema for \\(f\\) on \\(\\overline{D}\\). If we evaluate \\(f\\) on them we discover that the first two points are global maxima while the second ones are global minima.\n\nAs a last remark, we note that the method of Lagrange multipliers can be used also in another context. Specifically, we may be interested in maximizing or minimizing a given function \\((D,\\mathbb{R}^{n},f,\\mathbb{R})\\) subject to a constraint encoded in the equation \\(g(\\mathbf{x})=c\\). In this case, however, we are not sure the maxima or minima exist because we are not dealing, in general, with a continuous function on a closed and bounded domain. Therefore, ad hoc considerations are necessary to determine whether the points selected by the method of Lagrange multipliers are indeed suitable maxima or minima.\n\n[!exmp] Example: minimization under a constraint We want to find, if possile, the minimum of \\(\\mathbb{R},\\mathbb{R}^{2},f,\\mathbb{R}\\) with \\[f(x,y)=x^{2} + (y-2)^{2}\\] subject to the constraint \\[\ng(x,y)=x^{2}-y^{2}.\\] Applying the method of Lagrange multipliers, we obtain the system \\[\\left\\{\\begin{matrix}x& = & \\lambda x \\\\ (y-2) & = & -\\lambda y \\\\ x^{2}-y^{2}& = &1.\\end{matrix}\\right.\\] The constraint implies \\(x\\neq 0\\), and thus we obtain the solutions \\((x=\\pm\\sqrt{2},y=1,\\lambda=1)\\). Since the constraint \\(g(x,y)=x^{2} - y^{2}=1\\) is an hyperbola, which is not bounded, and thus is not the boundary of a closed and bounded region, we do not know if either of the two points found above is a minimum. That is, we do not know if there is a solution to our problem. However, we note that \\(f(x,y)\\) is nothing but the square of the distance of the point \\((x,y)\\) from the point \\((0,2)\\), so that our problem can be stated as the search for the point on the hyperbola \\(x^{2}-y^{2}=1\\) which is the closest to the point \\((0,2)\\). Of course, because of geometrical considerations, this problem must have solution, and both the points found with the method of Lagrange multipliers are viable solutions.",
    "crumbs": [
      "Home",
      "Teaching",
      "Calculus - Robotics Engineering - UC3M",
      "Critical points and local extrema for multivariable scalar functions"
    ]
  },
  {
    "objectID": "teaching/calculus-uc3m-robotics/1.7 Exercises.html",
    "href": "teaching/calculus-uc3m-robotics/1.7 Exercises.html",
    "title": "1 Limits of scalar functions",
    "section": "",
    "text": "1 Limits of scalar functions\n\nCompute \\[\n\\lim_{x\\rightarrow x_{0}} f(x)= f(x_{0})\n\\] for \\(f(x)=c\\), \\(f(x)=x\\), and \\(f(x)=x^{2}\\).\n\n\n[!note]- Solution Let us start considering the constant function \\(f(x)=c\\) with \\(x\\in\\mathbb{R}\\), and prove that \\[\n\\lim_{x\\rightarrow x_{0}} f(x)= f(x_{0}).\n\\] Indeed, it immediately follows that \\(|f(x) - f(x_{0})|=0&lt;\\epsilon\\) for every \\(x\\in\\mathbb{R}\\) and every \\(\\epsilon&gt;0\\). Now, let us consider the function \\(f(x)=x\\), and prove that \\[\n\\lim_{x\\rightarrow x_{0}} f(x)= f(x_{0}).\n\\] Again, it is immediate to check that \\(|f(x) - f(x_{0})|=|x - x_{0}|\\), and thus, taking \\(\\delta_{\\epsilon}&lt; \\epsilon\\), we get that \\(|x - x_{0}|&lt;\\delta_{\\epsilon}\\) implies \\(|f(x) - f(x_{0})|&lt;\\epsilon\\).\n\n&gt; Finally, let us consider the function $f(x)=x^{2}$, and prove that \n&gt; $$ \\lim_{x\\rightarrow x_{0}} f(x)= f(x_{0}) $$\n&gt; for every $x_{0}\\in\\mathbb{R}$.  We have \n&gt; $$ |x^{2} - x_{0}^{2}| =|(x+x_{0})(x-x_{0})|\\leq |x + x_{0}||x-x_{0}| . $$\n&gt; Recall that we must check that $|f(x) - f(x_{0})|&lt; \\epsilon$ whenever $|x - x_{0}|&lt;\\delta$, with $\\delta&gt;0$ to be determined depending on $\\epsilon&gt;0$.  A direct computation shows that \n&gt; $$ | x+ x_{0}| =|x - x_{0} + 2x_{0}|\\leq |x - x_{0}| + 2|x_{0}| $$\n&gt; so that \n&gt; $$ |x^{2} - x_{0}^{2}|\\leq (|x - x_{0}| + 2|x_{0}|)\\,|x-x_{0}| &lt; (2|x_{0}| + \\delta_{\\epsilon})\\,\\delta_{\\epsilon}. $$\n&gt; Therefore, if \n&gt; $$ \\delta_{\\epsilon}^{2} + 2|x_{0}| \\delta_{\\epsilon} - \\epsilon&lt;0 $$\n&gt; we are done. By solving the previous [quadratic equation](https://en.wikipedia.org/wiki/Quadratic_equation) in the variable $\\delta_{\\epsilon}$ we obtain the condition \n&gt; $$ 0 &lt; \\delta_{\\epsilon} &lt;  \\sqrt{x^{2}_{0} + \\epsilon} -|x_{0}| = \\sqrt{x^{2}_{0} + \\epsilon} - \\sqrt{x^{2}_{0}}. $$\n\nUsing previous results and [[1.2 Limits of scalar functions#^f50bb0|algebraic manipulations of limits]], check that every polynomial function is continuous.\nUse the [[1.2 Limits of scalar functions#Improper limits|definition]] to check the following limits: \\[ \\begin{split} 1) \\quad & \\lim_{x\\rightarrow +\\infty}\\,x^{n}=+\\infty \\\\ & \\\\ 2) \\quad & \\lim_{x\\rightarrow -\\infty}\\,x^{2n}=+\\infty \\\\ & \\\\ 3) \\quad & \\lim_{x\\rightarrow -\\infty}\\,x^{2n+1}=-\\infty \\\\ & \\\\ 4) \\quad & \\lim_{x\\rightarrow 0^{\\pm}} \\;\\frac{1}{x^{2n}} =+ \\infty \\\\& \\\\ 5) \\quad & \\lim_{x\\rightarrow 0^{\\pm}} \\;\\frac{1}{x^{2n+1}} =\\pm \\infty \\\\& \\\\    6)   \\quad & \\lim_{x\\rightarrow \\pm\\infty} \\;\\frac{1}{x^{n}} =0 \\\\& \\\\ \\end{split} \\] &gt;[!note]- Solution &gt;1) We must prove that, for every \\(K&gt;0\\), there is \\(M&gt;0\\) such that \\(x&gt;M\\) implies \\(f(x)=x^{n}&gt;K\\). Recall that \\(a&gt;b&gt;0\\) implies \\(a^{n}&gt; b^{n}\\) for every natural number \\(n&gt;0\\) (can you see why?). Then, taking \\(M=\\sqrt[n]{K}\\), we have \\(f(x)=x^{n}&gt; M^{n}=K\\) as requested. &gt;2) We must prove that, for every \\(K&gt;0\\), there is \\(M&gt;0\\) such that \\(x&lt;-M\\) implies \\(f(x)=x^{2n}&gt;K\\). Recall that \\(a&lt;b&lt;0\\) implies \\(a^{2n}&gt;b^{2n}&gt;0\\) for every natural number \\(n&gt;0\\) (can you see why?). Then, taking \\(M=\\sqrt[2n]{K}\\), we have \\(f(x)=x^{2n}&gt;M^{2n}=K\\) as requested. &gt;3) We must prove that, for every \\(K&gt;0\\), there is \\(M&gt;0\\) such that \\(x&lt;-M\\) implies \\(f(x)=x^{2n+1}&lt;-K\\). Recall that \\(a&lt;b&lt;0\\) implies \\(a^{2n+1}&lt;b^{2n+1}&lt;0\\) for every natural number \\(n&gt;0\\) (can you see why?). Then, taking \\(M=\\sqrt[2n+1]{K}\\), we have \\(f(x)=x^{2n+1}&lt;-M^{2n+1}=-K\\) as requested. &gt;4) For the right limit (\\(0^{+}\\)), we must prove that, for every \\(K&gt;0\\), there is \\(\\delta&gt;0\\) such that \\(0&lt;x&lt;\\delta\\) implies \\(f(x)&gt;K\\). If \\(0&lt;x&lt;\\delta\\), then \\(\\frac{1}{\\delta}&lt;\\frac{1}{x}\\), which means \\(\\frac{1}{\\delta^{2n}}&lt;\\frac{1}{x^{2n}}\\). Taking \\(\\delta=K^{\\frac{1}{2n}}\\), we have \\(f(x)=\\frac{1}{x^{2n}}&gt;\\frac{1}{\\delta^{2n}}=K\\) as requested. For the left limit (\\(0^{-}\\)), we must prove that, for every \\(K&gt;0\\), there is \\(\\delta&gt;0\\) such that \\(|x|&lt;\\delta\\), with \\(x&lt;0\\), implies \\(f(x)&gt;K\\). Recall that \\(x^{2n}&gt;0\\) even if \\(x&lt;0\\). If \\(|x|&lt;\\delta\\), then \\(\\frac{1}{\\delta}&lt;\\frac{1}{|x|}\\), which means \\(\\frac{1}{\\delta^{2n}}&lt;\\frac{1}{x^{2n}}\\). Taking \\(\\delta=K^{\\frac{1}{2n}}\\), we have \\(f(x)=\\frac{1}{x^{2n}}&gt;\\frac{1}{\\delta^{2n}}=K\\) as requested. &gt;5) For the right limit (\\(0^{+}\\)), we must prove that, for every \\(K&gt;0\\), there is \\(\\delta&gt;0\\) such that \\(0&lt;x&lt;\\delta\\) implies \\(f(x)&gt;K\\). If \\(x&lt;\\delta\\), then \\(\\frac{1}{\\delta}&lt;\\frac{1}{x}\\), which means \\(\\frac{1}{\\delta^{2n+1}}&lt;\\frac{1}{x^{2n+1}}\\). Taking \\(\\delta=K^{\\frac{1}{2n+1}}\\), we have \\(f(x)=\\frac{1}{x^{2n+1}}&gt;\\frac{1}{\\delta^{2n+1}}=K\\) as requested. For the left limit (\\(0^{-}\\)), we must prove that, for every \\(K&gt;0\\), there is \\(\\delta&gt;0\\) such that \\(|x|&lt;\\delta\\), with \\(x&lt;0\\), implies \\(f(x)&lt;-K\\), which is equivalent to \\(|f(x)|&gt;K\\). Recall that \\(x^{2n+1}&lt;0\\) when \\(x&lt;0\\). If \\(|x|&lt;\\delta\\), then \\(\\frac{1}{\\delta}&lt;\\frac{1}{|x|}\\), which means \\(\\frac{1}{\\delta^{2n+1}}&lt;\\frac{1}{|x^{2n+1}|}\\). Taking \\(\\delta=\\frac{1}{\\sqrt[2n+1]{K}}\\), we have \\(|f(x)|=\\frac{1}{|x^{2n+1}|}&gt;\\frac{1}{\\delta^{2n+1}}=K\\) as requested. &gt;6) For \\(x\\rightarrow+\\infty\\), we must prove that, for every \\(\\epsilon&gt;0\\), there is \\(M&gt;0\\) such that \\(x&gt;M\\) implies \\(|f(x) - 0|&lt;\\epsilon\\). If \\(x&gt;M&gt;0\\), then \\(|f(x)|=|\\frac{1}{x^{n}}|=\\frac{1}{x^{n}}&lt;\\frac{1}{M^{n}}\\), so that \\(M&gt;\\frac{1}{\\sqrt[n]{\\epsilon}}\\) does the job. For \\(x\\rightarrow-\\infty\\), we must prove that, for every \\(\\epsilon&gt;0\\), there is \\(M&gt;0\\) such that \\(x&lt;-M\\) implies \\(|f(x)-0|&lt;\\epsilon\\). If \\(x&lt;-M&lt;0\\) then \\(|x|&gt;M\\) so that \\(|f(x)|=\\frac{1}{|x^{n}|}&lt;\\frac{1}{M^{n}}\\), and thus \\(M&gt;\\frac{1}{\\sqrt[n]{\\epsilon}}\\) does the job.\nConsider the function \\(([0,+\\infty),\\mathbb{R},f,\\mathbb{R})\\), where \\(f(x)=x^{\\frac{1}{n}}\\equiv\\sqrt[n]{x}\\) with \\(n&gt;1\\):\n\nprove it is a continuous function;\nfind its limit at \\(+\\infty\\);\nwhat can we say regarding \\(f(x)=x^{\\frac{m}{n}}\\)?\n\n\n[!note]- Solution\n\n\nWe start noting that \\(([0,+\\infty),\\mathbb{R},f,\\mathbb{R})\\) is the [[1.1 Sets and functions#^ac035a|inverse function]] of \\(([0,+\\infty),\\mathbb{R},g,\\mathbb{R})\\) where \\(g(y)=y^{n}\\). In exercise 2) above, we proved that \\(((-\\infty,+\\infty),\\mathbb{R},g,\\mathbb{R})\\) is continuous. Therefore, \\(((0,+\\infty),\\mathbb{R},g,\\mathbb{R})\\) is continuous, and its inverse function \\(((0,+\\infty),\\mathbb{R},f,\\mathbb{R})\\) is continuous ([[1.2 Limits of scalar functions#^0478fa|see here]]). The continuity of \\(([0,+\\infty),\\mathbb{R},f,\\mathbb{R})\\) at \\(0\\) can be proved applying the [[1.2 Limits of scalar functions#^cbf378|very definition of limit]], and recalling that \\(0&lt;a&lt;b\\) implies \\(0&lt;a^{\\frac{1}{n}}&lt;b^{\\frac{1}{n}}\\) (can you see why?).\nUsing the previous inequality, we have that \\(0&lt;\\sqrt[n]{M}&lt;\\sqrt[n]{x}\\), so that \\(M=K^{n}\\) implies \\(x^{\\frac{1}{n}}&gt;K\\), which means that \\(\\lim_{x\\rightarrow+\\infty}f(x)=+\\infty\\).\nOnce we know \\(([0,+\\infty),\\mathbb{R},f,\\mathbb{R})\\) is continuous, we can use the [[1.2 Limits of scalar functions#^47f720|algebra of continuous functions]] to conclude \\(([0,+\\infty),\\mathbb{R},h,\\mathbb{R})\\), where \\(h(x)=x^{\\frac{m}{n}}\\), is continuous too.\n\nCheck the following limits: \\[ \\begin{split} 1) \\quad & \\lim_{x\\rightarrow 1}\\,\\frac{x}{x+1}=\\frac{1}{2}  \\\\ & \\\\ 2) \\quad &  \\lim_{x\\rightarrow 0}\\, \\frac{x}{|x|}=\\nexists \\\\ & \\\\ 3) \\quad &  \\lim_{x\\rightarrow 0}\\, \\frac{|x|}{ x }=\\nexists \\\\ & \\\\ 4) \\quad &  \\lim_{x\\rightarrow 0}\\, f(x) =\\nexists \\quad \\mbox{ with } f(x)= \\left\\{\\begin{matrix} 1-x^{2} & \\mbox{ if } x&lt;0 \\\\ x^{3} & \\mbox{ if } x\\geq 0\\end{matrix}\\right.  \\\\ & \\\\ 5) \\quad &  \\lim_{x\\rightarrow 1^{\\pm}}\\, \\frac{x^{2}-1}{ x^{2} -2x + 1 }= \\pm\\infty\\\\ & \\\\ 6) \\quad &  \\lim_{x\\rightarrow 2^{+}}\\, \\frac{\\sqrt{x^{2} -4}}{x-2} = +\\infty \\end{split} \\] &gt;[!note]- Solution\n\nUse [[1.2 Limits of scalar functions#^f50bb0|algebraic manipulation of limits]].\nWhen \\(x\\rightarrow 0^{+}\\), it is \\(\\frac{x}{|x|}=1\\), while \\(\\frac{x}{|x|}=-1\\) when \\(x\\rightarrow 0^{-}\\). Consequently, the left and right limits are different, and the limit for \\(x\\rightarrow 0\\) does not exist.\nThe proof is analogous to that of 2) above.\nThe proof is analogous to that of 2) above.\nWhen \\(x=1\\), both numerator and denominator vanish, and we get an indeterminate form. However, the fact that both polynomials have a common root (\\(x=1\\)) suggests we factorize them. In particular, we have \\(x^{2}-1=(x-1)(x+1)\\), and \\(x^{2}-2x +1=(x-1)(x-1)\\). Consequently, we have &gt; \\[ \\frac{x^{2}-1}{x^{2}-2x+1}=\\frac{x+1}{x-1}. \\] &gt; Now, the numerator tends to 2 when \\(x\\rightarrow 1^{\\pm}\\), while the denominator tends to \\(0\\). Therefore, using the [[1.2 Limits of scalar functions#^55a3be|algebraic manipulation of limits]], we obtain that the limit is \\(+\\infty\\) when \\(x\\rightarrow 1^{+}\\) (because the denominator tends to \\(0\\) from the left), and \\(-\\infty\\) when \\(x\\rightarrow 1^{-}\\) (because the denominator tends to \\(0\\) from the right).\nWhen \\(x=2\\), both numerator and denominator vanish, and we get an indeterminate form. Putting the denominator in the square root, and using the difference of two squares identity, we obtain &gt; \\[ \\frac{\\sqrt{x^{2}-4}}{x-2}=\\sqrt{\\frac{x+2}{x-2}}. \\] &gt; Then, the rest of the proof uses the [[1.2 Limits of scalar functions#^c8052c|limit of composite functions]] (with the outer function equal to the square root function) and a reasoning analogous to that of the previous point.\n\n\n\n\n2 Derivatives\n\nUsing its [[1.3 Derivatives of scalar functions#^971f59|very definition]], find the derivative of \\(f(x)=x^{3}\\) and \\(f(x)=x^{4}\\) for every \\(x_{0}\\in\\mathbb{R}\\). Hint: Use Newton’s binomial formula \\[ (a+b)^{n}=\\sum\\limits_{k=0}^{n}{n\\choose k} a^{n-k}b^{k}=\\sum\\limits_{k=0}^{n}{n\\choose k} a^{k}b^{n-k}, \\] where \\({n\\choose k}=\\frac{n!}{k!(n-k)!}\\) is the binomial coefficient counting the ways of choosing an (unordered) subset of \\(k\\) elements from a fixed set of \\(n\\) elements, to check that \\(\\frac{\\mathrm{d}f}{dx}=n\\, x^{n-1}\\) for \\(f(x)=x^{n}\\) and all \\(n&gt;0\\) in \\(\\mathbb{N}\\).\nCompute the derivative of \\(([0,+\\infty),\\mathbb{R},f,\\mathbb{R})\\), where \\(f(x)=\\sqrt[n]{x}=x^{\\frac{1}{n}}\\) with \\(n&gt;1\\), at each point. &gt;[!note]- Solution &gt;A direct computation shows that \\(([0,+\\infty),\\mathbb{R},f,\\mathbb{R})\\) is the inverse function of \\(([0,+\\infty),\\mathbb{R},g,\\mathbb{R})\\) with \\(g(y)=y^{n}\\). The function \\(([0,+\\infty),\\mathbb{R},g,\\mathbb{R})\\) is differentiable for every \\(y&gt;0\\). Therefore, [[1.3 Derivatives of scalar functions#^05d051|we conclude that]] \\(([0,+\\infty),\\mathbb{R},f,\\mathbb{R})\\) is differentiable for each \\(x&gt;0\\), and its derivative reads &gt;\\[ f'(x)=\\frac{1}{n} x^{\\frac{1-n}{n}}. \\] &gt;At \\(y=0\\), we can only compute the right derivative of \\(([0,+\\infty),\\mathbb{R},g,\\mathbb{R})\\) : &gt;\\[ \\lim_{h\\rightarrow 0^{+}} \\frac{(0+h)^{\\frac{1}{n}}-0}{h} = \\lim_{h\\rightarrow 0^{+}} h^{\\frac{1-n}{n}}=\\lim_{h\\rightarrow 0^{+}} \\frac{1}{h^{\\frac{n-1}{n}}} = + \\infty,\\] &gt; and thus \\(([0,+\\infty),\\mathbb{R},f,\\mathbb{R})\\) has no right derivative at \\(x=0\\).\nDetermine for which \\(x\\in\\mathbb{R}\\) the function \\((\\mathbb{R},\\mathbb{R},f,\\mathbb{R})\\) is differentiable in the following cases: \\[ \\begin{split}    1) \\quad & f(x)= |x - 2| \\\\ & \\\\ 2) \\quad & f(x)= \\sqrt{|x|} \\\\ & \\\\            3) \\quad & f(x)= \\left\\{\\begin{matrix}  x^{2} & \\mbox{ if } x\\leq 1 \\\\ 2-x & \\mbox{ if } x&gt; 1\\end{matrix}\\right. \\\\ & \\\\    4) \\quad & f(x)= \\left\\{\\begin{matrix} x^{2} -1 & \\mbox{ if } x\\leq 2 \\\\  3 & \\mbox{ if } x&gt; 2\\end{matrix}\\right. \\\\ & \\\\   5) \\quad & f(x)= \\left\\{\\begin{matrix}  4x & \\mbox{ if } x&lt;1 \\\\ 2x +2 & \\mbox{ if } x\\geq 1\\end{matrix}\\right. \\\\ & \\\\  6) \\quad & f(x)= \\left\\{\\begin{matrix}  \\frac{-x^{2}}{2}  & \\mbox{ if } x&lt;3 \\\\ -3x & \\mbox{ if } x\\geq 3\\end{matrix}\\right. \\end{split} \\] &gt;[!note]- Solution &gt;1) Given the appearance of the absolute value, we split our function in two pieces: \\(([2,\\infty),\\mathbb{R},f_{1},\\mathbb{R})\\) with \\(f_{1}(x)=x-2=f(x)\\), and \\(((-\\infty,2],\\mathbb{R},f,\\mathbb{R})\\), with \\(f_{2}(x)=2-x=f(x)\\). Since we already know the derivative of constant and linear functions, the [[1.3 Derivatives of scalar functions#^5d20ad|algebraic manipulations of derivatives]] ensure \\(([2,\\infty),\\mathbb{R},f_{1},\\mathbb{R})\\) is differentiable for every \\(x&gt;2\\), and its right derivative at \\(x=2\\) is equal to 1. Proceeding analogously for \\(((-\\infty,2],\\mathbb{R},f,\\mathbb{R})\\), with \\(f_{2}(x)=2-x=f(x)\\), we obtain it is differentiable for every \\(x&lt;2\\), and its left derivative at \\(x=2\\) is \\(-1\\). We thus conclude that \\((\\mathbb{R},\\mathbb{R},f,\\mathbb{R})\\) is differentiable everywhere except at \\(x=2\\). &gt;2) Following the idea used in point 1), we split the function in two pieces: \\(([0,\\infty),\\mathbb{R},f_{1},\\mathbb{R})\\) with \\(f_{1}(x)=\\sqrt{x}=f(x)\\), and \\(((-\\infty,0],\\mathbb{R},f,\\mathbb{R})\\), with \\(f_{2}(x)=\\sqrt{-x}=f(x)\\). Then, exercise 2.2) above shows the function is differentiable everywhere except at \\(x=0\\). &gt;3) Following the idea used in point 1) we conclude the function is differentiable for every \\(x\\neq 1\\). &gt;4) Following the idea used in point 1) we conclude the function is differentiable for every \\(x\\neq 2\\). &gt;5) Following the idea used in point 1) we conclude the function is differentiable for every \\(x\\neq 1\\). &gt;6) Following the idea used in point 1) we conclude the function is differentiable everywhere.\nWhenever possible, compute the derivatives at all points in the maximal domain of the scalar functions determined by: \\[ \\begin{split} 1) \\quad &  f(x)= x^{\\frac{m}{n}}  \\\\  & \\\\5) \\quad &  f(x)= \\frac{2x}{1-x}  \\\\  & \\\\ 3) \\quad &  f(x)=\\frac{x}{1-x}\\,\\frac{2-x}{3+x}  \\\\  & \\\\ 4) \\quad &  f(x)=\\frac{(x^{2} +1)^{3}}{x+3}   \\\\  & \\\\ 5) \\quad &  f(x)=\\sqrt{x^{4} + 3x^{2} -2}  \\\\  & \\\\ 6) \\quad &  f(x)=\\sqrt{\\frac{x +2}{x^{2}}}  \\\\  & \\\\ 7) \\quad &  f(x)=\\frac{x}{\\sqrt{x+2}}  \\\\  & \\\\ 8) \\quad &    f(x)=(g(x))^{\\frac{m}{n}} \\end{split} \\] &gt;[!note]- Solution &gt;1) The maximal domain is \\([0,+\\infty)\\). Using the result of exercise 2.2) above together with the [[1.3 Derivatives of scalar functions#^c87d27|chain rule]], we obtain \\(f'(x)=\\frac{m}{n}x^{\\frac{m}{n}-1}\\) for every \\(x&gt;0\\), and the right derivative at \\(x=0\\) is \\(0\\) when \\(m\\geq n\\), and undefined when \\(m&lt;n\\). &gt;2) The maximal domain is \\((-\\infty,1)\\cup (1,+\\infty)\\). Using the [[1.3 Derivatives of scalar functions#^5d20ad|algebraic manipulation of derivatives]] we obtain \\(f'(x)=\\frac{2}{(1-x)^{2}}.\\) &gt;3) The maximal domain is \\((-\\infty,-3)\\cup(-3,1)\\cup(1,+\\infty)\\). The derivative is then computed using the [[1.3 Derivatives of scalar functions#^5d20ad|algebraic manipulation of derivatives]]. You can check your result here, or look in the [[Home#Tools to help with exercises|home]] for further suggestions. &gt;4) The maximal domain is \\((-\\infty,-3)\\cup(-3,+\\infty)\\). The derivative is then computed using the [[1.3 Derivatives of scalar functions#^5d20ad|algebraic manipulation of derivatives]]. You can check your result here, or look in the [[Home#Tools to help with exercises|home]] for further suggestions. &gt;5) The maximal domain is the subset on which \\(x^{4}+3x^{2}-2\\geq 0\\). The derivative is then computed using the [[1.3 Derivatives of scalar functions#^5d20ad|algebraic manipulation of derivatives]] and the [[1.3 Derivatives of scalar functions#^c87d27|chain rule]]. You can check your result here, or look in the [[Home#Tools to help with exercises|home]] for further suggestions. &gt;6) The maximal domain is \\([-2,0)\\cup(0,+\\infty)\\). The right derivative at \\(x=-2\\) does not exist, while the derivative in the other points is computed using the [[1.3 Derivatives of scalar functions#^5d20ad|algebraic manipulation of derivatives]] and the [[1.3 Derivatives of scalar functions#^c87d27|chain rule]]. You can check your result here, or look in the [[Home#Tools to help with exercises|home]] for further suggestions. &gt;7) The maximal domain is \\((-2,+\\infty)\\). The derivative is then computed using the [[1.3 Derivatives of scalar functions#^5d20ad|algebraic manipulation of derivatives]] and the [[1.3 Derivatives of scalar functions#^c87d27|chain rule]]. You can check your result here, or look in the [[Home#Tools to help with exercises|home]] for further suggestions. &gt;8) The maximal domain here is the subset on which \\(g(x)\\geq 0\\). Then, we can use the [[1.3 Derivatives of scalar functions#^c87d27|chain rule]] to get &gt; \\[ f'(x)=\\frac{m}{n}(g(x))^{\\frac{m}{n}-1}\\,g'(x) \\] &gt; at all those \\(x\\) at which \\(g\\) is differentiable and such that \\(g(x)&gt;0\\). &gt; When \\(g\\) is differentiable but \\(g(x)=0\\), the right derivative is undefined when \\(m&lt;n\\) and equal to \\(0\\) when \\(m\\geq n\\). Obviously, at all those points at which \\(g\\) is not differentiable, then \\(f\\) is not differentiable.\n\n\n\n3 Numerical sequences and series\n\nProve the following limits \\[ \\begin{split} 1) \\quad & \\lim_{n\\rightarrow+\\infty}\\, n!= + \\infty \\\\ & \\\\   2) \\quad &  \\lim_{n\\rightarrow+\\infty}\\,\\frac{x^{n}}{n!}=0\\quad\\mbox{ for }\\quad |x|\\leq 1    \\\\ & \\\\   3) \\quad &  \\lim_{n\\rightarrow +\\infty}\\,\\sqrt[n]{n}=1  \\\\ & \\\\     4) \\quad & \\lim_{n\\rightarrow + \\infty}\\, \\sqrt[n]{n!} = +\\infty  \\\\ & \\\\   5) \\quad & \\lim_{n\\rightarrow +\\infty}\\,kx^{k+2}-(k+1)x^{k+1}=\\left\\{\\begin{matrix}0 &\\mbox{ if } |x|&lt;1 \\\\ +\\infty & \\mbox{ if } x&gt;1 \\\\ \\nexists & \\mbox{ if } x&lt;-1\\end{matrix}\\right.     \\end{split} \\]\n\n[!note]- Solution\n\nBecause of the factorial, we can not rely on limits of functions. We must prove that, for every \\(K&gt;0\\), there exists \\(N\\in\\mathbb{N}\\) such that \\(a_{n}=n!&gt;K\\) for all \\(n&gt;N\\). When \\(n&gt;1\\), it holds \\(n!=n(n-1)\\cdots 1&gt; n\\), so that, setting \\(n=N&gt;K\\), we get \\(a_{n}=n!&gt;n&gt;K\\) as desired.\nWhen \\(|x|&lt;1\\), exploiting a [[1.4 Sequences and series#^157012|previous example]], the result the previous point, and the algebraic manipulations of limits of sequences, the desired result follows. When \\(x=1\\), the sequence \\(a_{n}=1^n\\) has limit \\(1\\). Reasoning as before, the result follows. When \\(x=-1\\), the sequence \\(a_{n}=(-1)^n\\) has no limit. However, \\(\\vert\\frac{(-1)^n}{n!}\\vert=\\vert \\frac{1}{n!}\\vert\\), and we just poved that \\[ \\lim_{n\\rightarrow+\\infty}\\frac{1}{n!}=0 .\\] Therefore, we may exploit the [[1.4 Sequences and series#^0cefdc|very definition of limit]] and immediately conclude the desired result.\nSet \\(b_{n}\\equiv \\sqrt[n]{n} -1&gt;0\\), so that \\(n=(1+b_{n})^{n}\\). Recalling Newton’s binomial formula \\[ (a+b)^{n}=\\sum\\limits_{k=0}^{n}{n\\choose k} a^{n-k}b^{k}=\\sum\\limits_{k=0}^{n}{n\\choose k} a^{k}b^{n-k} , \\] it follows that \\[ n=(1+b_{n})^{n}=\\sum\\limits_{k=0}^{n}{n\\choose k} 1^{k}b_{n}^{n-k}\\,\\stackrel{k=n-2}{\\geq}\\, \\frac{n(n-1)}{2!} b_{n}^{2}, \\] which means that \\(0\\leq b_{n}\\leq\\sqrt{\\frac{2}{(n-1)}}\\). The squeeze theorem for sequences then implies \\[ \\lim_{n\\rightarrow+\\infty} b_{n}=0\\;\\Longleftrightarrow\\; \\lim_{n\\rightarrow+\\infty} \\sqrt[n]{n}-1=0 , \\] which proves the desired equality.\nWe [[1.4 Sequences and series#^5a7f61|have to prove that]], given \\(K&gt;0\\), there is \\(N&gt;0\\) such that \\(n&gt;N\\) implies \\(a_{n}=\\sqrt[n]{n!}&gt;K\\). Being \\(n&gt;N&gt;0\\), we can take \\(n\\geq1\\). We first prove that \\(n!\\geq (\\sqrt{\\frac{n}{2}})^{n}\\) when \\(n\\geq1\\) At this purpose, we introduce the so-called integer part function \\(\\lfloor x \\rfloor\\) which takes a real number \\(x\\) and gives back the first integer which is smaller than \\(x\\) (for instance, \\(\\lfloor 1.4 \\rfloor=1\\) and \\(\\lfloor -3.1 \\rfloor=-4\\))., so that \\(\\lfloor \\frac{n}{2}\\rfloor \\leq \\frac{n}{2}\\leq \\lfloor \\frac{n}{2}\\rfloor +1\\). Then, we have \\[ n!=1\\cdot 2\\cdots\\lfloor \\frac{n}{2}\\rfloor \\left(\\lfloor \\frac{n}{2}\\rfloor+1\\right)\\cdots n \\geq \\left(\\frac{n}{2}\\right)^{n-\\lfloor \\frac{n}{2}\\rfloor}\\geq \\left(\\frac{n}{2}\\right)^{\\frac{n}{2}} , \\] which means \\(n!\\geq\\left(\\sqrt{\\frac{n}{2}}\\right)^{n}\\) as claimed, and thus \\(\\sqrt[n]{n!}\\geq\\sqrt{\\frac{n}{2}}\\). Finally, the result follows taking \\(N&gt;2K^{2}\\) so that \\(n&gt;N&gt;2K^{2}\\) implies \\(\\sqrt[n]{n!}&gt;\\sqrt{\\frac{n}{2}}&gt;K\\).\nIt is \\(kx^{k+2}-(k+1)x^{k+1}=x^{k+1}(k(x-1)-1)\\). Therefore, when \\(X&gt;1\\) the sequence diverges. When \\(x&lt;-1\\), the series alternates positive and negative values, but this values grow indefinitely, so that the limit of the sequence does not exist. When \\(|x|&lt;1\\), we need to find a different procedure. We start considering \\(0\\leq x&lt; 1\\), and noting that there is \\(y&gt;0\\) such that \\(x=(1+y)^{-1}\\). Therefore, we can use the binomial formula as above to obtain \\[ (1+y)^{n}=\\sum\\limits_{k=0}^{n}{n\\choose k} y^{n-k}\\;\\stackrel{k=n-2}{\\geq}\\;\\frac{n(n-1)}{2!}y^{2} . \\] It is important to note that the condition \\(y&gt;0\\) is necessary to obtain the inequality above. Then, we get \\(nx^{n}\\leq \\frac{2x^{2}}{(1-x)^{2}(n-1)}\\), and thus the squeeze theorem for sequences implies the desired result. The case \\(-1&lt;x&lt;0\\) is handled as before, but taking an overall minus sign out.\n\n\n\n^98a973\n\nCompute the value of the truncated geometric series associated with \\[ a_{n} = \\left\\{ \\begin{matrix} a_{n}=0 & \\mbox{ for } n=0,...,r-1 \\\\ a_{n} = x^{n} & \\mbox{ for } n\\geq r . \\end{matrix} \\right. \\] &gt;[!note]- Solution &gt;The trick is to realize that\n&gt;\\[ \\sum_{n=0}^{+\\infty}a_{n}=\\sum_{n=r}^{+\\infty}x^{n}= \\sum_{n=0}^{+\\infty}x^{n+r} = x^{r}\\sum_{n=0}^{+\\infty}x^{n}, \\] &gt;which means that the truncated geometric series is proportional to the [[1.4 Sequences and series#^bf33d9|geometric series]], and thus its convergence, as well as its sum, can be deduced from the geometric series.\nDetermine the behavior of the arithmetic-geometric series defined by \\(a_{n}=nx^{n}\\) (starting at \\(n=0\\) or \\(n=1\\)). &gt;[!note]- Solution &gt;First of all, starting at \\(n=0\\) or \\(n=1\\) is irrelevant for the result. Then, once again, the series diverges when \\(x=1\\) because \\(S_{k}=\\sum_{n=0}^{k}a_{n}=1+2+3+\\cdots +(k-1) + k= \\frac{k(k+1)}{2}\\). When \\(k\\neq 1\\), we have &gt;\\[ S_{k} - x S_{k} =(x + 2 x^{2} + 3 x^{3} + \\cdots + k x^{k})-(x^{2} + 2 x^{3} + 3 x^{4} + k x^{k+1}) = x + x^{2} + \\cdots + x^{k} -kx^{k+1} .\\]\n&gt;The positive terms in the right-hand-side from the partial sum of the [[1.4 Sequences and series#^bf33d9|geometric series]] apart from the first term. Therefore, we have\n&gt;\\[ (1-x)S_{k}=\\frac{x^{k+1}-1}{x-1} -1 -kx^{k+1}=\\frac{x-(k+1)x^{k+1} +kx^{k+2}}{1-x}  \\Longrightarrow S_{k}=\\frac{x-(k+1)x^{k+1} +kx^{k+2}}{(1-x)^{2}}. \\]\n&gt;Because of exercise 3.1.5), when \\(x&gt;1\\) the sequence of partial sums diverges, when \\(x&lt;-1\\) it does not have a limit, while it converges to \\(0\\) when \\(|x|&lt;1\\). Consequently, the sum of the arithmetic-geometric series is\n&gt;\\[ \\sum\\limits_{n=0}^{+\\infty}nx^{n}=\\frac{x}{(1-x)^{2}} \\] &gt;when \\(|x|&lt;1\\), while its either unbounded or undefined in the other cases.\nProve that, if \\(a_{n}=u_{n}-u_{n+1}\\) for some sequence \\(\\{u_{n}\\}_{n\\in\\mathbb{N}}\\) (telescoping series), then \\[ \\sum\\limits_{n=1}^{\\infty}a_{n}=u_{1}-\\lim_{n\\rightarrow\\infty}u_{n} .\\] &gt;[!note]- Solution &gt;The partial sum of the series determined by \\(a_{n}\\) reads &gt; \\[ S_{k}=\\sum\\limits_{n=1}^{k}u_{n}-u_{n+1}=u_{1}-u_{k+1}, \\] &gt; and the result follows from the [[1.4 Sequences and series#^32251b|very definition]] of the sum of a series.\nCheck the following equalities: \\[ \\begin{split}    1) &\\qquad  \\sum_{n=1}^{+\\infty}\\,\\frac{1}{2n(n+1)}\\,=\\;\\frac{1}{2}\\\\ & \\\\  2)  &\\qquad  \\sum_{n=0}^{+\\infty}\\;\\frac{n}{10^{n}} \\,=\\;\\frac{10}{81} \\\\ & \\\\  3)  &\\qquad   \\sum_{n=2}^{+\\infty}\\;\\frac{1}{n^{2} -n}\\,=\\;1 \\\\ & \\\\    4)  &\\qquad  \\sum_{n=0}^{+\\infty}\\;\\frac{1}{(n+1)(n+3)} \\,=\\; \\frac{3}{4}\\\\ & \\\\    5)  &\\qquad  \\sum_{n=0}^{+\\infty}\\;\\frac{2^{n+3}}{3^{n}}\\,=\\; 24.\\end{split}\\] &gt;[!note]- Solution &gt;1) The result follows from exercise 3.4) above once we note that \\(a_{n}=\\frac{1}{2}(\\frac{1}{n}-\\frac{1}{n+1})\\). &gt;2) This exercise should deserve no explication. &gt;3) We note that \\(a_{n}= \\frac{1}{n-1}-\\frac{1}{n}\\). Then, we perform the replacement \\(n=k+1\\) so that &gt; \\[ \\sum\\limits_{n=2}^{+\\infty}a_{n}\\rightarrow\\sum\\limits_{k=1}^{+\\infty}a_{k} \\] &gt; with \\(a_{k}=\\frac{1}{k}-\\frac{1}{k+1}\\), and the result follows again from exercise 3.4) above. &gt;4) We note that \\(a_{n}=\\frac{1}{2}(\\frac{1}{n+1} - \\frac{1}{n+3})\\), so that the replacement \\(k=n+1\\) gives \\(a_{k}=\\frac{1}{2}(\\frac{1}{k} - \\frac{1}{k+2})\\). Now, we can not directly use the result of exercise 3.4) above, however, we can adapt the proof used there to this case and obtain\n&gt; \\[ \\sum_{k=1}^{\\infty}a_{k}=\\sum_{k=1}^{+\\infty}u_{k}-u_{k+2}=u_{1}+u_{2}+\\lim_{k\\rightarrow +\\infty}u_{k}, \\] &gt; from which the desired result immediately follows. &gt;5) As exercise 3.5.2) above, this exercise should deserve no explanation.\n\n\n\n4 Power series, trascendental functions, and Taylor expansion\n\nCompute the derivatives, at all points in the maximal domain at which it makes sense, of the scalar functions determined by: \\[ \\begin{split} 1) \\quad &  f(x) =\\ln(x-a) \\\\ & \\\\ 2) \\quad & f(x) =\\ln(|x|) \\\\ & \\\\ 3) \\quad & f(x)= x^{\\alpha}   \\\\ & \\\\ 4) \\quad & f(x) = \\arcsin(x) \\\\ & \\\\    5) \\quad & f(x) = \\arccos(x) \\\\ & \\\\    6) \\quad & f(x) = \\tan(x)  \\\\ & \\\\  7) \\quad & f(x) =\\cot(x) \\\\ & \\\\    8) \\quad & f(x) = \\arctan(x) \\\\ & \\\\    9) \\quad & f(x) =  x^{x} \\\\ & \\\\    10) \\quad & f(x) = \\sqrt{\\tan(x)} \\\\ & \\\\   11) \\quad & f(x) = \\arctan\\left(x^{3} - e^{x^{2}}\\right) \\\\ & \\\\    12) \\quad &  f(x)=(g(x))^{h(x)} \\\\ & \\\\ 13) \\quad & f(x)= \\arctan\\left(\\frac{g(x)}{h(x)}\\right)  \\end{split} \\] &gt;[!note]- Solution &gt; 1) Since \\(\\ln(x-a)\\) is the inverse function of \\(\\mathrm{exp}(x-a)=\\mathrm{e}^{x-a}\\), we can use the [[1.3 Derivatives of scalar functions#^05d051|formula for the derivative of the inverse function]] to obtain \\(f'(x)=\\frac{1}{x-a}\\). Alternatively, we can use \\(g(y)=\\ln(y)\\), \\(y(x)=x-a\\), and the [[1.3 Derivatives of scalar functions#^c87d27|chain rule]]. &gt; 2) Because of the absolute value, we have to split the function for \\(x&gt;0\\) and \\(x&lt;0\\) (note that \\(x=0\\) is forbidden). Once the split is done, we can proceed as in point 1) above. The result is \\(f'(x)=\\frac{1}{x}\\). &gt; 3) The trick is to write \\(x^{\\alpha}=\\mathrm{e}^{\\alpha\\ln(x)}\\) and then apply the [[1.3 Derivatives of scalar functions#^c87d27|chain rule]]. The result is thus \\(f'(x)=\\alpha x^{\\alpha-1}\\). &gt; 4) Recall that the domain of \\(\\arcsin(x)\\) is \\(-1\\leq x\\leq 1\\). Use the [[1.3 Derivatives of scalar functions#^05d051|formula for the derivative of the inverse function]] to obtain \\(f'(x)=\\frac{1}{\\cos(\\arcsin(x))}\\). Since \\(-\\frac{\\pi}{2}&lt;\\arcsin(x)&lt;\\frac{\\pi}{2}\\) when \\(-1&lt;x&lt;1\\), \\(\\cos(\\arcsin(x))&gt;0\\), and we can use the fact that \\(\\cos(x)=\\sqrt{1-\\sin^{2}(x)}\\) to obtain \\(f'(x)=\\frac{1}{\\sqrt{1-x^{2}}}\\). &gt; 5) Proceed as in point 4) above to get \\(f'(x)=-\\frac{1}{\\sqrt{1-x^{2}}}\\). &gt; 6) Use the derivative of \\(\\sin(x)\\) and \\(\\cos(x)\\) together with the [[1.3 Derivatives of scalar functions#^5d20ad|algebraic manipulations of derivatives]] to get \\(f'(x)=\\frac{1}{\\cos^{2}(x)}\\). &gt; 7) Use the derivative of \\(\\sin(x)\\) and \\(\\cos(x)\\) together with the [[1.3 Derivatives of scalar functions#^5d20ad|algebraic manipulations of derivatives]] to get \\(f'(x)=-\\frac{1}{\\sin^{2}(x)}\\). &gt; 8) Recalling that \\(\\frac{1}{\\cos^{2}(x)}=1 + \\tan^{2}(x)\\), proceed as in point 4) above to get \\(f'(x)=\\frac{1}{1+x^{2}}\\). &gt; 9) Proceed as in point 3) above to get \\(f'(x)=x^{x}(\\ln(x)+1)\\). &gt; 10) Use the [[1.3 Derivatives of scalar functions#^c87d27|chain rule]] to get \\(f'(x)=\\frac{1}{2}(\\sqrt{\\tan(x)}\\cos^{2}(x))^{-1}\\). &gt; 11) Use the [[1.3 Derivatives of scalar functions#^c87d27|chain rule]] to get \\(f'(x)=\\frac{3x^{2} - 2x\\mathrm{e}^{x^{2}}}{1+ (x^{3} - \\mathrm{e}^{x^{2}})^{2}}\\). &gt; 12) Proceed as in point 3) above, and use the [[1.3 Derivatives of scalar functions#^c87d27|chain rule]] to get \\(f'(x)=(g(x))^{h(x)}\\left(h(x)\\ln(g(x)) + \\frac{h(x)g'(x)}{g(x)}\\right)\\). &gt; 13) Use the [[1.3 Derivatives of scalar functions#^c87d27|chain rule]] to get \\(f'(x)=\\frac{g'(x)h(x)-g(x)h'(x)}{g^{2}(x)+h^{2}(x)}\\).\nCheck the following limits: \\[\\begin{split}     1) \\quad & \\lim_{x\\rightarrow 0}\\, \\frac{\\tan(x^{2})+2x}{x+x^{2}}= 2\\\\ & \\\\ 2) \\quad & \\lim_{x\\rightarrow 0}\\,\\frac{\\mathrm{e}^{x} - x - \\cos(x)}{\\sin(x^{2})} = 1 \\\\ & \\\\  3) \\quad & \\lim_{x\\rightarrow +\\infty}\\,\\left(1 + \\frac{1}{x}\\right)^{x}= \\mathrm{e}  \\\\ & \\\\   4) \\quad & \\lim_{x\\rightarrow +\\infty}\\, \\frac{\\left(1 + \\frac{1}{x}\\right)^{x^{2}}}{\\mathrm{e}^{x}} = \\mathrm{e}^{-\\frac{1}{2}}\\\\ & \\\\ 5) \\quad & \\lim_{x\\rightarrow 0} \\frac{\\arctan(x) -x}{\\sin(x) -x}= 2    \\end{split}\\] &gt; [!note]- Solution &gt; 1) We first write &gt; \\[ \\frac{\\tan(x^{2})+2x}{x+x^{2}}=\\frac{\\sin(x^{2})}{x(1+x)\\cos(x^{2})} + \\frac{2}{(1+x)},\\] &gt; from which it is clear that the second term in the right-hand-side will not give us problems. Then, we note that &gt; \\[\\frac{\\sin(x^{2})}{x(1+x)\\cos(x^{2})}=\\frac{x\\sin(x^{2})}{x^{2}(1+x)\\cos(x^{2})},\\] &gt; so that we can use the limit &gt; \\[ \\lim_{y\\rightarrow 0}\\frac{\\sin(y)}{y}=1\\] &gt; together with the [[1.2 Limits of scalar functions#^c8052c|compostion of limits]] to verify the claimed result. &gt; 2) Since the first order expansion of \\(\\sin(x^{2})\\) already contain \\(x^{2}\\), we expand \\(\\mathrm{e}^{x}\\) and \\(\\cos(x)\\) up to the second order. Then, the result follows from a direct computation taking into account [[1.5 Power series, trascendental functions, and Taylor series#^113a5b|the property of remainder]]. &gt; 3) Since the limit is for \\(x\\rightarrow+\\infty\\), we can not use Taylor’s theorem. Analogously, we can not apply L’Hôpital’s rule because we do not have a quotient of functions. However, we can exploit a trick. We start writing \\((1+\\frac{1}{x})^{x}=\\mathrm{e}^{x\\ln(1+\\frac{1}{x})}\\). Then, we introduce the auxiliary variable \\(y=(1+\\frac{1}{x})\\), so that \\(x=\\frac{1}{y-1}\\), and thus \\((1+\\frac{1}{x})^{x}=\\mathrm{e}^{\\frac{\\ln(y)}{y-1}}\\). Moreover, we have \\(y\\rightarrow 1^{+}\\) when \\(x\\rightarrow+\\infty\\), so that &gt; \\[ \\lim_{x\\rightarrow +\\infty} \\,(1+\\frac{1}{x})^{x}= \\mathrm{e}^{\\lim_{y\\rightarrow 1^{+}}\\,\\frac{\\ln(y)}{y-1}}, \\] &gt; where we applied the [[1.2 Limits of scalar functions#^c8052c|limit of composite functions]] to obtain the last equality. We can now apply L’Hôpital’s and verify the limit. &gt; 4) To verify the limit, we follow the steps in point 3) to obtain &gt; \\[ \\frac{\\left(1 + \\frac{1}{x}\\right)^{x^{2}}}{\\mathrm{e}^{x}} =\\mathrm{e}^{\\frac{\\ln(y)}{(y-1)^{2}} - \\frac{1}{y-1}} .\\] &gt; Then, we use [[1.5 Power series, trascendental functions, and Taylor series|Taylor’s expansion]] for \\(\\ln(y)\\) around \\(y_{0}=1\\), and the limit follows. &gt; 5) To verify the limit, it suffices to use [[1.5 Power series, trascendental functions, and Taylor series|Taylor’s expansion]].\nUse Taylor’s theorem to estimate:\n\n\\(\\mathrm{e}^{0.2}\\) with an error smaller than \\(0.0005\\);\n\\(\\sin(0.1)\\) with an error smaller than \\(0.0005\\).\n\n\n[!note]- Solution\n\nThe exponential function [[1.5 Power series, trascendental functions, and Taylor series#^113a5b|can be written as]] \\[ \\mathrm{e}^{x}=\\sum\\limits_{k=0}^{+\\infty} \\frac{x^{k}}{k!}=1 + x + \\frac{x}{2!} + \\cdots \\frac{x^{n}}{n!} + o(x^{n+1}),\\] where the remainder is \\[ o(x^{n+1})=\\frac{\\mathrm{e}^{\\theta x}}{(n+1)!}x^{n+1} \\] with \\(\\theta\\in(0,1)\\). In particular, we have \\(x=0.2\\), and we want the remainder to be smaller than \\(0.0005\\). Therefore, we obtain the chain of inequalities \\[|o(0.2^{n+1})|=\\left|\\frac{\\mathrm{e}^{\\theta 0.2}}{(n+1)!}0.2^{n+1}\\right|&lt;\\left|\\frac{3}{(n+1)!}0.2^{n+1}\\right|&lt;0.00005,\\] which means \\(5^{n+1}(n+1)!&gt;6000\\). The least value of \\(n\\) satisfying the inequality is \\(n=3\\), and we have \\[\\mathrm{e}^{0.2} \\approx 1+ 0.2 + \\frac{(0.2)^{2}}{2} + \\frac{(0.2)^{3}}{6}.\\]\nThe sine function [[1.5 Power series, trascendental functions, and Taylor series#^113a5b|can be written as]] \\[ \\sin(x)=\\sum_{k=0}^{+\\infty} \\frac{(-1)^{k}x^{2k+1}}{(2k+1)!}=x - \\frac{x^{3}}{3!} + \\cdots \\frac{(-1)^{n} x^{2n+1}}{(2n+1)!} + o(x^{2(n+1)+1}),\\] where the remainder is \\[ o(x^{2(n+1)+1})=\\frac{\\sin^{(n+1)}(\\theta x)}{(2(n+1)+1)!}x^{2(n+1)+1} \\] with \\(\\theta\\in(0,1)\\). In particular, we have \\(x=0.1\\), and we want the remainder to be smaller than \\(0.0005\\). Therefore, we obtain the chain of inequalities \\[|o(0.1^{2(n+1)+1})|=\\left|\\frac{\\sin^{(n+1)}(\\theta 0.2)}{(2(n+1)+1)!}0.1^{2(n+1)+1}\\right|\\leq\\left|\\frac{1}{(2(n+1)+1)!}0.1^{2(n+1)+1}\\right|&lt;0.00005,\\] which means \\(10^{2(n+1)+1}(2(n+1)+1)!&gt;2000\\). The least integer satisfying the inequality is \\(n=0\\).\n\n\n\n\n\n5 Global extrema of scalar functions\n\nStudy the global extrema of the following functions: \\[  \\begin{split}   1) \\quad & f(x)\\,=\\; x + \\frac{1}{x} \\\\ & \\\\    2) \\quad & f(x)\\,=\\; \\frac{x^{2}}{1+x} \\\\ & \\\\  3) \\quad & f(x)\\,=\\;  x^{2}\\,\\left(2+x\\right)^{\\frac{1}{3}}\\\\ & \\\\  4) \\quad & f(x)\\,=\\; |x - 3| + |2x + 1|  \\\\ & \\\\    5) \\quad & f(x)\\,=\\;  x + \\cos(2x) \\\\ & \\\\  6) \\quad & f(x)\\,=\\;  \\left\\{\\begin{matrix} 2 - 2x -x^{2} & \\quad -2\\leq x\\leq 0 \\\\ |x-2| & \\quad  0&lt;x&lt;3 \\\\ \\frac{1}{3}\\left(x-2\\right)^{2} & \\quad  3\\leq x \\leq 4 \\end{matrix}\\right.\\\\ & \\\\  7) \\quad & f(x)\\,=\\; \\left\\{\\begin{matrix} x^{2} +1 & \\quad  -2\\leq x &lt; 1 \\\\ 5 + 2x -x^{2}& \\quad  -1\\leq x\\leq 3 \\\\ x-1 & \\quad  3&lt; x &lt; 6 \\end{matrix}\\right.  \\\\ & \\\\ 8) \\quad & f(x)\\,=\\;  \\left\\{\\begin{matrix} \\frac{3-x^{2}}{2} & \\quad  x &lt; -1 \\\\ \\frac{1}{x} & \\quad  -1\\leq x \\neq 0    \\end{matrix}\\right.\\\\ & \\\\ 9) \\quad & f(x)\\,=\\; 1 - (x-1)^{\\frac{1}{3}} \\\\ & \\\\    10) \\quad & f(x)\\,=\\; \\sqrt{x} - \\frac{1}{\\sqrt{x}} \\\\ & \\\\ \\end{split} \\]\n\n[!note]- Solution\n\nThe maximal domain of the function is \\(D=\\mathbb{R}-\\{0\\}\\). The function is smooth (in particular, continuous) in \\(D\\) because it is the sum of two smooth functions in \\(D\\). The behavior at \\(0\\) is determined uniquely by \\(\\frac{1}{x}\\), which we know goes to \\(\\pm\\infty\\) when \\(x\\rightarrow0^{\\pm}\\) because of previous exercises. The limits for \\(x\\rightarrow\\pm\\infty\\) are determined by the piece \\(g(x)=x\\), and we know they are \\(\\pm\\infty\\) because of previous exercises. From this, we already conclude that there will be no global extrema. Concerning local extrema, we compute the derivative and look for critical points \\[ f'(x)= 1 -\\frac{2}{x^{2}}\\;\\Longrightarrow\\; f'(x)=0 \\;\\Longrightarrow x^{2}=2\\;\\Longrightarrow x=\\pm\\sqrt{2}.\\] There are no other critical points because the derivative exists at each point in the maximal domain of the function. The condition \\(f'(x)&gt;0\\) implies \\(x^2&gt;2\\), and thus we see that \\(x=\\sqrt{2}\\) is a local minimum, while \\(x=-\\sqrt{2}\\) is a local maximum.\nThe maximal domain of the function is \\(D=\\mathbb{R}-\\{-1\\}\\), and the function is smooth in \\(D\\) because it is the quotient of two smooth functions in \\(D\\). Remembering the [[1.2 Limits of scalar functions#^c8052c|limit of composite functions]] and what previous exercises taught us about \\(g(t)=\\frac{1}{t}\\) when \\(t\\rightarrow 0^{\\pm}\\), we conclude \\(f(x)\\) tends to \\(\\pm\\infty\\) when \\(x\\rightarrow -1^{\\pm}\\). Then, we have \\[ \\lim_{x\\rightarrow\\pm\\infty}\\frac{x^{2}}{x+1}=\\lim_{x\\rightarrow\\pm\\infty}\\frac{x^{2}}{x\\left(1+\\frac{1}{x}\\right)}=\\pm\\infty .\\] We thus conclude that there will be no global extrema for the function. Concerning local extrema, we have \\[f'(x)=\\frac{x(2 + x)}{(1+x)^{2}}\\;\\Longrightarrow\\;f'(x)\\geq 0\\;\\Longrightarrow x\\geq 0,\\;x\\leq -2 . \\] Consequently, \\(x=-2\\) is a local maximum, while \\(x=0\\) is a local minimum.",
    "crumbs": [
      "Home",
      "Teaching",
      "Calculus - Robotics Engineering - UC3M",
      "1 Limits of scalar functions"
    ]
  },
  {
    "objectID": "teaching/calculus-uc3m-robotics/1.2 Limits of scalar functions.html",
    "href": "teaching/calculus-uc3m-robotics/1.2 Limits of scalar functions.html",
    "title": "Distance and intervals",
    "section": "",
    "text": "Distance and intervals\nLet \\(f:X\\rightarrow Y\\) be a function. The notion of limit formalizes, rigorously, what happens to the output when the inputs gets closer and closer to a fixed value. Roughly speaking, we write \\[\n\\lim_{x\\rightarrow x_{0}} f(x)=l\n\\] meaning that the output \\(f(x)\\) gets closer to \\(l\\) if the input \\(x\\) is closer to \\(x_{0}\\).\nNote that , in general, it is not always true that a limit exists.\nIn order to make things precise, we first need to understand what we mean when we say that something is closer to a fixed value. In the case of real numbers, if we fix \\(x_{0}\\in \\mathbb{R}\\), we say that \\(x_{1}\\) is closer than \\(x_{2}\\) if \\[\n|x_{1}- x_{0}|\\leq |x_{2} - x_{0}|.\n\\] If \\(\\leq\\) is replaced by \\(&lt;\\), we say it is strictly smaller.\n\n[!definition] Euclidean distance function The two-point function \\(d\\colon \\mathbb{R}\\times\\mathbb{R}\\rightarrow\\mathbb{R}\\) given by \\[\nd(x,y)=\\sqrt{(x-y)^{2}}=|x-y|\n\\] is referred to as a Euclidean distance function on \\(\\mathbb{R}\\).\n\n\n[!remark] Remark An intuitive reason for \\(d\\) to measure distances follows from our direct experience, for instance with ruled paper (can you guess why/how?), but there are also more abstract reasons connected with the theory of metric spaces.\n\nBesides the notion of distance between real numbers, we need to introduce various types of intervals. These objects help in formalizing the way \\(x\\) approaches \\(x_{0}\\).\n\n[!definition] Definition: open and closed intervals Given \\(a,b\\in\\mathbb{R}\\), we introduce the open interval \\[\n(a,b):= \\{x\\in\\mathbb{R}\\,|\\quad a&lt; x &lt; b \\},\n\\] and the closed interval \\[\n[a,b]:= \\{x\\in\\mathbb{R}\\,|\\quad a\\leq x \\leq b \\}.\n\\]\n\n^19990d\nIntuitively speaking, the interval \\((a,b)\\) is open because its boundary points are not part of it, while \\([a,b]\\) is closed because its boundary points are part of it.\nQuite obviously, we also have the intervals \\[\n[a,b) := \\{x\\in\\mathbb{R}\\,|\\quad a\\leq x &lt; b\\}\n\\] \\[\n(a,b] := \\{x\\in\\mathbb{R}\\,|\\quad a &lt; x \\leq b\\},\n\\] which, however, are neither open nor closed.\nIt will be useful to also consider the symmetric open interval\n\\[ I_{x_{0},\\epsilon}\\equiv(x_{0}-\\epsilon, x_{0}+\\epsilon)=\\{x\\in\\mathbb{R}\\,|\\quad x_{0}-\\epsilon &lt; x &lt; x_{0} + \\epsilon\\} \\]\ncentered at \\(x_{0}\\) with length \\(2\\epsilon\\) with \\(\\epsilon&gt;0\\). It is not hard to see that if \\(x\\in I_{x_{0},\\epsilon}\\), then either \\(x-x_{0}&lt; \\epsilon\\) or \\(x_{0} - x &lt;\\epsilon\\). It then follows that \\(|x-x_{0}|&lt;\\epsilon\\), so that\n\\[ I_{x_{0},\\epsilon}=\\{x\\in\\mathbb{R}\\,| \\quad d(x_{0},x)&lt;\\epsilon\\}. \\] Therefore, the symmetric open intervals of \\(\\mathbb{R}\\) may be determined through the distance function.\nFinally, we can also describe unbounded intervals by introducing the formal symbols \\(+\\infty\\) and \\(-\\infty\\). In this case, we have the open unbounded intervals\n\\[\n\\begin{split}\n(a,+\\infty)&:=\\{x\\in\\mathbb{R}\\,|\\quad a&lt; x \\} \\\\ &\\\\ (-\\infty,a)&:=\\{x\\in\\mathbb{R}\\,|\\quad x&lt;a \\},\n\\end{split}\n\\]\nas well as the unbounded intervals\n\\[\n\\begin{split}\n[a,+\\infty)&:=\\{x\\in\\mathbb{R}\\,|\\quad a\\leq x\\} \\\\ &\\\\ (-\\infty,a]&:=\\{x\\in\\mathbb{R}\\,|\\quad x\\leq a\\},\n\\end{split}\n\\] which, however, are neither open nor closed.\n\n\nLimits of scalar functions\nAs mentioned above, the idea behind limits of functions is that of understanding the behavior around a point \\(x_{0}\\) in the input space of the function \\(f\\) under investigation. However, the point \\(x_{0}\\) is not arbitrary. It must be a left/right accumulation point for the domain of the function \\(f\\).\n\n[!definition] Definition: left/right accumulation point A point \\(x_{0}\\in \\mathbb{R}\\) is said to be a left accumulation point for the subset \\(D\\) if there is \\(r&gt;0\\) such that the open interval \\(I_{(x_{0},r)}\\) is contained in \\(D\\). A right accumulation point for \\(D\\) is then defined in the obvious way. A point which is a left (right) accumulation point but not a right (left) accumulation point is called a strict left accumulation point (strict right accumulation point). An accumulation point for \\(D\\) is a point that is both a left and right accumulation point for \\(D\\).\n\n^772289\n\n[!example|*] Example Let \\(D=[a,b]\\), then every \\(x\\in(a,b)\\) is an accumulation point, while \\(a\\) is a right accumulation point, and \\(b\\) is a left accumulation point. The same conclusions hold for \\([a,b)\\), \\((a,b]\\), and \\((a,b)\\). In the latter case, we may also let \\(a=-\\infty\\) or \\(b=+\\infty\\).\n\nThen, the notion of limit is formalized using the distance function introduced above and two numerical quantifiers, say \\(\\epsilon\\) and \\(\\delta\\), to explicitly evaluate how close we are to \\(l\\) and \\(x_{0}\\), respectively.\n\n[!definition] Definition: \\(\\epsilon\\)-\\(\\delta\\) definition of (left/right) limits Consider the function \\(f\\colon D\\subseteq \\mathbb{R}\\rightarrow \\mathbb{R}\\) and the left accumulation point \\(x_{0}\\) for \\(D\\). We say that the formal equality\n\\[\n\\lim_{x\\rightarrow x_{0}^{+}} f(x)=l\n\\] holds if for every \\(\\epsilon&gt;0\\) there is a \\(0&lt; \\delta\\) such that, for every \\(x_{0}&lt;x\\in D\\) satisfying \\(d(x_{0},x)=|x-x_{0}|&lt;\\delta\\), it holds \\[\nd(l,f(x))=|f(x)-l|&lt;\\epsilon .\n\\] In this case, we say that \\(l\\) is the right limit of \\(f\\) for \\(x\\) going to \\(x_{0}\\) from the right. Similarly, if \\(x_{0}\\) is a right accumulation point, we say that the formal equality\n\\[\n\\lim_{x\\rightarrow x_{0}^{-}} f(x)=l\n\\] holds if for every \\(\\epsilon&gt;0\\) there is a \\(0&lt; \\delta\\) such that, , for every \\(x_{0}&gt;x\\in D\\) satisfying \\(d(x_{0},x)=|x_{0}-x|&lt;\\delta\\), it holds \\[\nd(x_{0},x)=|x-x_{0}|&lt;\\delta \\Longrightarrow d(l,f(x))=|f(x)-l|&lt;\\epsilon .\n\\] In this case, we say that \\(l\\) is the left limit of \\(f\\) for \\(x\\) going to \\(x_{0}\\) from the left. If \\(x_{0}\\) is both a left and right accumulation point, we then say that the formal equality \\[\n\\lim_{x\\rightarrow x_{0}} f(x)=l\n\\] holds if both the left and right limit at \\(x_{0}\\) exist and are equal to \\(l\\). More specifically, the formal equality above holds if for every \\(\\epsilon&gt;0\\) there is a \\(0&lt; \\delta\\) such that, for every \\(x\\in D\\) satisfying \\(d(x_{0},x)=|x-x_{0}|&lt;\\delta\\), it holds \\[\nd(x_{0},x)=|x-x_{0}|&lt;\\delta \\Longrightarrow d(l,f(x))=|f(x)-l|&lt;\\epsilon\n\\] When this happens, we say that the limit of \\(f\\), for \\(x\\) going to \\(x_{0}\\), is \\(l\\).\n\n^cbf378\nThe previous definition is well-posed because the left/right limit, if it exists, is unique.\n\n[!proposition] Proposition: uniqueness of limits Consider the scalar function \\(f\\colon D\\subseteq\\mathbb{R}\\rightarrow \\mathbb{R}\\) and the left/right accumulation point \\(x_{0}\\) for \\(D\\). If the equalities \\[\n\\lim_{x\\rightarrow x_{0}^{\\pm}}f(x)=L\\quad\\mbox{ and }\\quad \\lim_{x\\rightarrow x_{0}^{\\pm}}f(x)=M\n\\] hold, then \\(L=M\\).\n\n^6fcbc5\nHere is an interactive visualization of the \\((\\epsilon-\\delta)\\)-definition of the limit of a function when everything works:\n\n\nHere is an interactive visualization of the \\((\\epsilon-\\delta)\\)-definition of the limit of a function when things do not work (note that a [[1.5 Power series, trascendental functions, and Taylor series#The trigonometric functions|trignometric function]] is used even if they are introduced in a later section):\n\n\n\n\nProperties of limits\nThe computation of limits through the definition given above is often difficult. As it is customary in mathematics, a set of simple cases is investigated using educated guesses (that is, the sophisticated siblings of sheer luck), and then general theorems are proved to help us exploit to the maximum the simple examples solved.\nThe first type of general results we consider are algebraic manipulations of limits. The following proposition, whose proof is an instructive exercise for the willful reader, essentially states that the limit of the sum/product/quotient is the sum/product/quotient of the limits.\nTo correctly write this proposition, we need to introduce the symbols \\(\\sum\\) and \\(\\prod\\) which help to condensate the writing of sums and products, respectively. Specifically, we set \\[\n\\sum\\limits_{j=1}^{n} A_{j}\\equiv A_{1} + A_{2} + \\cdots + A_{n}\n\\] \\[\n\\prod_{j=1}^{n}A_{j}=A_{1}\\,A_{2}\\,\\cdots\\,A_{n} ,\n\\] where \\((A_{1},\\cdots,A_{n})\\) can be a finite sequence of numbers or functions.\n\n[!important] Proposition: algebraic manipulations of limits Let \\((\\alpha_{1},\\cdots\\alpha_{n})\\) be a finite sequence of real numbers, \\((f_{1},\\cdots,f_{n})\\) a finite sequence of scalar functions, and \\(x_{0}\\in\\mathbb{R}\\) be a left accumulation point for the domains of all the functions mentioned before. Suppose that \\[\n\\lim_{x\\rightarrow x_{0}^{+}}f_{j}(x)=L_{j}\\in \\mathbb{R}\n\\] for every \\(j=1,...,n\\). Then, it holds\n\\[\n\\begin{split}\n1)\\quad & \\lim_{x\\rightarrow x_{0}^{+}} \\sum_{j=1}^{n}\\,\\alpha_{j}\\,f_{j}(x) =\\sum_{j=1}^{n}\\,\\lim_{x\\rightarrow x_{0}^{+}} \\left(\\alpha_{j}\\,f_{j}(x)\\right) = \\sum_{j=1}^{n}\\,\\alpha_{j}\\,L_{j} \\\\ & \\\\\n2) \\quad &\\lim_{x\\rightarrow x_{0}^{+}}\\prod_{j=1}^{n} \\,\\alpha_{j}\\,f_{j}(x) =\\prod_{j=1}^{n}\\,\\lim_{x\\rightarrow x_{0}^{+}} \\left(\\alpha_{j}\\,f_{j}(x)\\right) = \\prod_{j=1}^{n}\\,\\alpha_{j}\\,L_{j} .\n\\end{split}\n\\] Moreover, if \\(n=2\\) and \\(\\alpha_{2},L_{2}\\neq 0\\), it holds \\[\n\\lim_{x\\rightarrow x_{0}^{+}}\\,\\frac{\\alpha_{1}f_{1}(x)}{\\alpha_{2}f_{2}(x)}=\\frac{\\alpha_{1}L_{1}}{\\alpha_{2} L_{2}}.\n\\] Under the appropriate conditions, the same conclusions hold for left limits and for limits.\n\n^f50bb0\nThe next theorem is of paramount importance, and is known to humans by many names (very much like the Many-Faced God). Informally speaking, the underlying idea behind the theorem is that, if a function \\(f\\) lies between two other functions, say \\(g\\) and \\(h\\), the limits of \\(f\\) are determined by the limits of \\(g\\) and \\(h\\).\n\n[!important] Theorem: squeeze (pinching) theorem Let \\(f,g,h\\) be scalar functions with the same domain \\(D\\), and let \\(x_{0}\\) be a left accumulation point for \\(D\\). If \\(h(x)\\leq f(x)\\leq g(x)\\) for all \\(x\\in D\\) and \\[\n\\lim_{x\\rightarrow x_{0}^{+}}h(x)=\\lim_{x\\rightarrow x_{0}^{+}}g(x)=L\\in\\mathbb{R},\n\\] then it follows that \\[\n\\lim_{x\\rightarrow x_{0}^{+}}f(x)=L.\n\\] Under the appropriate conditions, the same conclusions hold for left limits and for limits.\n\n^ec717d\nNext is a proposition that regulates how limits behave with respect to composition of functions.\n\n[!important] Proposition: limit of composite functions Consider the scalar functions \\(f\\) and \\(g\\) such that \\[\n\\lim_{y\\rightarrow l} f(y) =L , \\qquad  \\lim_{x\\rightarrow x_{0}}g(x) = l.\n\\] If there is no open interval containing \\(x_{0}\\) on which \\(g(x)\\) is constantly equal to \\(l\\), then it follows that \\[\n\\lim_{x\\rightarrow x_{0}} f\\circ g(x)=L\n\\]\n\n^c8052c\nA useful corollary is obtained when \\(g(x)= x_{0} + x\\).\n\n[!important] Proposition\nConsider the scalar function \\(f\\) with domain \\(D\\), and the accumulation point \\(x_{0}\\). Then, it follows that \\[\n\\lim_{x\\rightarrow x_{0}}f(x)=L \\quad \\Longleftrightarrow \\quad \\lim_{h\\rightarrow 0}f(x_{0}+h)=L .\n\\]\n\n^7ca684\n\n\nContinuous functions\n^91cff8\nIf the left accumulation point \\(x_{0}\\) is also part of the domain of the function \\(f\\), it may happen that \\[\n\\lim_{x\\rightarrow x_{0}^{+}}f(x)=l=f(x_{0}) .\n\\] Functions of this type are particularly important in mathematics and its applications.\n\n[!defn] Definition: Continuity at a point Consider the function \\(f\\colon D\\subseteq \\mathbb{R}\\rightarrow \\mathbb{R}\\), and the [[1.2 Limits of scalar functions#^772289|left accumulation point]] \\(x_{0}\\in D\\). Then, \\(f\\) is said to be left continuous at \\(x_{0}\\in D\\) if \\[\n\\lim_{x\\rightarrow x_{0}^{-}}f(x)=f(x_{0}).\n\\] Right continuity is defined in the obvious way. The function \\(f\\) is said to be continuous at \\(x_{0}\\in D\\) if \\[\n\\lim_{x\\rightarrow x_{0}}f(x)=f(x_{0}),\n\\] which means that it is both left and right continuous at \\(x_{0}\\). The function \\(f\\colon D\\subseteq \\mathbb{R}\\rightarrow \\mathbb{R}\\) said to be continuous if it is continuous at all accumulation points \\(x_{0}\\in D\\), and left/right continuous at all right/left accumulation points \\(x_{0}\\in D\\).\n\n^979f48\n\n[!rem] Remark From an intuitive point of view, a scalar function is continuous if its graph can be drawn without lifting the pen/pencil from the paper, that is, in a continuous way.\n\nJust as there is an algebra of limits, there is an algebra of continuous functions.\n\n[!important] Proposition: Algebraic manipulations of continuous functions\nLet \\((\\alpha_{1},\\cdots\\alpha_{n})\\) be a finite sequence of real numbers, and \\((f_{1},\\cdots,f_{n})\\) a finite sequence of scalar functions which are all (left/right) continuous at \\(x_{0}\\). Then, it follows that \\[\nF(x)=\\sum\\limits_{j=1}^{n}\\alpha_{j}f_{j}(x),\\qquad G(x)=\\prod_{j=1}^{n}\\alpha_{j}f_{j}(x)\n\\] are (left/right) continuous at \\(x_{0}\\). Moreover, when \\(n=2\\) and \\(\\alpha_{2}f_{2}(x_{0})\\neq 0\\), it follows that \\[\nH(x)=\\frac{\\alpha_{1}f_{1}(x)}{\\alpha_{2}f_{2}(x)}\n\\] is (left/right) continuous at \\(x_{0}\\).\n\n^47f720\nMoreover, continuous functions behaves well with respect to composition of functions.\n\n[!important] Proposition: composition of continuous functions Let \\(g\\) be continuous at \\(x_{0}\\) and \\(f\\) be continuous at \\(g(x_{0})\\). Then, the composite function \\(f\\circ g\\) is continuous at \\(x_{0}\\). In particular, the restriction of a continuous function \\(f\\) to an open subset of its domain is continuous, and, if \\(f\\) is invertible and continuous on an open interval \\(I\\), its inverse \\(f^{-1}\\) is also continuous.\n\n^0478fa\n\n[!rem] Remark While the composition of continuous functions is continuous, it is not true that a continuous composite functions is the composition of continuous functions. Indeed, take\n\\[\nf(x)=g(x)= \\left\\{\\begin{matrix} \\frac{1}{x} & \\mbox{ if } x\\neq 0 \\\\ 0 & \\mbox{ if } x = 0\\end{matrix}\\right.\n\\]\nAs we will be able to appreciate once we introduce improper limits, these functions have an essential discontinuity at \\(x=0\\). However, the composite function \\(f\\circ g\\) reads \\[\nf\\circ g(x) = x\n\\] which is clearly continuous for every \\(x\\in\\mathbb{R}\\).\n\n^343b10\nTwo reasons why continuous functions are tremendously useful in applications are contained in the following theorems.\n\n[!important] Theorem: intermediate value theorem If \\(f\\) is continuos in \\([a,b]\\) and \\(K\\) is any number between \\(f(a)\\) and \\(f(b)\\), then there is at least one point \\(c\\in (a,b)\\) such that \\(f(c)=K\\).\n\n\n[!important] Theorem: extreme value theorem If \\(f\\) is continuous in \\([a,b]\\) then it attains a maximum and minimum value in \\([a,b]\\).\n\n^f806e3\n\n\nImproper limits\nLet us now consider the function \\(f(x)=\\frac{1}{x}\\) with domain \\(D=(-\\infty),0\\cup(0,+\\infty)\\). Since the constant function \\(c(x)=1\\) and the identity function \\(g(x)=x\\) are well-defined and continuous over all \\(\\mathbb{R}\\), we [[1.2 Limits of scalar functions#^47f720|conclude that]] \\(f\\) is continuous in all its domain. Recall that, since \\[\n\\lim_{x\\rightarrow 0} x=0,\n\\] we can not say anything about the limit of \\(f\\) for \\(x\\) going to \\(0\\) using the limits of \\(c(x)=1\\) and \\(g(x)=x\\) for \\(x\\) going to \\(0\\). However, it is clear that \\(0\\) is an accumulation point for \\(D\\), so that evaluating the (left/right) limit of \\(f\\) for \\(x\\) going to \\(0\\) makes sense.\nThe first observation that we make is that \\(f(x)&gt;0\\) when \\(x&gt;0\\), while \\(f(x)&lt;0\\) when \\(x&lt;0\\). Therefore, the limit of \\(f\\) for \\(x\\) going to \\(0\\) exists if and only if both the left and the right limit exist and are \\(0\\).\nThen, we can compute \\(f(x)\\) for values of \\(x\\) which are closer and closer to \\(0\\). For positive values of \\(x\\), we see that the closer \\(x\\) is to \\(0\\), the bigger \\(f(x)\\) is. However, for negative values of \\(x\\), the closer \\(x\\) is to \\(0\\), the more \\(f(x)\\) becomes bigger in absolute value but with negative sign. Therefore, both the left and right limits do not exist.\nThe problem seems solved, but we should not feel satisfied. Indeed, even if we understand that the left/right limit of \\(f\\) for \\(x\\) going to \\(0\\) do not exist, we can still intuitively say that we know how \\(f\\) behaves around \\(0\\): it grows indefinitely when \\(x\\rightarrow 0^+\\), and degrows indefinitely when \\(x\\rightarrow 0^{-}\\).\nCan we make this intuitive understanding rigorous?\nThe answer is affirmative, and leads to the definition of unbounded limit.\n\n[!defn] Definition: Unbounded limit Consider the function \\(f\\colon D\\subseteq \\mathbb{R}\\rightarrow \\mathbb{R}\\), and let \\(x_{0}\\) be a left accumulation point for \\(D\\). We say that \\[\n\\lim_{x\\rightarrow x_{0}^{+}} f(x)=+\\infty\n\\] if for every \\(K&gt;0\\) there is a \\(0&lt; \\delta\\) such that, , for every \\(x_{0}&lt;x\\in D\\) satisfying \\(d(x_{0},x)=|x-x_{0}|&lt;\\delta\\), it holds \\(f(x)&gt; K\\). Analogously, we say that \\[ \\lim_{x\\rightarrow x_{0}^{+}} f(x)=-\\infty\n\\] if for every \\(K&gt;0\\) there is a \\(0&lt; \\delta\\) such that , for every \\(x_{0}&lt;x\\in D\\) satisfying \\(d(x_{0},x)=|x-x_{0}|&lt;\\delta\\), it holds \\(f(x)&lt;- K\\). Right unbounded limits are defined analogously. Then, we say that \\[\n\\lim_{x\\rightarrow x_{0}}f(x)=\\pm \\infty\n\\] if both left and right unbounded limits at \\(x_{0}\\) exist and are equal.\n\n\n[!rem] Remark Exploiting the definition of unbounded limit, we can prove that \\[\n\\lim_{x\\rightarrow 0^{\\pm}}\\frac{1}{x}=\\pm\\infty,\n\\] so that the right and left limits at \\(0\\) exist but are not equal, so that \\(f(x)\\) has no limit for \\(x\\rightarrow 0\\).\n\nAnother relevant instance for which we need to introduce yet another notion of improper limit is that in which we want to understand what happens when the input variable does not approach a finite value but either grows indefinitely, or degrows indefinitely (when possible).\n\n[!defn] Definition: Limit at \\(+\\infty\\) Let \\(f\\colon D\\rightarrow \\mathbb{R}\\) be such that \\(+\\infty\\) is a right accumulation point for \\(D\\). We say that \\[\n\\lim_{x\\rightarrow +\\infty} f(x)= l\n\\] with \\(l\\in\\mathbb{R}\\), if for every \\(\\epsilon&gt;0\\) there is \\(M&gt;0\\) such that, for every \\(x\\in D\\) satisfying \\(x&gt;M\\), it holds \\(|f(x) -l|&lt; \\epsilon\\). We say that \\[\n\\lim_{x\\rightarrow +\\infty} f(x)=+\\infty\n\\] if for every \\(K&gt;0\\) there is a \\(M&gt;0\\) such that, for every \\(x\\in D\\) satisfying \\(x&gt;M\\), it holds \\(f(x)&gt; K\\). We say that \\[\n\\lim_{x\\rightarrow +\\infty} f(x)=-\\infty\n\\] if for every \\(K&gt;0\\) there is a \\(M&gt;0\\) such that, for every \\(x\\in D\\) satisfying \\(x&gt;M\\), it holds \\(f(x)&lt; -K\\).\n\n^9b2c4b\n\n[!defn] Definition: Limit at \\(-\\infty\\) Let \\(f\\colon D\\rightarrow \\mathbb{R}\\) be such that \\(-\\infty\\) is a left accumulation point for \\(D\\). We say that \\[\n\\lim_{x\\rightarrow -\\infty} f(x)= l\n\\] with \\(l\\in\\mathbb{R}\\), if for every \\(\\epsilon&gt;0\\) there is \\(M&gt;0\\) such that, for every \\(x\\in D\\) satisfying \\(x&lt;-M\\), it holds \\(|f(x) -l|&lt; \\epsilon\\). We say that \\[\n\\lim_{x\\rightarrow -\\infty} f(x)=+\\infty\n\\] if for every \\(K&gt;0\\) there is a \\(M&gt;0\\) such that, for every \\(x\\in D\\) satisfying \\(x&lt;-M\\), it holds \\(f(x)&gt; K\\). We say that \\[\n\\lim_{x\\rightarrow -\\infty} f(x)=-\\infty\n\\] if for every \\(K&gt;0\\) there is a \\(M&gt;0\\) such that, for every \\(x\\in D\\) satisfying \\(x&lt;-M\\), it holds \\(f(x)&lt; -K\\).\n\nThe [[1.2 Limits of scalar functions#^f50bb0|algebraic manipulations of limits]] we encountered before can be appropriately modified to handle also improper limits, provided we handle infinities with care.\n\n[!important] Proposition: Algebraic manipulation of limits 2 Let \\(f,g\\colon D\\rightarrow \\mathbb{R}\\) be scalar function, and let \\(x_{0}\\) be a left/right accumulation point for \\(D\\) (possibly, also \\(\\pm\\infty\\)). If the right/left limit of both \\(f\\) and \\(g\\) exist at \\(x_{0}\\), then we have: \\[\n\\begin{split}\n1) & \\lim_{x\\rightarrow x_{0}^{\\pm}}(f(x) + g(x))= \\lim_{x\\rightarrow x_{0}^{\\pm}}f(x) + \\lim_{x\\rightarrow x_{0}^{\\pm}} g(x) \\\\ & \\\\\n2) & \\lim_{x\\rightarrow x_{0}^{\\pm}}(f(x) - g(x))= \\lim_{x\\rightarrow x_{0}^{\\pm}}f(x) - \\lim_{x\\rightarrow x_{0}^{\\pm}} g(x) \\\\ & \\\\\n3) & \\lim_{x\\rightarrow x_{0}^{\\pm}}f(x)\\,\\cdot \\, g(x)= \\lim_{x\\rightarrow x_{0}^{\\pm}}f(x)\\, \\cdot  \\lim_{x\\rightarrow x_{0}^{\\pm}} g(x) \\\\ & \\\\\n4) & \\lim_{x\\rightarrow x_{0}^{\\pm}}\\frac{f(x)}{g(x)}= \\frac{\\lim_{x\\rightarrow x_{0}^{\\pm}}f(x)}{\\lim_{x\\rightarrow x_{0}^{\\pm}} g(x)} \\\\ & \\\\\n5) & \\lim_{x\\rightarrow x_{0}^{\\pm}}(f(x))^{g(x)}= \\left(\\lim_{x\\rightarrow x_{0}^{\\pm}}f(x)\\right)^{\\lim_{x\\rightarrow x_{0}^{\\pm}} g(x)}  \n\\end{split}\n\\] with the following “conventions”: \\[\n\\begin{split}\n1) & \\quad  l + (\\pm\\infty) =\\pm \\infty \\;\\mbox{ if }\\;\\; l\\neq \\mp\\infty \\\\ & \\\\\n2) & \\quad l\\cdot \\pm\\infty = \\pm \\infty  \\;\\mbox{ if }\\;\\; l&gt;0\\\\ & \\\\\n3) & \\quad l\\cdot \\pm\\infty = \\mp \\infty  \\;\\mbox{ if }\\;\\; l&lt;0\\\\ & \\\\\n4) & \\quad \\frac{l}{\\pm\\infty} = 0  \\;\\mbox{ if }\\;\\; l\\neq  \\pm \\infty\\\\ & \\\\\n5) & \\quad \\frac{\\pm\\infty}{l} = \\pm \\infty  \\;\\mbox{ if }\\;\\; l\\in ( 0,+\\infty)\\\\ & \\\\\n6) & \\quad \\frac{\\pm\\infty}{l} = \\mp \\infty  \\;\\mbox{ if }\\;\\; l\\in (-\\infty,0)\\\\ & \\\\\n7) & \\quad (+\\infty)^{l} =  +\\infty \\;\\mbox{ if }\\;\\; l&gt; 0\\\\ & \\\\\n8) & \\quad (+\\infty)^{l} =  0 \\;\\mbox{ if }\\;\\; l&lt; 0\\\\ & \\\\\n9) & \\quad l^{+\\infty} = 0  \\;\\mbox{ if }\\;\\; 0&lt;l&lt; 1\\\\ & \\\\\n10) & \\quad l^{+\\infty} = +\\infty  \\;\\mbox{ if }\\;\\; l&gt; 1\\\\ & \\\\\n11) & \\quad l^{-\\infty} = 0  \\;\\mbox{ if }\\;\\; l&gt; 1\\\\ & \\\\\n12) & \\quad l^{-\\infty} = +\\infty  \\;\\mbox{ if }\\;\\; 0&lt;l&lt; 1 .\n\\end{split}\n\\] All other cases are called indeterminate forms, and a case by case analysis is needed to determine if the limit exists or not.\n\n^55a3be\n\n\nDiscontinuities\nConsider the function \\(f\\colon D\\rightarrow\\mathbb{R}\\), and let \\(x_{0}\\) be an accumulation point for \\(D\\). Four possibilities present to us:\n\nthe limit at \\(x_{0}\\in D\\) exists, but either \\(x_{0}\\) is not in \\(D\\), or the limit is different from \\(f(x_{0})\\) (removable discontinuity);\nthe right and left limits at \\(x_{0}\\in D\\) exist, are finite, but are different (jump discontinuity);\nthe right and left limits at \\(x_{0}\\) exist but at least one of them is either \\(+\\infty\\) or \\(-\\infty\\) (essential discontinuity);\nat least one between the left and right limits do not exist.\n\nIn the first case (removable discontinuity), we can define a new function \\(\\tilde{f}\\colon D\\cup\\{x_{0}\\}\\rightarrow\\mathbb{R}\\) by setting \\[\n\\tilde{f}(x)=\\left\\{\\begin{matrix} f(x) & \\mbox{ if } x_{0}\\neq x\\in D \\\\ & \\\\  \\lim_{x\\rightarrow x_{0}}f(x) & \\mbox{ if } x=x_{0}\\end{matrix}\\right. .\n\\] This new function is called an extension by continuity of \\(f\\) and it is a continuous function at \\(x_{0}\\) by construction. Note that this procedure works whether \\(x_{0}\\) is in the domain \\(D\\) of the original function or not. If \\(x_{0}\\) were in \\(D\\), then we are redefining \\(f\\) at that point. If \\(x_{0}\\) was not in \\(D\\), then we are genuinely extending \\(f\\).\nIn the second case (jump discontinuity), whether \\(x_{0}\\) lies in \\(D\\) or not, we can not redefine \\(f\\) in such a way that the result is a continuous function at \\(x_{0}\\). The best we can do is obtain an extension of \\(f\\) which is either left or right continuous at \\(x_{0}\\).\nIn the third case (essential singularity), there is nothing we can do to ameliorate the discontinuities of the function \\(f\\). This is the case considered in [[1.2 Limits of scalar functions#^343b10|a previous]] example.",
    "crumbs": [
      "Home",
      "Teaching",
      "Calculus - Robotics Engineering - UC3M",
      "Distance and intervals"
    ]
  },
  {
    "objectID": "teaching/calculus-uc3m-robotics/1.6 Derivatives and extrema of scalar functions.html",
    "href": "teaching/calculus-uc3m-robotics/1.6 Derivatives and extrema of scalar functions.html",
    "title": "Derivatives and local behavior",
    "section": "",
    "text": "Derivatives and local behavior\nWhen using functions to model physical situations, it is often very helpful to be able to determine whether there are points at which the function takes a maximum or minimum value. It turns out that, for differentiable functions, derivatives are quite helpful in looking for these points.\n\n[!defn] Definition: increasing and decreasing functions A scalar function \\((D,\\mathbb{R},f,\\mathbb{R})\\) is said to be increasing in the interval \\(I\\subseteq D\\) if \\(x\\leq y\\) implies \\(f(x)\\leq f(y)\\) for all \\(x,y\\in I\\). Similarly, it is said to be decreasing in the interval \\(I\\) if \\(x\\leq y\\) implies \\(f(x)\\geq f(y)\\) for all \\(x,y\\in I\\).\n\n\n[!defn] Definition: interior and boundary points Given the function \\((D,\\mathbb{R},f,\\mathbb{R})\\), a point \\(x_{0}\\in D=\\mathrm{Dom}(f)\\) is said to be an interior point if there exists \\(\\delta&gt;0\\) such that the open interval \\(I_{x_{0},\\delta}:=(x_{0}-\\delta,x_{0}+\\delta)\\) lies inside \\(\\mathrm{Dom}(f)\\). A point which is not an interior point is called a boundary point. A boundary point is either a [[1.2 Limits of scalar functions#^772289|left or right accumulation point]] which belongs to the domain. ^efd1a3\n\n\n[!important] Proposition: derivatives and local behavior Let \\((D,\\mathbb{R},f,\\mathbb{R})\\) be differentiable at the [[1.6 Derivatives and extrema of scalar functions#^efd1a3|interior point]] \\(x_{0}\\). It follows that: 1) if \\(f'(x_{0})&gt;0\\), the function is increasing in \\(I_{x_{0},\\delta}\\) for some \\(\\delta&gt;0\\); 2) if \\(f'(x_{0})&lt;0\\) , the function is decreasing in \\(I_{x_{0},\\delta}\\) for some \\(\\delta&gt;0\\).\nMoreover, if \\((D,\\mathbb{R},f,\\mathbb{R})\\) is differentiable in the open interval \\(I\\), then: 1) if \\(f'(x)&gt;0\\) for all \\(x\\in I\\), the function is increasing in \\(I\\); 2) if \\(f'(x)&lt;0\\) for all \\(x\\in I\\), the function is decreasing in \\(I\\); 3) if \\(f'(x)=0\\) for all \\(x\\in I\\), the function is constant in \\(I\\) and the converse holds.\nFor a closed interval \\(I=[a,b]\\subseteq D\\) (\\(a,b\\neq \\pm\\infty\\)), the previous conclusions hold provided \\((D,\\mathbb{R},f,\\mathbb{R})\\) is differentiable in the interior \\((a,b)\\) of \\(I\\) and continuous in the whole \\(I=[a,b]\\). ^adc8fa\n\n\n\nLocal extrema and where/how to find them\nWe are now ready to define what local extrema are.\n\n[!defn] Definition: local extrema Given the function \\((D,\\mathbb{R},f,\\mathbb{R})\\) and the [[1.6 Derivatives and extrema of scalar functions#^efd1a3|interior point]] \\(x_{0}\\in D=\\mathrm{Dom}(f)\\), then: 1) if there is \\(\\epsilon&gt;0\\) such that \\(f(x_{0})&gt;f(x)\\) for all \\(x\\) in \\((x_{0}-\\epsilon,x_{0}+\\epsilon)\\), the function has a local maximum at \\(x_{0}\\); 2) if there is \\(\\epsilon&gt;0\\) such that \\(f(x_{0})&lt;f(x)\\) for all \\(x\\) in \\((x_{0}-\\epsilon,x_{0}+\\epsilon)\\), the function has a local minimum at \\(x_{0}\\).\nIn general, \\((D,\\mathbb{R},f,\\mathbb{R})\\) is said to have a local extremum at \\(x_{0}\\) if \\(x_{0}\\) is either a local maximum or a local minimum of \\(f\\).\n\nGiven how [[1.6 Derivatives and extrema of scalar functions#^adc8fa|derivatives dictate local behavior]], it is clear that a local extremum can not be a point \\(x_{0}\\) at which either \\(f'(x_{0})&gt;0\\) or \\(f'(x_{0})&lt;0\\). We are thus left with only types of points that can are candidates to be local extrema. These points deserve a name of their own.\n\n[!defn] Definition: critical point Given the function \\((D,\\mathbb{R},f,\\mathbb{R})\\), an [[1.6 Derivatives and extrema of scalar functions#^efd1a3|interior point]] \\(x_{0} \\in D=\\mathrm{Dom}(f)\\) is called a critical point if either \\(f'(x_{0})=0\\) or \\(f'(x_{0})\\) does not exist. ^d2fcd0\n\n\n[!important] Proposition: local extrema and critical points If \\((D,\\mathbb{R},f,\\mathbb{R})\\) has a local extremum at the [[1.6 Derivatives and extrema of scalar functions#^efd1a3|interior point]] \\(x_{0}\\in  D=\\mathrm{Dom}(f)\\), then \\(x_{0}\\) is a [[1.6 Derivatives and extrema of scalar functions#^d2fcd0|critical point]] of \\((D,\\mathbb{R},f,\\mathbb{R})\\).\n\n\n[!important] Rolle’s theorem Let \\(a,b\\in\\mathbb{R}\\) and \\(([a,b],\\mathbb{R},f,\\mathbb{R})\\) be continuous and differentiable in \\((a,b)\\). If \\(f(a)=f(b)\\) then there is \\(y\\in(a,b)\\) such that \\(f'(y)=0\\).\n\n\n[!important] Mean value theorem Let \\(a,b\\in\\mathbb{R}\\) and \\(([a,b],\\mathbb{R},f,\\mathbb{R})\\) be continuous and differentiable in \\((a,b)\\). Then, there is \\(y\\in(a,b)\\) such that \\(f'(y)=\\frac{f(b)-f(a)}{b-a}\\).\n\nWe can collect all the results summarized above to obtain a criteria for discovering local extrema of differentiable scalar functions.\n\n[!important] Proposition: first derivative test for local extrema Given the function \\((D,\\mathbb{R},f,\\mathbb{R})\\) and a [[1.6 Derivatives and extrema of scalar functions#^d2fcd0|critical point]] \\(x_{0}\\in D\\), we have that: 1) if there exists \\(\\delta&gt;0\\) such that \\(f'(x)&gt;0\\) for \\(x\\in (x_{0}-\\delta,x_{0})\\) and \\(f'(x)&lt;0\\) for \\(x\\in (x_{0},x_{0} + \\delta)\\) then \\(x_{0}\\) is a local maximum for \\((D,\\mathbb{R},f,\\mathbb{R})\\); 2) if there exists \\(\\delta&gt;0\\) such that \\(f'(x)&lt;0\\) for \\(x\\in (x_{0}-\\delta,x_{0})\\) and \\(f'(x)&gt;0\\) for \\(x\\in (x_{0},x_{0} + \\delta)\\) then \\(x_{0}\\) is a local minimum for \\((D,\\mathbb{R},f,\\mathbb{R})\\).\n\n\n[!attention] Remark: critical points need not be local extrema Note that critical points need not be extrema. Indeed, consider the function \\((\\mathbb{R},\\mathbb{R},f,\\mathbb{R})\\) given by \\(f(x)=x^{3}\\) and the interior point \\(x_{0}=0\\). A direct computation shows that \\(x_{0}\\) is a critical point because \\(f'(0)=0\\). However, \\(f'(x)&gt;\\) whenever \\(x\\neq 0\\), and the function is increasing in the whole \\(D\\).\n\n^f4bbab\nIf we are blessed by the existence of the second derivative of our function, we may rely also on the following proposition.\n\n[!important] Proposition: second derivative test for local extrema Given the function \\((D,\\mathbb{R},f,\\mathbb{R})\\) and a [[1.6 Derivatives and extrema of scalar functions#^d2fcd0|critical point]] \\(x_{0}\\in D\\), assuming the second derivative \\(f''(x_{0})\\) exists, we have that: 1) if \\(f''(x_{0})&lt;0\\) then \\(x_{0}\\) is a local maximum for \\(f\\); 2) if \\(f''(x_{0})&gt;0\\) then \\(x_{0}\\) is a local minimum for \\(f\\); 3) if \\(f''(x_{0})=0\\) then you should seek for help somewhere else in the beautiful and wild realm of Mathematics.\n\n^c80ae8\nIt is true that the second derivative test may be computationally faster in most cases, so we may be inclined to dismiss the first derivative test, and forget it exists. However, doing it would be a mistake. The second derivative test requires second derivative to exist, and this is not always the case.\n\n[!attention] Remark: the first derivative test is more general than the second derivative test As anticipated, there are situations in which the first derivative test can be applied while the second derivative test can not be applied. The prototypical such example is \\(f(x)=|x|\\) and \\(x_{0}=0\\).\n\nIn conclusion, never rely on the fastest/simple method without previously thinking and understanding the conceptual situation at hand.\n\n\nGlobal extrema and where/how to find them\nLocal extrema are, quite obviously, local. This means that, even if \\(x_{0}\\) is a local maximum, there may be a point \\(x_{1}\\) such that \\(f(x_{1})&gt;f(x_{0})\\) as long as \\(x_{1}\\) is sufficiently far from \\(x_{0}\\). Moreover, it may also happen that, again when we are sufficiently far from \\(x_{0}\\), the function grows or degrows indefinitely. For instance, a direct application of the [[1.6 Derivatives and extrema of scalar functions#^c80ae8|second derivative test]] shows that the [[1.3 Derivatives of scalar functions#^a6b2f2|smooth function]] \\((\\mathbb{R},\\mathbb{R},f,\\mathbb{R})\\) given by \\(f(x)=x^{3} +x^{2}\\) has a local maximum at \\(x=-\\frac{2}{3}\\). However, it is also true that \\[ \\lim_{x\\rightarrow +\\infty}f(x)=+\\infty ,\\] from which it follows that the function grows indefinitely as \\(x\\) grows.\nWe thus define global extrema as follows.\n\n[!defn] Definition: global extrema Given the function \\((D,\\mathbb{R},f,\\mathbb{R})\\) and the point \\(x_{0}\\in D\\), we have that: 1) \\(x_{0}\\) is a global maximum if \\(f(x_{0})&gt;f(x)\\) for all \\(x\\in D\\); 2) \\(x_{0}\\) is a global minimum if \\(f(x_{0})&lt;f(x)\\) for all \\(x\\in D\\);\nIn general, the point \\(x_{0}\\) is called a global extremum if it is either a global maximum or a global minimum.\n\nFor continuous functions whose domain \\(D\\) is closed and bounded, global extrema exist because of the [[1.2 Limits of scalar functions#^f806e3|extreme value theorem]] we recall here:\n\n[!important] Theorem (Extreme value theorem) If \\(f\\) is continuous in \\([a,b]\\) then it attains a maximum and minimum value in \\([a,b]\\).\n\nWhen the function is not continuous or the domain is not closed, or not bounded, global extrema may or may not exist. To check if global extrema exist, we need to obviously look for local extrema (if they exist), and then compute the limit at each [[1.2 Limits of scalar functions#^772289|strict left/right accumulation point]] and/or at \\(\\pm\\infty\\) (if possible).\nThen, we are faced with a multitude of situations depending on the results of the previous computations. It would be quite uninspiring to make a list of the possible cases. It is better to refer to exercises to train how to investigate the possible cases (you are also invited to consult this page and this other page for additional insights).",
    "crumbs": [
      "Home",
      "Teaching",
      "Calculus - Robotics Engineering - UC3M",
      "Derivatives and local behavior"
    ]
  },
  {
    "objectID": "teaching/calculus-uc3m-robotics/3.2 Integrals.html",
    "href": "teaching/calculus-uc3m-robotics/3.2 Integrals.html",
    "title": "Definite integrals and anti-derivatives",
    "section": "",
    "text": "Definite integrals and anti-derivatives\nConsider the scalar function \\(f\\colon D\\subseteq\\mathbb{R}\\rightarrow\\mathbb{R}\\) and the closed and bounded interval \\([a,b]\\subseteq D\\), and build the set \\[\nS_{[a,b]}^{f}:=\\left\\{(x,y)\\in\\mathbb{R}^{2}\\,|\\quad x\\in[a,b],\\;\\;y=f(x)\\right\\} .\n\\] The notion of definite integral of \\(f\\) over \\([a,b]\\) gives rigorous meaning to the idea of measuring the area \\(\\mathrm{A}(S_{[a,b]}^{f})\\) of \\(S_{[a,b]}^{f}\\) in such a way that this mathematical construction reduces to the results obtained by elementary geometry in simple cases like the area of a square (e.g., \\(f\\colon \\mathbb{R}\\rightarrow \\mathbb{R}\\) given by \\(f(x)=1\\) over the interval \\([0,1]\\)), or the area of a triangle (e.g., \\(f(x)=x\\) over the interval \\([0,1]\\)).\nLet us start with a function \\(f\\colon D\\subseteq\\mathbb{R}\\rightarrow \\mathbb{R}\\) which is non-negative and increasing. The main idea behind the procedure of integrating \\(f\\) is to approximate the area of \\(\\mathrm{A}(S_{[a,b]}^{f})\\) “from above” and “from below”. ^964215\nTo better understand the idea, we visualize the procedure of finding the “approximation from below”, on the left, and “from above”, on the right (source-left, source-right):\n![[Integral_lower_upper_sum.gif]]\nOther helpful visualizations can be found here.\nMore mathematically, we first divide \\([a,b]\\) in two intervals selecting \\(c\\in(a,b)\\). The approximation of \\(\\mathrm{A}(S_{[a,b]}^{f})\\) from below will be given by \\[\n\\mathrm{A}_{2}^{-}(S_{[a,b]}^{f})=(c-a)(f(a)) + (b-c)(f(c)).\n\\] This is nothing but the sum of the area of the rectangle with vertices \\((a,0)\\), \\((c,0)\\), \\((a,f(a))\\), \\((c,f(a))\\) with the area of the rectangle with vertices \\((c,0)\\), \\((b,0)\\), \\((c,f(c))\\), \\((b,f(c))\\). Quite similarly, the approximation of \\(\\mathrm{A}(S_{[a,b]}^{f})\\) from above will be given by \\[\n\\mathrm{A}_{2}^{+}(S_{[a,b]}^{f})=(c-a)(f(c)) + (b-c)(f(b)).\n\\]\nWe go on and divide \\([a,b]\\) in 3 parts and compute \\(\\mathrm{A}_{3}^{-}(S_{[a,b]}^{f})\\) and \\(\\mathrm{A}_{3}^{+}(S_{[a,b]}^{f})\\), and we go on increasing the number of parts in which we divide \\([a,b]\\). If the function \\(f\\) is “nice enough”, this procedure will give us two sequences, say \\(\\mathrm{A}_{n}^{-}(S_{[a,b]}^{f})\\) and \\(\\mathrm{A}_{n}^{+}(S_{[a,b]}^{f})\\), that converge to the same limit which is the area \\(\\mathrm{A}(S_{[a,b]}^{f})\\) we were looking for.\nThe procedure outlined above can be generalized to functions which are not necessarily non-negative, and not necessarily increasing, and all functions for which \\(\\mathrm{A}(S_{[a,b]}^{f})\\) actually exists are called (Darboux or Riemann) integrable. Of course the interpretation of the integral as an area fails for functions admitting negative values, unless we allow for a more flexible idea of areas which contemplates also negative values.\nFor reasons that will be clear below, the integral \\(\\mathrm{A}(S_{[a,b]}^{f})\\) of \\(f\\colon D\\subseteq\\mathbb{R}\\rightarrow\\mathbb{R}\\) over \\([a,b]\\) is denoted as \\[\n\\mathrm{A}(S_{[a,b]}^{f})=\\int_{a}^{b} f(x)\\,\\mathrm{d}x,\n\\] where it is clear that we are borrowing the notation from the anti-derivative operation.\nThe existence of the integral of a function is not guaranteed in general.\n\n[!important] Proposition: continuous functions are integrable Let \\(f\\colon [a,b]\\subset \\mathbb{R}\\rightarrow\\mathbb{R}\\) be continuos except at most at \\(a\\) or \\(b\\), then the integral \\(\\mathrm{A}(S_{[a,b]}^{f})\\) exists.\n\nAs you might guess, evaluating the integral of a function by means of the approximation procedures described above is incredibly difficult, even for “nice” functions. However, some talented humans discovered a deep, beautiful, and perhaps surprising link between integrals and anti-derivatives that greatly simplify the task.\n\n[!important] Theorem: first fundamental theorem of calculus Let \\(f\\colon [a,b]\\rightarrow \\mathbb{R}\\) be continuous. For every \\(x\\in [a,b]\\), the function \\[\nF(x):=\\int_{a}^{x}f(t)\\,\\mathrm{d}t =\\mathrm{A}(S_{[a,x]}^{f})\n\\] is continuous on \\([a,b]\\) and differentiable in \\((a,b)\\). Moreover, its derivative satisfies \\[\nF'(x) =f(x)\n\\] which means that \\(F(x)\\) is an anti-derivative of \\(f(x)\\).\n\n^0523d5\n\n[!important] Theorem: second fundamental theorem of calculus Let \\(f\\colon [a,b]\\rightarrow\\mathbb{R}\\) be such that it admits a continuous anti-derivative \\(F\\) at all \\(x\\in (a,b)\\). Then, if \\(f\\) is integrable it holds \\[\n\\mathrm{A}(S_{[a,b]}^{f})=\\int_{a}^{b}f(t)\\,\\mathrm{d}t=F(b) - F(a) .\n\\]\n\nEven if the two theorems above are similar, they are different. For instance, the first one assumes \\(f\\) to be continuous so that its anti-derivative is a [[1.3 Derivatives of scalar functions#^c07f2b|\\(C^{1}\\)-function]], while the second one only assumes \\(F\\) to be differentiable. Then, the first one determines the anti-derivative \\(F\\) uniquely since \\[\nF(a)=\\int_{a}^{a}f(t)\\,\\mathrm{d}t = \\mathrm{A}(S_{[a,a]}^{f}) =0\n\\] because the area over a single point is \\(0\\). However, we know that \\(G(x)=F(x) + c\\) with \\(c\\in\\mathbb{R}\\) is another good anti-derivative. Indeed, \\(G(x)\\) can be used to compute the integral of \\(f\\) using the second theorem since \\[\nF(b)\\overbrace{=}^{\\mathrm{Thm}\\,1}\\mathrm{A}(S_{[a,b]}^{f})=\\int_{a}^{b}f(t)\\mathrm{d}t\\overbrace{=}^{\\mathrm{Thm}\\,2}G(b) - G(a)\\overbrace{=}^{G(x)=F(x)+c} F(b) +c - F(a) - c \\overbrace{=}^{F(a)=0} F(b) .\n\\]\nIn the following, when computing \\(\\mathrm{A}(S_{[a,b]}^{f})\\) we simply look for an anti-derivative \\(F\\) of \\(f\\) and then write \\(\\mathrm{A}(S_{[a,b]}^{f})=F(b)-F(a)\\).\nWe list now some important results concerning definite integral.\n\n[!important] Proposition: properties of the integral Let \\(f,g\\colon[a,b]\\subset\\mathbb{R}\\rightarrow\\mathbb{R}\\) be integrable, and let \\(\\alpha,\\beta\\in\\mathbb{R}\\). It holds that \\[\n\\begin{split}\n\\int_{c}^{c}f(t)\\,\\mathrm{d}t&=0 \\\\ & \\\\\n\\int_{c}^{d}f(t)\\,\\mathrm{d}t&= - \\int_{d}^{c}f(t)\\,\\mathrm{d}t \\\\ & \\\\ \\int_{a}^{b}\\left(\\alpha f(t) + \\beta g(t)\\right) \\,\\mathrm{d}t &= \\alpha \\int_{a}^{b}f(t) \\,\\mathrm{d}t+ \\beta \\int_{a}^{b}g(t) \\,\\mathrm{d}t ,\n\\end{split}\n\\] where the last property is known as the linearity of the integral.\n\n\n[!important] Proposition: interval additivity Let \\(a&lt;b&lt;c\\) be real numbers. The function \\(f\\colon [a,c]\\rightarrow\\mathbb{R}\\) is integrable on \\([a,c]\\) if and only if it is integrable in \\([a,b]\\) and \\([b,c]\\). Moreover, it holds \\[\n\\int_{a}^{c}f\\;\\mathrm{d}x = \\int_{a}^{b}f\\;\\mathrm{d}x + \\int_{b}^{c}f\\;\\mathrm{d}x .\n\\] In particular, a function \\(f\\colon [a,b]\\rightarrow \\mathbb{R}\\) which is continuous except at a finite (but perhaps arbitrarily big) number of points in \\([a,b]\\) is integrable in \\([a,b]\\).\n\n^54f5d8\n\n[!important] Proposition Let \\(f\\colon[a,b]\\rightarrow\\mathbb{R}\\) be continuous, and let \\(g_{1},g_{2}\\colon(a,b)\\rightarrow\\) be differentiable. Then \\[\nH(x):=\\int_{g_{1}(x)}^{g_{2}(x)}f(t)\\;\\mathrm{d}t\n\\] is differentiable in \\((a,b)\\) and it is such that \\[\nH'(x)=f(g_{2}(x))\\,g_{2}'(x) - f(g_{1}(x))\\,g_{1}'(x)\n\\] for all \\(x\\in (a,b)\\).\n\n\n[!important] Proposition (Change of variables for definite integrals) Let \\(g\\colon[a,b]\\rightarrow \\mathbb{R}\\) be continuous and differentiable in \\((a,b)\\), set \\(I=g([a,b])\\), and let \\(f\\colon I\\subset\\mathbb{R}\\rightarrow \\mathbb{R}\\) be continuous. Then \\[\n\\int_{g(a)}^{g(b)}\\,f(t)\\;\\mathrm{d}t= \\int_{a}^{b}\\,f\\circ g(x)\\;\\mathrm{d}x .\n\\]",
    "crumbs": [
      "Home",
      "Teaching",
      "Calculus - Robotics Engineering - UC3M",
      "Definite integrals and anti-derivatives"
    ]
  },
  {
    "objectID": "teaching/calculus-uc3m-robotics/1.3 Derivatives of scalar functions.html",
    "href": "teaching/calculus-uc3m-robotics/1.3 Derivatives of scalar functions.html",
    "title": "Derivatives of scalar functions",
    "section": "",
    "text": "Derivatives of scalar functions\nThe derivative of the function \\(f\\) at a point \\(x_{0}\\) is nothing but a particular limit associated with \\(f\\) and \\(x_{0}\\). A way to understand it is to think to the following physical instance. There is an object \\(O\\) moving on a line. Its position may be thought of as a function \\(\\gamma\\colon I\\rightarrow\\mathbb{R}\\) where \\(I=(-t^{*},t^{*})\\) is a time interval, and \\(\\gamma(t)\\) is the distance between \\(O\\) and a reference point at time \\(t\\).\nAssociated with \\(\\gamma\\) and a choice of initial time \\(t_{0}\\), there is the function \\[\n\\Delta \\gamma (t):=\\frac{ \\gamma(t)-\\gamma(t_{0})}{t-t_{0}}.\n\\] This function tells us the rate of change of the position of \\(O\\) in the time interval \\(\\Delta t=t-t_{0}\\). Obviously, if \\(\\Delta t\\) is fixed, the bigger \\(\\Delta \\gamma\\) the further \\(O\\) has moved from the origin point \\(\\gamma(t_{0})\\). Consequently, \\(\\Delta \\gamma\\) seems like a nice candidate to mathematically represent what we intuitively picture as velocity/rapidity.\nHowever, upon closer examination, \\(\\Delta \\gamma\\) can only inform us about velocity/rapidity in the time interval \\(\\Delta t=t-t_{0}\\), and we conclude \\(\\Delta \\gamma\\) is more like an average velocity in the time interval \\(\\Delta t\\) rather than an instantaneous at \\(t=t_{0}\\).\nWhat happens when \\(t=t_{0}\\)?\nLooking at the very definition of \\(\\Delta \\gamma\\), we immediately realize that \\(t=t_{0}\\) brings \\(0\\) in the denominator. We are thus led to a meaningless expression unless we remember that we introduced [[1.2 Limits of scalar functions#^cbf378|limits]] precisely to deal with these type of situations. Once limits enter the game, we obtain the notion of (left/right) derivative of \\(\\gamma\\) at the point \\(t=t_{0}\\), and instantaneous velocities suddenly make sense!\n\n[!defn] Definition: derivative of a scalar function Let \\(f\\colon D\\rightarrow \\mathbb{R}\\) be a scalar function and let \\(x_{0}\\in D\\) be a [[1.2 Limits of scalar functions#^772289|left accumulation point]]. Consider the function \\(\\Delta f\\colon (x_{0},x_{0}+c)\\subset D\\rightarrow \\mathbb{R}\\) given by \\[\n\\Delta f(h):=\\frac{f(x_{0}+h) - f(x_{0})}{h} .\n\\] If the right limit \\[\n\\lim_{h\\rightarrow 0^{+}}\\;\\Delta f = l\n\\] exists with \\(l\\in\\mathbb{R}\\), then \\(l\\) is called the right derivative of \\(f\\) at \\(x_{0}\\) and \\(f\\) is said to be right differentiable at \\(x_{0}\\). Starting from a right accumulation point, we obtain the notion of left derivative and left differentiability at \\(x_{0}\\). If both the left and right derivatives at \\(x_{0}\\) exist and are equal, then we say that \\(f\\) is differentiable at \\(x_{0}\\) and write \\[\nf'(x_{0})\\equiv \\lim_{h\\rightarrow 0}\\;\\Delta f .\n\\] We also write \\[\n\\frac{\\mathrm{d}f}{\\mathrm{d} x}(x_{0})\\equiv f'(x_{0}),\n\\] and refer to this expression as the Leibniz notation for derivatives.\nThe function associated with the rule \\(x\\mapsto f'(x)\\) is called the derivative of \\(f\\). Its domain need not coincide with the domain of \\(f\\).\n\n^971f59\nBoth [[1.2 Limits of scalar functions#^979f48|continuity]] and differentiability are defined through limits, and it turns out there is a hierarchy between these two properties.\n\n[!important] Proposition: differentiability implies continuity If the function\\(f\\colon D\\rightarrow\\mathbb{R}\\) is (left/right) differentiable at \\(x_{0}\\in D\\), then it is also (left/right) continuous at \\(x_{0}\\).\n\nThe converse of the previous proposition does not hold, and the typical example of function which is continuous but not differentiable is the absolute value function given by \\[\n|x|=\\sqrt{x^{2}}=\\left\\{\\begin{matrix}x & \\mbox{ if } x\\geq 0 \\\\ & \\\\ -x & \\mbox{ if } x&lt;0 .\\end{matrix}\\right.\n\\] Indeed, we have \\[\n\\begin{split}\n\\lim_{x\\rightarrow 0^{+}}\\Delta |x|&=\\lim_{x\\rightarrow 0^{+}}\\frac{x-0}{x}=1 \\\\ & \\\\\n\\lim_{x\\rightarrow 0^{-}}\\Delta |x|&=\\lim_{x\\rightarrow 0^{-}}\\frac{-x-0}{x}=-1 ,\n\\end{split}\n\\] which means that the left and right derivatives at \\(0\\) exist but are different.\n\n[!exmp] Examples The first example of derivative we consider is that of a constant function \\(f(x)=c\\) with \\(c\\in \\mathbb{R}\\). Following the very definition of derivative given above, we have \\[\nf'(x_{0})=\\lim_{h\\rightarrow 0} \\frac{f(x_{0}+h)-f(x_{0})}{h}=0\n\\] for every \\(x_{0}\\in\\mathbb{R}\\).\nThe second example we consider is that of the function \\(f(x)=x\\): \\[\nf'(x_{0})=\\lim_{h\\rightarrow 0} \\frac{f(x_{0}+h)-f(x_{0})}{h}=\\lim_{h\\rightarrow 0} \\frac{x_{0} + h -x_{0}}{h}=1\n\\] for every \\(x_{0}\\in\\mathbb{R}\\).\nThe third example we consider is that of the function \\(f(x)=x^{2}\\): \\[\n\\begin{split}\nf'(x_{0})&=\\lim_{h\\rightarrow 0} \\frac{f(x_{0}+h)-f(x_{0})}{h}= \\\\ &=\\lim_{h\\rightarrow 0} \\frac{(x_{0} + h)^{2} -x_{0}^{2}}{h}= \\\\ &\\lim_{h\\rightarrow 0} \\frac{2x_{0}h + h^{2}}{h}=2x_{0}\n\\end{split}\n\\] for every \\(x_{0}\\in\\mathbb{R}\\).\n\nFrom an intuitive geometrical point of view, the derivative \\(f'(x_{0})\\) of \\(f\\) at \\(x_{0}\\) may be interpreted as the value \\(\\tan(\\theta_{0})\\) of a certain angle \\(\\theta_{0}\\) (refer to the figure adapted from here):\n![[geometric_derivative.png|500]]\nConsidering the square triangle with vertices \\(A\\equiv(x_{0},f(x_{0}))\\), \\(B\\equiv(x_{0}+h,f(x_{0}))\\), and \\(C\\equiv(x_{0}+h,f(x_{0}+h))\\), and denoting with \\(\\theta_{h}\\) the angle between the vectors \\(\\vec{AB}\\) and \\(\\vec{AC}\\), some trigonometric considerations lead us to the equality \\[\n\\frac{f(x_{0}+h) - f(x_{0})}{h}=\\tan(\\theta_{h}).\n\\] Then, as \\(h\\rightarrow 0\\), the segment \\(\\vec{AC}\\) becomes closer and closer to the line \\(L\\) tangent to the graph of \\(f\\) passing through the point \\((x_{0},f(x_{0}))\\), and the resulting limit is the derivative \\(f'(x_{0})=\\tan(\\theta_{0})\\), with \\(\\theta_{0}\\) the angle between the horizontal axis and the tangent line \\(L\\).\n%% We can visualize in which sense the derivative is related to the tangent line as follows (source):\n![[Graph_of_sliding_derivative_line_no_text.gif]] %% Intuitively speaking, the line tangent to the graph of a function at a point is the limit of the secant lines connecting said point with other points on the graph. Here is an interactive visualization of this fact:\n\n\n\n[!defn] Definition: \\(C^{1}\\)-functions The function \\(f\\colon D\\rightarrow\\mathbb{R}\\) is said to be differentiable in \\(D\\) if it is (left/right) differentiable at all [[1.2 Limits of scalar functions#^772289|(left/right) accumulation points]] in \\(D\\). The function \\(f'\\colon D\\rightarrow \\mathbb{R}\\) is called the derivative function. The function \\(f\\) is said to be \\(C^{1}\\) in \\(D\\) if it is differentiable in \\(D\\) and its derivative function is [[1.2 Limits of scalar functions#^979f48|continuous]] in \\(D\\).\n\n^c07f2b\nThe intuitive idea of derivative as rate of change may be easy to grasp because it relies on the idea of velocity which the vast majority of humans can understand (even if only in a non-rigorous way). However, this interpretation is not very helpful when the input of the functions considered is no longer a single number (which can always be thought of as being the “time parameter” labeling something that changes).\nA perspective which is suitable to multivariable functions is that of thinking of the derivative \\(f'(x_{0})\\) as providing the best linear approximation of the function near \\(x_{0}\\). Specifically, we can define \\(f'(x_{0})\\) as the number (if it exists) such that \\[ \\lim_{x\\rightarrow x_{0}}\\frac{\\mid f(x) - [f(x_{0}) +f'(x_{0})\\,(x-x_{0})]\\mid}{\\mid x-x_{0}\\mid}=0. \\] Roughly speaking, the previous limit is telling us that the difference between \\(f(x)\\) and \\([f(x_{0}) +f'(x_{0})\\,(x-x_{0})]\\) gets smaller as \\(x\\) approaches \\(x_{0}\\), and it gets smaller “faster” than how \\(x\\) approaches \\(x_{0}\\) (because the limit with \\(x-x_{0}\\) in the denominator is finite).\nConsequently, when the derivative at \\(x_{0}\\) exists, we may approximate reasonably well \\(f(x_{0})\\) with the linear function \\([f(x_{0}) + f'(x_{0})\\,(x-x_{0})]\\) as long as \\(x\\) is close to \\(x_{0}\\).\nOnce we understand how the derivative is connected with linear approximation, we may go back to the interpretation of the derivative as velocity, and we can try to understand velocity as being connected with the best linear approximation of the position for small times.\n\n\nProperties of derivatives\n^f3bc1e\nJust as it happens for limits, the actual computation of derivatives may often be carried without directly computing the limit in its definition by suitably exploiting powerful results.\nThe first result is related with derivatives of sums and products.\n\n[!important] Proposition: Algebraic manipulations of derivatives Let \\((\\alpha_{1},\\cdots\\alpha_{n})\\) be a finite sequence of real numbers and \\((f_{1},\\cdots,f_{n})\\) a finite sequence of scalar functions. Suppose that \\(f_{j}'(x)\\) exists for every \\(j=1,...,n\\). Then, it holds \\[\n\\begin{split}\n\\left(\\sum_{j=1}^{n}\\,\\alpha_{j}\\,f_{j}\\right)'(x) &=\\sum_{j=1}^{n}\\, \\alpha_{j}\\,f_{j}'(x)   \\\\ & \\\\\n\\left(\\prod_{j=1}^{n} \\,\\alpha_{j}\\,f_{j}\\right)'(x) &=\\sum_{j=1}^{n}\\,\\alpha_{j}\\,f_{j}'(x)\\,\\left(\\prod_{k\\neq j}\\alpha_{k}\\,f_{k}(x)\\right) .\n\\end{split}\n\\] In particular, if \\(n=2\\) and \\(\\alpha_{1}=\\alpha_{2}=1\\), the derivative of the product takes on the very well-known form (Leibniz rule) \\[\n\\left(f_{1}f_{2}\\right)'(x)=f_{1}'(x)\\,f_{2}(x) + f_{1}(x)\\,f_{2}'(x).\n\\] Moreover, if \\(n=2\\), \\(f_{2}'(x)\\neq0\\) and \\(\\alpha_{1}=\\alpha_{2}=1\\), the derivative of the quotient reads \\[\n\\left(\\frac{ f_{1}}{f_{2}}\\right)'(x)=\\frac{f_{1}'(x)f_{2}(x) - f_{1}(x)\\,f_{2}'(x)}{(f_{2}(x))^{2}}.\n\\]\n\n^5d20ad\nNext, we consider the composition of functions and obtain the important chain rule for derivatives.\n\n[!important] Proposition: Chain rule for derivatives Let \\(f,g\\) be scalar functions such that \\(g\\) is differentiable at the point \\(x_{0}\\) and \\(f\\) is differentiable at the point \\(y_{0}=g'(x_{0})\\). The composite function \\(f\\circ g\\) is differentiable at \\(x_{0}\\) and its derivative reads \\[\n(f\\circ g)'(x_{0})=(f'\\circ g)(x_{0})\\,g'(x_{0}).\n\\] If we introduce the new variable \\(y=g(x)\\) we can exploit the Leibniz notation for derivatives to write the chain rule above as \\[\n\\frac{\\mathrm{d}f}{\\mathrm{d}x}=\\frac{\\mathrm{d}f}{\\mathrm{d}y}\\frac{\\mathrm{d}y}{\\mathrm{d}x}.\n\\] This expression is particularly useful if we want to extend the chain rule to the composition of more than two functions.\n\n^c87d27\nFinally, if a function admits an inverse, then their derivatives are connected to one another.\n\n[!important] Proposition: Differentiability of the inverse function Let \\(g\\) be a scalar function which is invertible and differentiable, and let \\(f\\) denote its inverse. If \\(g(a)=b\\) and \\(g'(a)\\neq 0\\) then \\(f\\) is differentiable at \\(b=g(a)\\) and it holds \\[\nf'(b)= \\frac{1}{g'(a)} .\n\\]\n\n^05d051\nA good discussion of all the subtleties involved in the previous proposition can be found in the section “Inverse function” of Michael Spivak’s “Calculus” book.\n\n\nL’Hôpital’s rule and indeterminate forms\nDerivatives can help us in computing the limit of some quotients of functions in the indeterminate cases \\(\\frac{0}{0}\\) or \\(\\frac{\\pm\\infty}{\\pm\\infty}\\).\n\n[!important] Theorem: L’Hôpital’s rule Let \\(x_{0}\\in\\overline{\\mathbb{R}}:=\\mathbb{R}\\cup\\{+\\infty,-\\infty\\}\\) and let \\(I\\) be an open interval containing \\(x_{0}\\) if \\(x_{0}\\neq \\pm\\infty\\), or having \\(x_{0}\\) as an endpoint if \\(x_{0}=\\pm\\infty\\). Let \\(f,g\\) be scalar functions differentiable in \\(I\\) except possibly at \\(x_{0}\\). If \\[\n\\lim_{x\\rightarrow x_{0}}f(x)=\\lim_{x\\rightarrow x_{0}}g(x)=0\\,\\mbox{ or }\\; \\pm \\infty,\n\\] if \\(g'(x)\\neq 0\\) for every \\(x\\in I\\) such that \\(x\\neq x_{0}\\), and if \\[\n\\lim_{x\\rightarrow x_{0}}\\frac{f'(x)}{g'(x)} = l \\in\\mathbb{R},\n\\] then \\[\n\\lim_{x\\rightarrow x_{0}}\\frac{f(x)}{g(x)} =\\lim_{x\\rightarrow x_{0}}\\frac{f'(x)}{g'(x)} .\n\\] Whenever even one of the previous assumptions fail, the result is not true!\n\nNote that L’Hôpital’s rule may be applied more than one time if necessary. For instance to compute\n\\[1)\\quad \\lim_{x\\rightarrow 0} \\frac{ax^{4}+bx^{3}+cx^{2}}{b'x^{3}+c'x^{2}} = \\frac{c}{c'} ,\\]\nwith \\((c,c')\\neq (0,0)\\).\nIndeed, if we apply L’Hôpital’s rule once, we get\n\\[2)\\quad \\lim_{x\\rightarrow 0} \\frac{4ax^{3}+3bx^{2}+2cx}{3b'x^{2}+2c'x} \\stackrel{?}{=} \\frac{0}{0}, \\]\nwhich means we can not solve 1) with 2). However, we can look at 2) on its own, an apply L’Hôpital’s rule to it, obtaining\n\\[ 3)\\quad \\lim_{x\\rightarrow 0} \\frac{12ax^{2}+6bx +2c }{6b'x +2c'} = \\frac{c}{c'}. \\]\nSince we assumed \\((c,c')\\neq (0,0)\\), the right hand side of 3) makes sense, and thus L’Hôpital’s rule can be applied to conclude that the result of 2) is \\(\\frac{c}{c'}\\). However, once we are here, we know that the limit in 2) makes sense, and we can L’Hôpital’s rule applies again to conclude that the result of 1) is \\(\\frac{c}{c'}\\).\nL’Hôpital’s rule is very powerful and useful, but it is easy to abuse it or apply it incorrectly. In particular, it is relevant to mention the case \\[\n\\lim_{x\\rightarrow 0} \\frac{\\sin(x)}{x}=1\n\\]\n^77ad27\neven if we did not yet introduced the trigonometric function \\(\\sin(x)\\). Suppose trigonometric functions are introduced just making reference to the geometry of angles as it is often done. If we want to apply L’Hôpital’s rule to check the previous limit, we need to compute the derivative of \\(\\sin(x)\\) at \\(x=0\\). Assuming we know that \\(\\sin(0)=0\\), by the very definition of derivative, we get \\[\n\\sin'(0)=\\lim_{h\\rightarrow 0} \\frac{\\sin(0+h) -\\sin(0)}{h}=\\lim_{h\\rightarrow 0}\\frac{\\sin(h)}{h}.\n\\] It is thus clear that we reach a circular situation because evaluating \\(\\sin'(x)\\) is equivalent to using L’Hôpital’s rule to solve the indeterminate limit!\nThis instance should make clear that L’Hôpital’s rule should never be applied without really understanding what is going on at a deeper level.\n\n[!rem] Remark We [[1.5 Power series, trascendental functions, and Taylor series#^ed7011|will see]] that L’Hôpital’s rule can be applied to solve the previous limit if we introduce trigonometric functions using power series.\n# Second and higher order derivatives\n\nBy definition, if the scalar function \\(f\\) is \\(C^{1}\\) in \\(D\\), its derivative \\(f'\\) is a continuous function in \\(D\\). Therefore, it makes sense to consider \\(f'\\) as a scalar function on its own, say \\(g\\equiv f'\\), and investigate its differentiability properties. If everything works fine, we obtain the second derivative \\(f''\\) of \\(f\\). In the \\(\\frac{\\mathrm{d}}{\\mathrm{d}x}\\)-notation for derivatives introduced above, the second derivative is written as \\[\n\\frac{\\mathrm{d}^{2}f}{\\mathrm{d}x^{2}} =  \\frac{\\mathrm{d}}{\\mathrm{d}x}\\left(\\frac{\\mathrm{d}f}{\\mathrm{d}x}\\right)=f''.\n\\] &gt;[!exmp] Examples &gt;It should not be surprising that the second derivative of the constant function \\(f(x)=c\\) identically vanishes because \\(f'(x)=0\\). &gt; &gt;Quite similarly, the second derivative of \\(f(x)=x\\) also vanishes. Indeed, since \\(f'(x)=1\\), setting \\(g(x)=f'(x)=1\\), we obtain that \\(f''(x)=g'(x)=0\\) because \\(g'(x)\\) is a constant function. &gt; &gt;On the other hand, if we consider the function \\(f(x)=x^{2}\\), we know from the examples above that \\(f'(x)=2x\\), so that, setting \\(g(x)=f'(x)\\), it immediately follows that \\(f''(x)=g'(x)=2\\).\nAs long as things make sense, we can take derivatives as many times as we want, thus reaching the third derivative \\(f'''=f^{(3)}\\), or the fourth derivative \\(f^{(4)}\\), or even the 26453271935261-th derivative \\(f^{(26453271935261)}\\).\n\n[!defn] Definition: \\(C^{k}\\) and smooth functions Given \\(k\\in\\mathbb{N}-\\{0\\}\\), a function \\(f\\colon D \\rightarrow \\mathbb{R}\\) is said to be \\(C^{k}\\) on \\(D\\) if its \\(k\\)-th order derivative exists and is continuous on \\(D\\). A continuous function is also said to be \\(C^{0}\\). If \\(f\\) is \\(C^{k}\\) for all \\(k\\in\\mathbb{K}\\), then it is called \\(C^{\\infty}\\) or smooth.\n\n^a6b2f2",
    "crumbs": [
      "Home",
      "Teaching",
      "Calculus - Robotics Engineering - UC3M",
      "Derivatives of scalar functions"
    ]
  },
  {
    "objectID": "teaching/calculus-uc3m-robotics/1.1 Sets and functions.html",
    "href": "teaching/calculus-uc3m-robotics/1.1 Sets and functions.html",
    "title": "1.1 Sets and functions",
    "section": "",
    "text": "Sets and numbers\nWe assume a basic (intuitive) understanding of concepts like sets and numbers (natural \\(\\mathbb{N}\\), integers \\(\\mathbb{Z}\\), rational \\(\\mathbb{Q}\\), and real \\(\\mathbb{R}\\)).\n\n[!rem] Remark: sets and numbers The notion of set is not as innocuous as it may seem. For instance, if not handled with care, it directly leads to Russel’s paradox. To avoid these unpleasant instances, deep thinking and sophisticated tools are needed (see here and here). Similarly, the notion of number is both subtle and deep, and we will content ourselves with the intuitive notion that most humans attending University have already acquired through experience and instruction (but feel free to ask all the questions you want in class). Take a look at this Wikipedia page for an introductory overview on the beauty of numbers.\n\nIn the rest of these notes, we are willing to think that the following is a reasonably good definition of a set.\n\n[!defn] Definition: Set A set \\(A\\) is a collection of distinct objects. The set \\(B\\) is a subset of the set \\(A\\) if every element of \\(B\\) belongs to \\(A\\). In addition: - if \\(x\\) is an element in \\(S\\) we write \\(x\\in S\\), otherwise, we write \\(x\\notin S\\); - if \\(B\\) is a subset of \\(A\\) and \\(A\\neq B\\), we write \\(B\\subset A\\) or \\(A \\supset B\\), and we write \\(B\\subseteq A\\) or \\(A\\supseteq B\\) if we do not know whether \\(A\\neq B\\) or not.\n\nSometimes, you are able to list all elements in a set, as it happens with the set \\(S\\) of the best guitar players of all time: \\[\nS= \\{\\mathrm{Jimi\\; Hendrix,\\; Frank \\;Zappa}\\}.\n\\]\nSome other times it is better to describe the elements in a set through a common property (the symbol \\(|\\) means “such that”):\n\\[\nS= \\{ x|\\,x\\mbox{ is a student in UC3M born on February 29th} \\}.\n\\]\n\n[!rem] Remark: the empty set We assume the existence of the EMPTY SET \\(\\emptyset\\). The empty set \\(\\emptyset\\) is assumed to be a subset of every set, and every set \\(A\\) is a subset of itself.\n\nIt is possible to operate on sets to obtain other sets:\n\nUnion\n\n\\[\nA\\cup B:=\\{x|\\,x\\in A\\mbox{ or } x\\in B\\}\\,\\Rightarrow A,B\\subseteq A\\cup B \\mbox{ and } A\\cup\\emptyset = A;\n\\] - Intersection\n\\[\nA\\cap B:=\\{x|\\,x\\in A\\mbox{ and } x\\in B\\}\\,\\Rightarrow A\\cap B\\subseteq A,B \\mbox{ and } A\\cup\\emptyset = A;\n\\] - Cartesian product\n\\[\nA\\times B:=\\left\\{(x,y)|\\,x\\in A\\mbox{ and } y \\in B\\right\\}.\n\\]\n\n\nFunctions\nLet us start with a definition.\n\n[!defn] Definition: Functions Given two sets \\(X\\) and \\(Y\\), a function can be thought of as a rule that assigns one and only one element \\(y\\equiv f(x)\\in Y\\) to every \\(X\\in D\\subseteq A\\) . A function is specified by a quadruple \\((D,X,f,Y)\\) where: - \\(X\\) is the input space of the function; - \\(\\mathrm{Dom}(f)\\equiv D\\subseteq X\\) is the domain of the function (the set of all \\(x\\in X\\) on which the function can act); - \\(Y\\) is the output space of the function; - \\(f\\) is the rule by means of which we associated \\(y=f(x)\\) to \\(x\\).\nFor the sake of notational simplicity, instead of \\((D,X,f,Y)\\), it is customary to write \\(f\\colon X\\rightarrow Y\\), or \\(f\\colon D\\rightarrow Y\\), or simply \\(f\\) to denote a function. However, we should always have a clear understanding of all the ingredients needed to define a function. - The function \\(f\\) is called injective (or one-to-one) if it satisfies \\(f(x)\\neq f(y)\\) for every \\(x,y\\in X\\) such that \\(x\\neq y\\) ; - The function \\(f\\) is called surjective (or onto) if its range is the whole \\(Y\\) ; - The function \\(f\\) is called bijective if it is both injective and surjective.\n\nAssociated with a function \\((D,X,f,Y)\\) there are three additional sets to be discovered:\n\nthe maximal domain which is the biggest subset \\(D_{max}\\subseteq X\\) for which \\(f(x)\\) makes sense;\nthe range \\(\\mathrm{ran}(f)\\equiv f(D):=\\{y\\in Y\\,|\\; \\exists\\,x\\in D \\mbox{ with } f(x)=y\\}\\) of the function;\nthe graph \\(\\mathrm{graph}(f):=\\{(x,f(x))\\,|\\;\\;x\\in X\\}\\) of the function (which is a subset of \\(A\\times B\\)).\n\nAdditionally, important types of functions are:\n\ninjective (or one-to-one): if \\(f(x)\\neq f(y)\\) for every \\(x,y\\in D\\) such that \\(x\\neq y\\) ;\nsurjective (or onto): if \\(\\mathrm{ran}(f)=B\\) ;\nbijective: if the function is both injective and surjective.\n\n\n[!exmp] Example\nGiven the rule \\(x\\mapsto f(x)=x^{2}\\), we note that \\(([0,1],\\mathbb{R},f,\\mathbb{R})\\) and \\((\\mathbb{R},\\mathbb{R},f,\\mathbb{R})\\) define different functions because their respective domains are different.\n\nAn incredibly powerful characteristic of functions is that they can be composed (like arrows): \\[ A\\stackrel{f}{\\longrightarrow} B \\stackrel{g}{\\longrightarrow} C , \\] \\[ x \\quad \\stackrel{f}{\\mapsto}\\quad  y=f(x)\\quad \\stackrel{g}{\\mapsto} \\quad z=g(y)=g(f(x))\\equiv g\\circ f (x) \\]\nThe result is again a function, which is denoted by \\(g\\circ f\\), and it is called the composition of \\(f\\) with \\(g\\).\n\\[ x \\quad \\stackrel{f}{\\mapsto}\\quad  y=f(x)\\quad \\stackrel{g}{\\mapsto} \\quad z=g(y)=g(f(x))\\equiv g\\circ f (x) \\]\nThe function \\(g\\circ f\\colon A\\rightarrow D\\) is called the composition of \\(f\\) with \\(g\\). Of course, everything makes sense if and only if the domain \\(C\\) of \\(g\\) contains the range \\(f(A)\\) of \\(f\\). In particular, this means that it must hold \\(B=C\\).\nOnce we know how to compose two functions, we can start composing an arbitrary number of functions, and it is instructive to check that\n\\[ (f\\circ g)\\circ h = f\\circ (g\\circ h) \\]\nwhich means that the composition of functions is associative whenever it makes sense.\nThe composition operation \\(\\circ\\) allows to rigorously define the operation of restriction of a function to a subset. Specifically, if \\(C\\) is a subset of \\(A\\), there is a function \\(i_{CA}\\colon C\\rightarrow A\\) called the canonical immersion of \\(C\\) in \\(A\\), whose action on every \\(x\\in C\\subseteq A\\) is given by \\(i_{CA}(x)=x\\).\nTherefore, given the function \\(f\\colon A\\rightarrow B\\), we can immediately define the restriction of \\(f\\) to \\(C\\subseteq A\\), often denoted by \\(f|_{C}\\), by simply composing \\(f\\) with the canonical immersion \\(i_{CA}\\):\n\\[ f|_{C}:=f\\circ i_{CA}. \\]\nGiven the function \\(f\\colon A\\rightarrow B\\), if there exists a function \\(g\\colon B\\rightarrow A\\) such that\n\\[ g\\circ f(x) = x \\]\nfor every \\(x\\in A\\), then \\(f\\) is called left-invertible and \\(g\\) is called the left-inverse of \\(f\\). We can rewrite the previous equation without mentioning \\(x\\) as follows:\n\\[ g\\circ f = \\mathrm{id}_{A}, \\]\nwhere we introduced the identity function \\(\\mathrm{id}_{A}\\) on the set \\(A\\) whose action is trivially \\(\\mathrm{id}_{A}(x)=x\\) for every \\(x\\in A\\).\nSimilarly, \\(f\\) is called right-invertible if there is a function \\(g\\colon B\\rightarrow A\\) such that\n\\[ f\\circ g = \\mathrm{id}_{B}, \\] in which case, \\(g\\) is called the right-inverse of \\(f\\).\nThe function \\(f\\) is called invertible if it is both left and right-invertible. ^ac035a",
    "crumbs": [
      "Home",
      "Teaching",
      "Calculus - Robotics Engineering - UC3M",
      "1.1 Sets and functions"
    ]
  },
  {
    "objectID": "teaching/calculus-uc3m-robotics/2.5 Exercises.html",
    "href": "teaching/calculus-uc3m-robotics/2.5 Exercises.html",
    "title": "Limits of multivariable functions",
    "section": "",
    "text": "Recalling how the [[2.1 From 1 to many dimensions#^1f484e|cylindrical]] and [[2.1 From 1 to many dimensions#^b5d3f4|spherical]] coordinates are expressed in terms of the Cartesian coordinates, find the function determining the passage from cylindrical to spherical coordinates and its inverse.\n&gt;[!note]- Solution: &gt; Compose \\(\\psi_{cyl}\\) of [[2.1 From 1 to many dimensions#^1f484e|cylindrical coordinates]] with \\(\\psi^{-1}_{sph}\\) of [[2.1 From 1 to many dimensions#^b5d3f4|spherical coordinates]] to get how cylindrical coordinates are expressed in terms of spherical coordinates. Exchange the role of \\(\\psi\\) and \\(\\psi^{-1}\\) to obtain how spherical coordinates are expressed using cylindrical coordinates.\nDetermine the expression in cylindrical and spherical coordinates of the function \\((D,\\mathbb{R}^{3},f,\\mathbb{R})\\) expressed in Cartesian coordinates by: \\[  \\begin{split}   1) &\\qquad  f(x,y,z)= x^{2} + y^{2} +z^{2}  \\\\ & \\\\     2) &\\qquad  f(x,y,z)=\\left(x,\\;y-z,\\; 3 \\right) \\\\ & \\\\     3) & \\qquad f(x,y,z)= \\mathrm{e}^{x^{2} + y^{2} +z} . \\end{split} \\] &gt;[!note]- Solution: &gt;Compose \\(f\\) with \\(\\psi^{-1}\\) of [[2.1 From 1 to many dimensions#^1f484e|cylindrical coordinates]] or [[2.1 From 1 to many dimensions#^b5d3f4|spherical coordinates]].\nIf the expression of \\(f\\colon\\mathbb{R}^{2}\\rightarrow \\mathbb{R}\\) in polar coordinates is \\[  f(r,\\theta) = r + \\theta ,  \\] determine the value of \\(f\\) a the point \\(\\mathbf{p}\\) with Cartesian coordinates \\((p_{x}=0, p_{y}=1)\\), \\((p_{x}=1,p_{y}=1)\\), and \\((p_{x}=2,p_{y}=3)\\). &gt;[!note]- Solution &gt;In Cartesian coordinates, the point \\((0,1)\\) corresponds to the point \\((1,\\pi/2)\\) in polar coordinates, while the point \\((2,3)\\) corresponds to the point \\((\\sqrt{13},\\arctan(3/2))\\).\nLet \\((\\mathbb{R},\\mathbb{R},f, \\mathbb{R})\\) be expressed as \\[ f(x)=\\frac{\\mathrm{e}^{2x} - 2 + \\mathrm{e}^{-2x}}{4}\n\\] in Cartesian coordinates. Determine a change of coordinate \\((A,\\mathbb{R},\\psi,\\mathbb{R})\\) leading to \\[ f\\circ\\psi(v) = v^{2} \\] and discuss its domain of applicability. &gt;[!note]- Solution &gt;Note that \\(\\mathrm{e}^{2x} - 2 + \\mathrm{e}^{-2x}=(\\mathrm{e}^{x}-\\mathrm{e}^{-x})^{2}\\). The change of coordinates can be understood as \\(x(v)=\\psi(v)\\), and we must have &gt;\\[ \\frac{(\\mathrm{e}^{x}-\\mathrm{e}^{-x})^{2}}{4}=f\\circ\\psi(v)=v^{2},\\] &gt;which means we can take \\(\\psi(v)=\\frac{\\mathrm{e}^{x}-\\mathrm{e}^{-x}}{2}\\). This change of coordinates is valid for all \\(v\\) and \\(x\\) because both the domain and range of \\(\\psi\\) are \\(\\mathbb{R}\\). Note that \\(\\tilde{\\psi}(v)=-\\psi(v)\\) is another, perfectly valid choice.\nLet \\(f\\colon\\mathbb{R}^{2}\\rightarrow \\mathbb{R}\\) be expressed as \\[ f(x,y)= ax + by \\] in Cartesian coordinates with \\(a,b\\neq 0\\). Determine a change of coordinate \\(\\psi\\colon A\\subseteq \\mathbb{R}^{2}\\rightarrow \\mathbb{R}^{2}\\) leading to \\[ f\\circ\\psi(v,w)= v^2 - w^{3},\\] and discuss its domain of applicability. Is \\(\\psi\\) unique? &gt;[!note]- Solution &gt;There are infinitely many change of coordinates satisfying the constraint imposed. Indeed, we can take \\(\\psi(v,w)=(x(v,w),y(v,w))\\) with \\(x(v,w)=\\alpha v^{2} +\\beta w^{3}\\), \\(y(v,w)=\\gamma v^{2} +\\delta w^{3}\\) and \\(\\alpha,\\beta,\\gamma,\\delta\\) satisfying \\(a(\\alpha + \\gamma )=1\\) and \\(b(\\beta + \\delta)=-1\\). In order for \\(\\psi\\) to be a valid change of coordinates, we must impose that \\(\\alpha,\\beta,\\gamma,\\delta\\) are such that \\(\\psi\\) can be inverted somewhere, and this determines the domain of applicability. For instance, taking \\(\\alpha=1/a\\), \\(\\delta=-1/b\\), and \\(\\beta=\\gamma=0\\), we have that \\(\\psi^{-1}\\) only exists when \\(x\\geq 0\\). On the other hand, if we take \\(\\alpha=\\beta=0\\), \\(\\gamma=1/a\\), and \\(\\delta=-1/b\\), then \\(\\psi^{-1}\\) does not exist because \\((v,w)=\\psi^{-1}\\circ\\psi(v,w)=\\psi^{-1}(0,v^{2}+w^{3})\\) is impossible.",
    "crumbs": [
      "Home",
      "Teaching",
      "Calculus - Robotics Engineering - UC3M",
      "Change of coordinates"
    ]
  },
  {
    "objectID": "teaching/calculus-uc3m-robotics/2.5 Exercises.html#change-of-coordinates",
    "href": "teaching/calculus-uc3m-robotics/2.5 Exercises.html#change-of-coordinates",
    "title": "Limits of multivariable functions",
    "section": "",
    "text": "Recalling how the [[2.1 From 1 to many dimensions#^1f484e|cylindrical]] and [[2.1 From 1 to many dimensions#^b5d3f4|spherical]] coordinates are expressed in terms of the Cartesian coordinates, find the function determining the passage from cylindrical to spherical coordinates and its inverse.\n&gt;[!note]- Solution: &gt; Compose \\(\\psi_{cyl}\\) of [[2.1 From 1 to many dimensions#^1f484e|cylindrical coordinates]] with \\(\\psi^{-1}_{sph}\\) of [[2.1 From 1 to many dimensions#^b5d3f4|spherical coordinates]] to get how cylindrical coordinates are expressed in terms of spherical coordinates. Exchange the role of \\(\\psi\\) and \\(\\psi^{-1}\\) to obtain how spherical coordinates are expressed using cylindrical coordinates.\nDetermine the expression in cylindrical and spherical coordinates of the function \\((D,\\mathbb{R}^{3},f,\\mathbb{R})\\) expressed in Cartesian coordinates by: \\[  \\begin{split}   1) &\\qquad  f(x,y,z)= x^{2} + y^{2} +z^{2}  \\\\ & \\\\     2) &\\qquad  f(x,y,z)=\\left(x,\\;y-z,\\; 3 \\right) \\\\ & \\\\     3) & \\qquad f(x,y,z)= \\mathrm{e}^{x^{2} + y^{2} +z} . \\end{split} \\] &gt;[!note]- Solution: &gt;Compose \\(f\\) with \\(\\psi^{-1}\\) of [[2.1 From 1 to many dimensions#^1f484e|cylindrical coordinates]] or [[2.1 From 1 to many dimensions#^b5d3f4|spherical coordinates]].\nIf the expression of \\(f\\colon\\mathbb{R}^{2}\\rightarrow \\mathbb{R}\\) in polar coordinates is \\[  f(r,\\theta) = r + \\theta ,  \\] determine the value of \\(f\\) a the point \\(\\mathbf{p}\\) with Cartesian coordinates \\((p_{x}=0, p_{y}=1)\\), \\((p_{x}=1,p_{y}=1)\\), and \\((p_{x}=2,p_{y}=3)\\). &gt;[!note]- Solution &gt;In Cartesian coordinates, the point \\((0,1)\\) corresponds to the point \\((1,\\pi/2)\\) in polar coordinates, while the point \\((2,3)\\) corresponds to the point \\((\\sqrt{13},\\arctan(3/2))\\).\nLet \\((\\mathbb{R},\\mathbb{R},f, \\mathbb{R})\\) be expressed as \\[ f(x)=\\frac{\\mathrm{e}^{2x} - 2 + \\mathrm{e}^{-2x}}{4}\n\\] in Cartesian coordinates. Determine a change of coordinate \\((A,\\mathbb{R},\\psi,\\mathbb{R})\\) leading to \\[ f\\circ\\psi(v) = v^{2} \\] and discuss its domain of applicability. &gt;[!note]- Solution &gt;Note that \\(\\mathrm{e}^{2x} - 2 + \\mathrm{e}^{-2x}=(\\mathrm{e}^{x}-\\mathrm{e}^{-x})^{2}\\). The change of coordinates can be understood as \\(x(v)=\\psi(v)\\), and we must have &gt;\\[ \\frac{(\\mathrm{e}^{x}-\\mathrm{e}^{-x})^{2}}{4}=f\\circ\\psi(v)=v^{2},\\] &gt;which means we can take \\(\\psi(v)=\\frac{\\mathrm{e}^{x}-\\mathrm{e}^{-x}}{2}\\). This change of coordinates is valid for all \\(v\\) and \\(x\\) because both the domain and range of \\(\\psi\\) are \\(\\mathbb{R}\\). Note that \\(\\tilde{\\psi}(v)=-\\psi(v)\\) is another, perfectly valid choice.\nLet \\(f\\colon\\mathbb{R}^{2}\\rightarrow \\mathbb{R}\\) be expressed as \\[ f(x,y)= ax + by \\] in Cartesian coordinates with \\(a,b\\neq 0\\). Determine a change of coordinate \\(\\psi\\colon A\\subseteq \\mathbb{R}^{2}\\rightarrow \\mathbb{R}^{2}\\) leading to \\[ f\\circ\\psi(v,w)= v^2 - w^{3},\\] and discuss its domain of applicability. Is \\(\\psi\\) unique? &gt;[!note]- Solution &gt;There are infinitely many change of coordinates satisfying the constraint imposed. Indeed, we can take \\(\\psi(v,w)=(x(v,w),y(v,w))\\) with \\(x(v,w)=\\alpha v^{2} +\\beta w^{3}\\), \\(y(v,w)=\\gamma v^{2} +\\delta w^{3}\\) and \\(\\alpha,\\beta,\\gamma,\\delta\\) satisfying \\(a(\\alpha + \\gamma )=1\\) and \\(b(\\beta + \\delta)=-1\\). In order for \\(\\psi\\) to be a valid change of coordinates, we must impose that \\(\\alpha,\\beta,\\gamma,\\delta\\) are such that \\(\\psi\\) can be inverted somewhere, and this determines the domain of applicability. For instance, taking \\(\\alpha=1/a\\), \\(\\delta=-1/b\\), and \\(\\beta=\\gamma=0\\), we have that \\(\\psi^{-1}\\) only exists when \\(x\\geq 0\\). On the other hand, if we take \\(\\alpha=\\beta=0\\), \\(\\gamma=1/a\\), and \\(\\delta=-1/b\\), then \\(\\psi^{-1}\\) does not exist because \\((v,w)=\\psi^{-1}\\circ\\psi(v,w)=\\psi^{-1}(0,v^{2}+w^{3})\\) is impossible.",
    "crumbs": [
      "Home",
      "Teaching",
      "Calculus - Robotics Engineering - UC3M",
      "Change of coordinates"
    ]
  },
  {
    "objectID": "publications/proceedings.html",
    "href": "publications/proceedings.html",
    "title": "Proceedings",
    "section": "",
    "text": "The list is ordered according to the date of publication, and not the date of creation.\n\n\nF.M. Ciaglia, F. Di Cosmo, and G. Marmo: Hamilton-Jacobi Theory and Information Geometry: Geometric Science of Information: 2017-10: doi: 10.1007/978-3-319-68445-1_58. Available at: https://arxiv.org/abs/1711.01129\n\n\nF.M. Ciaglia, A. Ibort, and G. Marmo: Differential Geometry of Quantum States, Observables and Evolution: Quantum Physics and Geometry: 2019: doi: 10.1007/978-3-030-06122-7_7. Available at: http://link.springer.com/10.1007/978-3-030-06122-7_7\n\n\nF.M. Ciaglia, G. Marmo, and L. Schiavone: From Classical Trajectories to Quantum Commutation Relations: Classical and Quantum Physics: 2019: Available at: http://doi.org/10.1007/978-3-030-24748-5_9\n\n\nF.M. Ciaglia, A. Ibort, and G. Marmo: On the Notion of Composite System: Geometric Science of Information: 2019: doi: 10.1007/978-3-030-26980-7_67. Available at: http://link.springer.com/10.1007/978-3-030-26980-7_67\n\n\nF.M. Ciaglia and F. Di Nocera: Group Actions and Monotone Metric Tensors: The Qubit Case: Geometric Science of Information: 2021-07: doi: 10.1007/978-3-030-80209-7_17. Available at: https://arxiv.org/abs/2105.15067\n\n\nF.M. Ciaglia and F. Di Cosmo: Some Remarks on the Notion of Transition: Geometric Science of Information: 2023: doi: 10.1007/978-3-031-38299-4_42\n\n\nF.M. Ciaglia, F. Di Cosmo, and L. González-Bravo: Can Čencov Meet Petz: Geometric Science of Information: 2023-08: doi: 10.1007/978-3-031-38299-4_38. Available at: https://arxiv.org/abs/2305.12482",
    "crumbs": [
      "Home",
      "Publications",
      "Proceedings"
    ]
  },
  {
    "objectID": "publications/inf-geo.html",
    "href": "publications/inf-geo.html",
    "title": "Information geometry",
    "section": "",
    "text": "To be honest, instead of “Information geometry”, I should have written “Geometrical, categorical, and statistical aspects of states on C*-algebras/von Neumann algebras/Jordan algebras”, but it may have been perceived as being a little too much for a title.\nThis is the theoretical playground in which I pass the vast majority of my spare time, and I plan (somewhen) to write down a better description of how entertaining it is.\nThe list is ordered according to the date of publication/arXiv submission, and not the date of creation. trdy\n\n\nF.M. Ciaglia, F. Di Cosmo, D. Felice, S. Mancini, G. Marmo, and J.M. Pérez-Pardo: Hamilton-Jacobi approach to potential functions in information geometry: Journal of mathematical physics 58(6): 2017-06: doi: 10.1063/1.4984941. Available at: https://arxiv.org/abs/1608.06584\n\n\nF.M. Ciaglia, F. Di Cosmo, and G. Marmo: Hamilton-Jacobi Theory and Information Geometry: Geometric Science of Information: 2017-10: doi: 10.1007/978-3-319-68445-1_58. Available at: https://arxiv.org/abs/1711.01129\n\n\nF.M. Ciaglia, A. Ibort, and G. Marmo: Geometrical structures for classical and quantum probability spaces: International journal of quantum information 15(08): 2017-12: doi: 10.1142/s021974991740007x. Available at: https://arxiv.org/abs/1711.09774\n\n\nF.M. Ciaglia, F.D. Cosmo, D. Felice, S. Mancini, G. Marmo, and J.M. Pérez-Pardo: Aspects of Geodesical Motion with Fisher-Rao Metric: Classical and Quantum: Open systems & information dynamics 25(01): 2018-03: doi: 10.1142/s1230161218500051. Available at: https://arxiv.org/abs/1608.06105\n\n\nF.M. Ciaglia, F. Di Cosmo, M. Laudato, G. Marmo, F.M. Mele, F. Ventriglia, and P. Vitale: A pedagogical intrinsic approach to relative entropies as potential functions of quantum metrics: The q – z family: Annals of physics 395: 2018-08: doi: 10.1016/j.aop.2018.05.015. Available at: https://arxiv.org/abs/1807.00519\n\n\nF.M. Ciaglia, G. Marmo, and J.M. Pérez-Pardo: Generalized potential functions in differential geometry and information geometry: International journal of geometric methods in modern physics 16(01): 2019-02: doi: 10.1142/s0219887819400024. Available at: https://arxiv.org/abs/1804.10414\n\n\nF.M. Ciaglia, A. Ibort, J. Jost, and G. Marmo: Manifolds of classical probability distributions and quantum density operators in infinite dimensions: Information geometry 2(2): 2019-12: doi: 10.1007/s41884-019-00022-1. Available at: https://arxiv.org/abs/1907.00732\n\n\nF.M. Ciaglia, F.D. Cosmo, A. Figueroa, G. Marmo, and L. Schiavone: Geometry from divergence functions and complex structures: International journal of quantum information 18(01): 2020-02: doi: 10.1142/s021974991941020x. Available at: https://arxiv.org/abs/2002.02891\n\n\nF.M. Ciaglia: Quantum states, groups and monotone metric tensors: The european physical journal plus 135(6): 2020-06: doi: 10.1140/epjp/s13360-020-00537-y. Available at: https://arxiv.org/abs/2006.10595\n\n\nF.M. Ciaglia, J. Jost, and L. Schwachhöfer: From the Jordan Product to Riemannian Geometries on Classical and Quantum States: Entropy 22(6): 2020-06: doi: 10.3390/e22060637. Available at: https://arxiv.org/abs/2005.02023\n\n\nF.M. Ciaglia, J. Jost, and L. Schwachhöfer: Differential Geometric Aspects of Parametric Estimation Theory for States on Finite-Dimensional C\\(\\ast\\)-Algebras: Entropy 22(11): 2020-11: doi: 10.3390/e22111332. Available at: https://arxiv.org/abs/2010.14394\n\n\nF.M. Ciaglia, S. Mancini, and M. Ha Quang: Focus point: Classical and quantum information geometry: The european physical journal plus 136(5): 2021-05: doi: 10.1140/epjp/s13360-021-01541-6\n\n\nF.M. Ciaglia and F. Di Nocera: Group Actions and Monotone Metric Tensors: The Qubit Case: Geometric Science of Information: 2021-07: doi: 10.1007/978-3-030-80209-7_17. Available at: https://arxiv.org/abs/2105.15067\n\n\nF.M. Ciaglia and F. Di Nocera: Group Actions and Monotone Quantum Metric Tensors: Mathematics 10(15): 2022-07: doi: 10.3390/math10152613. Available at: https://arxiv.org/abs/2206.10394\n\n\nF.M. Ciaglia, F. Di Cosmo, and L. González-Bravo: Can Čencov Meet Petz: Geometric Science of Information: 2023-08: doi: 10.1007/978-3-031-38299-4_38. Available at: https://arxiv.org/abs/2305.12482\n\n\nF.M. Ciaglia, F. Di Cosmo, laura González-Bravo, A. Ibort, and G. Marmo: The categorical foundations of quantum information theory: Categories and the Cramér–Rao inequality: Mod. Phys. Lett. a 38(16 & 17): 2023-08: doi: 10.1142/S0217732323500852. Available at: https://arxiv.org/abs/2309.10428\n\n\nF.M. Ciaglia, J. Jost, and L.J. Schwachhöfer: Information geometry, jordan algebras, and a coadjoint orbit-like construction: SIGMA 19: 2023-10: doi: 10.3842/SIGMA.2023.078. Available at: https://arxiv.org/abs/2112.09781\n\n\nF.M. Ciaglia, F. Di Cosmo, A. Ibort, and G. Marmo: G-dual Teleparallel Connections in Information Geometry: Information geometry 7(1): 2024-01: doi: 10.1007/s41884-023-00117-w. Available at: https://arxiv.org/abs/2207.08694\n\n\nF.M. Ciaglia, F. Di Nocera, J. Jost, and L. Schwachhöfer: Parametric models and information geometry on W*-algebras: Information geometry 7(1): 2024-01: doi: 10.1007/s41884-022-00094-6. Available at: https://arxiv.org/abs/2207.09396\n\n\nF.M. Ciaglia, F. Di Cosmo, F. Di Nocera, and P. Vitale: Monotone metric tensors in quantum information geometry: International journal of geometric methods in modern physics 21(10): 2024-09: doi: 10.1142/S0219887824400048. Available at: https://arxiv.org/abs/2203.10857\n\n\nF.M. Ciaglia, S. Jiang, J. Jost, and L. Schwachhöfer: A coadjoint orbit–like construction for Jordan superalgebras: Journal of geometry and physics 209: 2025-03: doi: 10.1016/j.geomphys.2024.105404. Available at: https://arxiv.org/abs/2311.01333\n\n\nF.M. Ciaglia, F. Di Cosmo, and L. González-Bravo: Towards a category-theoretic foundation of classical and quantum information geometry: 2025-09: doi: 10.48550/arXiv.2509.10262. Available at: https://arxiv.org/abs/2509.10262",
    "crumbs": [
      "Home",
      "Publications",
      "Information geometry"
    ]
  },
  {
    "objectID": "publications/preprints.html",
    "href": "publications/preprints.html",
    "title": "Preprints",
    "section": "",
    "text": "The list is ordered according to the date of arXiv submission, and not the date of creation.\n\n\nF.M. Ciaglia, F. Di Cosmo, A. Ibort, G. Marmo, L. Schiavone, and A. Zampini: The Geometry of the solution space of first order Hamiltonian field theories I: From particle dynamics to free Electrodynamics: 2022-08: Available at: http://arxiv.org/abs/2208.14136\n\n\nF.M. Ciaglia, F. Di Cosmo, A. Ibort, G. Marmo, L. Schiavone, and A. Zampini: The Geometry of the solution space of first order Hamiltonian field theories II: Non-Abelian gauge theories: 2022-08: Available at: http://arxiv.org/abs/2208.14155\n\n\nF.M. Ciaglia, F. Di Cosmo, and L. González-Bravo: Towards a category-theoretic foundation of classical and quantum information geometry: 2025-09: doi: 10.48550/arXiv.2509.10262. Available at: https://arxiv.org/abs/2509.10262",
    "crumbs": [
      "Home",
      "Publications",
      "Preprints"
    ]
  }
]